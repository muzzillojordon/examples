{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import (\n",
    "    KMeans,\n",
    "    MiniBatchKMeans,\n",
    "    AgglomerativeClustering,\n",
    "    DBSCAN,\n",
    "    MeanShift,\n",
    "    estimate_bandwidth,\n",
    ")\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample, random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pathlib\n",
    "\n",
    "# ***********notes*******************\n",
    "# NOTE: Alert Type Meanings:\n",
    "# HHA= high high alert\n",
    "# LLA = Low low alert\n",
    "# AAF = Area Alert - Fast Response\n",
    "# AAS: areal alert - slow response\n",
    "# FRZ: Frozen Data\n",
    "# FRQ: Anomaly Frequency\n",
    "# OSC: Oscillations\n",
    "# WOR: Watch Override\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL PLOT FUNCTIONS:\n",
    "def plt_heat(df_toplt, plt_name=\"1\", fig_hight = 0, fig_width = 0, fntsze=12):\n",
    "    # Correlation between columns\n",
    "    if fig_hight ==0:\n",
    "        fig_hight = len(df_toplt.columns)\n",
    "    if fig_width ==0:\n",
    "        fig_width = len(df_toplt.columns)\n",
    "\n",
    "    figsize = (fig_width, fig_hight)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        data=df_toplt.corr(),\n",
    "        annot=True,\n",
    "        linewidths=0.05,\n",
    "        cmap=\"coolwarm\",\n",
    "        square=True,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"fontsize\":fntsze}\n",
    "    ).tick_params(labelsize=fntsze+2)\n",
    "\n",
    "    \n",
    "    # plt.figure.yaxis('', size=36)\n",
    "    # plt.figure.set_xlabel('', size=36)\n",
    "    plt.title(\"CORRELATION MATRIX\", fontsize=fntsze, color=\"dimgrey\", fontweight=\"bold\")\n",
    "    plt.savefig(f\"trials/v_{version}/data_images/heatmap_{plt_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plt_boxplot(df_toplt, plt_name=\"1\", fig_hight = 0, fig_width = 0, fntsze=12):\n",
    "    \n",
    "    if fig_hight ==0:\n",
    "        fig_hight = len(df_toplt.columns)\n",
    "    if fig_width ==0:\n",
    "        fig_width = len(df_toplt.columns)+10\n",
    "\n",
    "    figsize = (fig_width, fig_hight)\n",
    "\n",
    "    # Listing numeric columns\n",
    "    v_n = list(df_toplt.select_dtypes(include=[\"float64\", \"int64\"]).columns)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.set(font_scale=1)\n",
    "    x = 1\n",
    "    for column in v_n:\n",
    "        plt.subplot(((df_toplt.shape[1] // 3) + 1), 3, x)\n",
    "        sns.boxplot(df_toplt[column]).tick_params(labelsize=fntsze)\n",
    "        plt.title(\n",
    "            \"BoxPlot {}\".format(column), fontsize=fntsze, color=\"dimgrey\", fontweight=\"bold\"\n",
    "        )\n",
    "        x += 1\n",
    "    plt.savefig(f\"trials/v_{version}/data_images/box_{plt_name}.png\")\n",
    "\n",
    "\n",
    "def plt_distplot(df_toplt, plt_name=\"1\", fig_hight = 0, fig_width = 0, fntsze=12):\n",
    "\n",
    "    if fig_hight ==0:\n",
    "        fig_hight = len(df_toplt.columns)\n",
    "    if fig_width ==0:\n",
    "        fig_width = len(df_toplt.columns)+10\n",
    "\n",
    "    figsize = (fig_width, fig_hight)\n",
    "\n",
    "    v_n = list(df_toplt.select_dtypes(include=[\"float64\", \"int64\"]).columns)\n",
    "    plt.figure(figsize=figsize)\n",
    "    x = 1\n",
    "    sns.set(font_scale=1)\n",
    "    \n",
    "    for column in v_n:\n",
    "        \n",
    "        plt.subplot(((df_toplt.shape[1] // 3) + 1), 3, x)\n",
    "        sns.distplot(df_toplt[column], kde=True, norm_hist=True, bins=100).tick_params(labelsize=fntsze)\n",
    "        plt.title(\n",
    "            \"Distribution {}\".format(column),\n",
    "            fontsize=fntsze,\n",
    "            color=\"dimgrey\",\n",
    "            fontweight=\"bold\",\n",
    "            y=1,\n",
    "        )\n",
    "        x += 1\n",
    "\n",
    "    plt.savefig(f\"trials/v_{version}/data_images/histogram_{plt_name}.png\")\n",
    "\n",
    "\n",
    "def plt_histo(df_toplt, plt_name=\"1\", fig_hight = 0, fig_width = 0, fntsze=12, bns=60):\n",
    "\n",
    "    if fig_hight ==0:\n",
    "        fig_hight = len(df_toplt.columns)+10\n",
    "    if fig_width ==0:\n",
    "        fig_width = len(df_toplt.columns)+15\n",
    "\n",
    "    figsize = (fig_width, fig_hight)\n",
    "\n",
    "    v_n = list(df_toplt.select_dtypes(include=[\"float64\", \"int64\"]).columns)\n",
    "    plt.figure(figsize=figsize)\n",
    "    x = 1\n",
    "    for column in v_n:\n",
    "        plt.subplot(((df_toplt.shape[1] // 3) + 1), 3, x)\n",
    "        plt.hist(df_toplt[column], bins=bns, linewidth=0.5, edgecolor=\"white\")\n",
    "        plt.xticks(fontsize=fntsze)\n",
    "        plt.title(\n",
    "            \"Distribution {}\".format(column),\n",
    "            fontsize=fntsze+2,\n",
    "            color=\"dimgrey\",\n",
    "            fontweight=\"bold\",\n",
    "            y=1,\n",
    "        )\n",
    "        x += 1\n",
    "    \n",
    "\n",
    "def variance_bars_plt(plt_name=\"1\"):\n",
    "    # Instantiate the Percentage of Variance Explained\n",
    "    pve = pca.explained_variance_ratio_\n",
    "    # Instantiate the Cumulative Percentage of the Variance\n",
    "    pcve = np.cumsum(pve)\n",
    "\n",
    "    plt.bar(\n",
    "        range(0, len(pve)),\n",
    "        pve,\n",
    "        alpha=0.5,\n",
    "        align=\"center\",\n",
    "        label=\"Percentage of Individually Explained Variance\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plt.step(\n",
    "        range(0, len(pcve)),\n",
    "        pcve,\n",
    "        where=\"mid\",\n",
    "        label=\"Cumulative Percentage of Explained Variance\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.xticks(range(0, pca.n_features_in_))\n",
    "    plt.ylabel(\"Percentage of Explained Variance\")\n",
    "    plt.xlabel(\"Components\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"trials/v_{version}/data_images/variance_bars_{plt_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def percent_variance_plt_ratio(\n",
    "    plots, yaxis_title, xaxis_title, dashed_pos, plt_name=\"1\"\n",
    "):\n",
    "    rows = (len(plots) // 2) + 1\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(plots)):\n",
    "        plt.subplot(rows, 2, i + 1)\n",
    "        plt.style.use(\"ggplot\")\n",
    "\n",
    "        if dashed_pos[i] > 0.0:\n",
    "            plt.axhline(y=dashed_pos[i], color=\"black\", linestyle=\"--\")\n",
    "\n",
    "        plt.plot(plots[i], marker=\"o\")\n",
    "        plt.xticks(range(0, pca.n_features_in_))\n",
    "        plt.xlabel(xaxis_title[i])\n",
    "        plt.ylabel(yaxis_title[i])\n",
    "        plt.title(\"Scree Plot\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    plt.savefig(f\"trials/v_{version}/data_images/variance_ratios_{plt_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plt_kelbow_metric(data, model_touse, k_range, metric_touse, visualizer_touse):\n",
    "    rows = (len(metric_touse) // 2) + 1\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 8\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "    for i in range(len(metric_touse)):\n",
    "        plt.subplot(rows, 2, i + 1)\n",
    "        if visualizer_touse == \"kmeans\":\n",
    "            visualizer = KElbowVisualizer(\n",
    "                model_touse, k=k_range, metric=metric_touse[i], timings=False\n",
    "            )\n",
    "        elif visualizer_touse == \"AgglomerativeClustering\":\n",
    "            visualizer = KElbowVisualizer(\n",
    "                AgglomerativeClustering(), metric=metric_touse[i], timings=False\n",
    "            )\n",
    "        visualizer.fit(data)\n",
    "        visualizer.finalize()\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    plt.savefig(f\"trials/v_{version}/scatter_images/kelbow_{visualizer_touse}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_kmeans_plotly(xycol, kmeans, ttle):\n",
    "    x_col = col_index_name[xycol[0]]\n",
    "    y_col = col_index_name[xycol[1]]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        x=features_df.iloc[:, x_col[0]],\n",
    "        y=features_df.iloc[:, y_col[0]],\n",
    "        color=kmeans.labels_,\n",
    "        title=ttle,\n",
    "    ).update_layout(xaxis_title=x_col[1], yaxis_title=y_col[1])\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_general(df, xycol, plt_name='', figsiz=(20, 15), fntsze=10):\n",
    "    col_index_name = [(df.columns.get_loc(c), c) for c in df]\n",
    "    rows = (len(xycol) // 2) + 1\n",
    "    plt.figure(figsize=figsiz)\n",
    "    plt.rcParams[\"xtick.labelsize\"] = fntsze\n",
    "    plt.rcParams[\"ytick.labelsize\"] = fntsze\n",
    "\n",
    "    for i in range(len(xycol)):\n",
    "        # i=0\n",
    "        x_col = col_index_name[xycol[i][0]]\n",
    "        y_col = col_index_name[xycol[i][1]]\n",
    "\n",
    "        plt.subplot(rows, 2, i + 1)\n",
    "        sns.scatterplot(data=df, x=x_col[1], y=y_col[1], hue=\"cluster\", palette=\"Set2\")\n",
    "        plt.xticks(fontsize=fntsze)\n",
    "        plt.title(f\"{x_col[1]} vs {y_col[1]}\", fontsize=fntsze+2, color=\"dimgrey\", fontweight=\"bold\")\n",
    "        plt.xlabel(x_col[1], color=\"dimgrey\", labelpad=5, fontweight=\"bold\", fontsize=fntsze)\n",
    "        plt.ylabel(y_col[1], color=\"dimgrey\", fontweight=\"bold\", fontsize=fntsze)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    if not plt_name == '':\n",
    "        plt.savefig(f\"trials/v_{version}/scatter_images/{plt_name}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def Silhouette_plot(model, data):\n",
    "    visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "    visualizer.fit(data)\n",
    "    return visualizer\n",
    "\n",
    "\n",
    "def plot_bars_box(df, y_col, plt_name):\n",
    "    col_index_name = [(df.columns.get_loc(c), c) for c in df]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 8\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # plt.subplot(rows, 2, i+x)\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"cluster\",\n",
    "        y=y_col,\n",
    "        palette=\"Set2\",\n",
    "        order=df.groupby([\"cluster\"])[y_col]\n",
    "        .median()\n",
    "        .sort_values(ascending=False)\n",
    "        .index,\n",
    "    )\n",
    "    sns.despine()\n",
    "    plt.title(\n",
    "        f\"BOXPLOT - CLUSTERS VS. {y_col}\",\n",
    "        fontsize=9,\n",
    "        color=\"dimgrey\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.xlabel(\"CLUSTERS\", color=\"dimgrey\", labelpad=5, fontweight=\"bold\", fontsize=7)\n",
    "    plt.ylabel(y_col, color=\"dimgrey\", fontweight=\"bold\", fontsize=7)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # plt.subplot(rows, 2, i+x+1)\n",
    "    cores = [\"#66c2a5\", \"#8da0cb\", \"#fc8d62\"]\n",
    "    sns.set_palette(sns.color_palette(cores))\n",
    "    mean = np.mean(df[y_col])\n",
    "    ax = sns.barplot(\n",
    "        x=\"cluster\",\n",
    "        y=y_col,\n",
    "        data=df,\n",
    "        order=df.groupby([\"cluster\"])[y_col].mean().sort_values(ascending=False).index,\n",
    "    )\n",
    "    sns.despine()\n",
    "    ax.axhline(mean, color=\"r\", linestyle=\"--\")\n",
    "    plt.title(f\"CLUSTERS VS. {y_col}\", fontsize=9, color=\"dimgrey\", fontweight=\"bold\")\n",
    "    plt.xlabel(\"CLUSTERS\", color=\"dimgrey\", labelpad=5, fontweight=\"bold\", fontsize=7)\n",
    "    plt.ylabel(f\"AVERAGE {y_col}\", color=\"dimgrey\", fontweight=\"bold\", fontsize=7)\n",
    "\n",
    "    # x += 1\n",
    "    plt.subplots_adjust(wspace=0.4)\n",
    "    plt.savefig(f\"trials/v_{version}/results/plotBars_{y_col}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# OTHER GENERAL FUNCTIONS\n",
    "def variance_table(data, explained_var, title):\n",
    "    # Table of the amount of variation explained\n",
    "    num = range(1, len(data.columns) + 1)\n",
    "    df_com = pd.DataFrame(num, columns=[\"Number of Components\"])\n",
    "    ev = explained_var\n",
    "    df_ev = pd.DataFrame(ev, columns=[title])\n",
    "    df_ev = pd.concat([df_com, df_ev], axis=1)\n",
    "    df_ev.index = df_ev.index + 1\n",
    "    return df_ev\n",
    "\n",
    "\n",
    "def apply_kmeans(n_clust, data_to_fit, data_to_show):\n",
    "    # Apply the K-Means algorithm to 3 Clusters\n",
    "    kmeans = KMeans(n_clusters=n_clust, random_state=18)\n",
    "    kmeans.fit(data_to_fit)\n",
    "    df = data_to_show.copy()\n",
    "    df[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "\n",
    "# Evaluating metrics for several different cluster values\n",
    "def metrics_table(n_cluster_range, data, model_touse, metrics):\n",
    "    # Metrics\n",
    "    n_clusters = []\n",
    "    silhouette = []\n",
    "    calinski_harabasz = []\n",
    "    davies_bouldin = []\n",
    "    inertia = []\n",
    "\n",
    "    for n_cluster in n_cluster_range:\n",
    "        if model_touse == \"kmeans\":\n",
    "            model = KMeans(n_clusters=n_cluster, random_state=18)\n",
    "        elif model_touse == \"hc\":\n",
    "            model = AgglomerativeClustering(n_clusters=n_cluster)\n",
    "\n",
    "        pred = model.fit_predict(data)\n",
    "\n",
    "        if \"silhouette\" in metrics:\n",
    "            silhouette.append(silhouette_score(data, pred))\n",
    "        else:\n",
    "            silhouette = [0] * len(n_cluster_range)\n",
    "        if \"calinski_harabasz\" in metrics:\n",
    "            calinski_harabasz.append(calinski_harabasz_score(data, pred))\n",
    "        else:\n",
    "            calinski_harabasz = [0] * len(n_cluster_range)\n",
    "        if \"davies_bouldin\" in metrics:\n",
    "            davies_bouldin.append(davies_bouldin_score(data, pred))\n",
    "        else:\n",
    "            davies_bouldin = [0] * len(n_cluster_range)\n",
    "        if \"inertia\" in metrics:\n",
    "            inertia.append(model.inertia_)\n",
    "        else:\n",
    "            inertia = [0] * len(n_cluster_range)\n",
    "\n",
    "        n_clusters.append(n_cluster)\n",
    "\n",
    "    # Results\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            \"Model\": model_touse,\n",
    "            \"Clusters\": n_clusters,\n",
    "            \"Silhouette\": silhouette,\n",
    "            \"Calinski Harabasz\": calinski_harabasz,\n",
    "            \"Davies Bouldin\": davies_bouldin,\n",
    "            \"Inertia\": inertia,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"silhouette\" in metrics:\n",
    "        result.sort_values(\"Silhouette\", ascending=False)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def columnsnames_to_index(col_index_name, xycol_names):\n",
    "    col_index_nameunzip = list(zip(*col_index_name))\n",
    "    xycol = [\n",
    "        (col_index_nameunzip[1].index(item[0]), col_index_nameunzip[1].index(item[1]))\n",
    "        for item in xycol_names\n",
    "    ]\n",
    "    return xycol\n",
    "\n",
    "\n",
    "def get_outliers(df, quantile_num=0.99, upper_lower='>'):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        df (_type_, optional): _description_. Defaults to aggreg_df.\n",
    "        quantile_num (float, optional): _description_. Defaults to .98.\n",
    "\n",
    "    Returns:\n",
    "        dictionary: of lists, key is column name, list is value at the quartile and how many values are above the quartile\n",
    "    \"\"\"\n",
    "\n",
    "    # show outliers: show how many items /rows above upper quantiles\n",
    "    df_col = df.select_dtypes(include=[np.float64, np.int64]).columns.tolist()\n",
    "    if upper_lower == '>':\n",
    "        quantiles_dict = dict(\n",
    "            [\n",
    "                (\n",
    "                    item,\n",
    "                    [\n",
    "                        df[item].quantile(quantile_num),\n",
    "                        df.loc[df[item] > df[item].quantile(quantile_num)].shape[0],\n",
    "                    ],\n",
    "                )\n",
    "                for item in df_col\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        quantiles_dict = dict(\n",
    "            [\n",
    "                (\n",
    "                    item,\n",
    "                    [\n",
    "                        df[item].quantile(quantile_num),\n",
    "                        df.loc[df[item] < df[item].quantile(quantile_num)].shape[0],\n",
    "                    ],\n",
    "                )\n",
    "                for item in df_col\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame.from_dict(quantiles_dict).rename(\n",
    "        index={0: \"quntile_value\", 1: \"rows_in_quantile\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 10\n",
    "\n",
    "# read in final aggregated data\n",
    "df_original = pd.read_pickle(\"mqi_data_aggregated/aggreg_df.pkl\")\n",
    "\n",
    "# setup path to save images\n",
    "paths_to_make = [\"data_images\", \"results\", \"scatter_images\"]\n",
    "for i in range(len(paths_to_make)):\n",
    "    path = pathlib.Path(f\"./trials/v_{version}/{paths_to_make[i]}\")\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path = pathlib.Path(f\"./trials/v_{version}/\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix all Null Values and apply general data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.copy()\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#Remove any models that have null values for actual vs expected, upper, lower percent difference, these models have never predicted (ie, sensor issues or something like that)\n",
    "df = df.loc[~((df[\"actual_vs_expected_percdiff\"].isna()) | (df[\"upper_vs_actual_percdiff\"].isna()) | (df[\"lower_vs_actual_percdiff\"].isna()))]\n",
    "print(df.shape)\n",
    "\n",
    "#remove any models that model is not at least 4 weeks old and remove models older than 3.6 years\n",
    "df = df.loc[df['model_age'] > 12]\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "#fill any null model score with 0, fixed limit, moving average, forcast, rate of change do not have model score\n",
    "df['model_score_avg'].fillna(0, inplace=True)\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert any non numeric columns to numeric\n",
    "print(df.shape)\n",
    "df = pd.get_dummies(df, columns=[\"modeltype\", \"site\", \"position_class_group\"])\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns we do not want to cluster on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Copy of data\n",
    "dfcpy = df.copy()\n",
    "df_clean_original = df.copy()\n",
    "\n",
    "# Touch_freq is a sum of all these columns\n",
    "action_col = [\n",
    "    \"actiontype_Clear Alert Status_freq\",\n",
    "    \"actiontype_Diagnose Cleared_freq\",\n",
    "    \"actiontype_Diagnose Set_freq\",\n",
    "    \"actiontype_Ignore Expiration_freq\",\n",
    "    \"actiontype_Ignore Set_freq\",\n",
    "    \"actiontype_Model Maintenance Cleared_freq\",\n",
    "    \"actiontype_Model Maintenance Set_freq\",\n",
    "    \"actiontype_Note Added_freq\",\n",
    "    \"actiontype_Quick Watch Set_freq\",\n",
    "    \"actiontype_Stop Ignoring_freq\",\n",
    "    \"actiontype_Watch Cleared_freq\",\n",
    "    \"actiontype_Watch Expiration_freq\",\n",
    "    \"actiontype_Watch Override_freq\",\n",
    "    \"actiontype_Watch Set_freq\",\n",
    "]\n",
    "\n",
    "#Equipment Model is for is not something we want to cluster on, it will be good for cluster interpretation \n",
    "position_col = [\n",
    "    \"position_class_group_EP\",\n",
    "    \"position_class_group_FP-COMP_VALVE_PIPE_OTHER\",\n",
    "    \"position_class_group_FP-EXCH\",\n",
    "    \"position_class_group_FP-FIRED\",\n",
    "    \"position_class_group_FP-tank-vessel\",\n",
    "    \"position_class_group_IP-LOOPS_XMTR_OTHER\",\n",
    "    \"position_class_group_IP-VALVE\",\n",
    "    \"position_class_group_MP-COMP\",\n",
    "    \"position_class_group_MP-FAN_TURBN_GRBX_OTHER\",\n",
    "    \"position_class_group_MP-PUMP\",\n",
    "    \"position_class_group_OP_P\",\n",
    "    'position_class_group_unknown'\n",
    "]\n",
    "\n",
    "#Type of Model is not something we want to cluster on, it will be good for cluster interpretation \n",
    "model_types = [\n",
    "    \"modeltype_APR\",\n",
    "    \"modeltype_Fixed Limit\",\n",
    "    # \"modeltype_Forecast\",\n",
    "    \"modeltype_Moving Average\",\n",
    "    \"modeltype_Rate of Change\",\n",
    "    \"modeltype_Rolling Average\",\n",
    "]\n",
    "\n",
    "#Site of Model is not something we want to cluster on, it will be good for cluster interpretation \n",
    "model_site = [\n",
    "\"site_FHR-CC\", \n",
    "\"site_FHR-PB\", \n",
    "\"site_Pipelines and Terminals\", \n",
    "'site_unknown'\n",
    "]\n",
    "\n",
    "alert_col = [\n",
    "    \"HHA_freq\",\n",
    "    \"FRQ_freq\",\n",
    "    \"AAF_freq\",\n",
    "    \"LLA_freq\",\n",
    "    \"WOR_freq\",\n",
    "    \"AAS_freq\",\n",
    "    \"OSC_freq\",\n",
    "]\n",
    "\n",
    "#No_alert_freq summarize these columns (opposite but should give same result)\n",
    "dfcpy.drop(columns=alert_col, inplace=True)\n",
    "\n",
    "#case count and frequency is not a good metric, it is not tied directly to a model but instead tied to a piece of equipment a model is on (may be misleading)\n",
    "dfcpy.drop(columns=['case_count_freq', 'case_count'], inplace=True)\n",
    "\n",
    "#Drop Columns:\n",
    "dfcpy.drop(columns=model_site+action_col+position_col+model_types, inplace=True)\n",
    "\n",
    "#drop is not something we want to cluster on, it will be good for cluster interpretation\n",
    "# dfcpy.drop(columns=['currently_active_False', 'currently_active_True'], inplace=True)\n",
    "#dropping currently_active_False bc more values will be 1 in currently_active_True\n",
    "dfcpy.drop(columns=['currently_active_False'], inplace=True)\n",
    "dfcpy.drop(columns=['currently_active_True'], inplace=True)\n",
    "\n",
    "#drop model_score_avg because not all models have a model score ie. Fixedlimit and rolling average\n",
    "dfcpy.drop(columns=['model_score_avg'], inplace=True)\n",
    "\n",
    "\n",
    "# #action_note_len_avg may lead to incorrect clustering, there are alot of auto generated notes etc, that has collinearity with touch_freq\n",
    "# dfcpy.drop(columns=['action_note_len_avg'], inplace=True)\n",
    "\n",
    "#drop due to collinearity with model_build_freq\n",
    "dfcpy.drop(columns=['model_save_freq'], inplace=True)\n",
    "# dfcpy.drop(columns=['model_build_freq'], inplace=True)\n",
    "\n",
    "dfcpy.drop(columns=['noAlert_freq'], inplace=True)\n",
    "# dfcpy.drop(columns=['in_alert_freq'], inplace=True)\n",
    "\n",
    "#drop due to collinearity with actual_vs_expected_percdiff. Using upper vs expected, because fixed limit models do not difference in actual vs expected (ie. == 0)\n",
    "#drop actual_vs_expected_percdiff because not all models have a model score ie. Fixedlimit and rolling average\n",
    "dfcpy.drop(columns=['actual_vs_expected_percdiff'], inplace=True)\n",
    "# dfcpy.drop(columns=['actual_vs_expected_percdiff', 'lower_vs_actual_percdiff'], inplace=True)\n",
    "\n",
    "dfcpy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look correlations at data distributions\n",
    "plt_heat(dfcpy, fig_hight=10, fig_width=10, fntsze=10)\n",
    "plt_distplot(dfcpy, plt_name='raw', fig_hight=35, fig_width=15)\n",
    "# plt_boxplot(dfcpy, plt_name='raw', fig_hight=35, fig_width=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop model age due to high collinearity  with currently active true and model build_freq\n",
    "dfcpy.drop(columns=['model_age'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upper outliers\n",
    "outliers_high = get_outliers(dfcpy, 0.98, '>')\n",
    "print(dfcpy.shape)\n",
    "for col, val in outliers_high.T.iterrows():\n",
    "    dfcpy = dfcpy.loc[dfcpy[col] <= val.values[0]]\n",
    "print(dfcpy.shape)\n",
    "dfcpy.describe().T\n",
    "\n",
    "#lower outliers only for model_build_freq\n",
    "outliers_high = get_outliers(dfcpy[['model_build_freq']], 0.002, '<')\n",
    "print(dfcpy.shape)\n",
    "for col, val in outliers_high.T.iterrows():\n",
    "    dfcpy = dfcpy.loc[dfcpy[col] >= val.values[0]]\n",
    "print(dfcpy.shape)\n",
    "dfcpy.describe().T\n",
    "\n",
    "\n",
    "\n",
    "# model=IsolationForest(n_estimators=150, max_samples='auto', contamination=float(0.1), max_features=1.0)\n",
    "# model.fit(dfcpy)\n",
    "\n",
    "# IsolationForest(contamination=0.1, n_estimators=150)\n",
    "\n",
    "# scores=model.decision_function(dfcpy)\n",
    "# anomaly=model.predict(dfcpy)\n",
    "\n",
    "# dfcpy['scores']=scores\n",
    "# dfcpy['anomaly']=anomaly\n",
    "\n",
    "# anomaly = dfcpy.loc[dfcpy['anomaly']==-1]\n",
    "# anomaly_index = list(anomaly.index)\n",
    "# print('Total number of outliers is:', len(anomaly))\n",
    "\n",
    "# # dropping outliers\n",
    "# dfcpy = dfcpy.drop(anomaly_index, axis = 0)\n",
    "# dfcpy.drop(columns=['scores', 'anomaly'], inplace=True)\n",
    "# print(dfcpy.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are alot of models with zero touches and zero alerts, this is causing data to have a large skew. will need to remove some of these to try and get better distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(dfcpy[0:])\n",
    "# fig.savefig(f\"trials/v_{version}/scatter_images/pairplot_kmeans_3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random sample to remove some zero values\n",
    "# print(dfcpy.shape)\n",
    "# touches_random_sample = dfcpy.loc[((dfcpy['touches_count_freq']==0))]\n",
    "# index_to_remove = touches_random_sample.sample(int(touches_random_sample.shape[0]//2)).index\n",
    "# dfcpy.drop(index=index_to_remove, inplace=True)\n",
    "# print(dfcpy.shape)\n",
    "\n",
    "# print(dfcpy.shape)\n",
    "# touches_random_sample = dfcpy.loc[((dfcpy['in_alert_freq']==0))]\n",
    "# index_to_remove = touches_random_sample.sample(int(touches_random_sample.shape[0]//2)).index\n",
    "# dfcpy.drop(index=index_to_remove, inplace=True)\n",
    "# print(dfcpy.shape)\n",
    "\n",
    "# print(dfcpy.shape)\n",
    "# touches_random_sample = dfcpy.loc[((dfcpy['upper_vs_actual_percdiff']==0))]\n",
    "# index_to_remove = touches_random_sample.sample(int(touches_random_sample.shape[0]//2)).index\n",
    "# dfcpy.drop(index=index_to_remove, inplace=True)\n",
    "# print(dfcpy.shape)\n",
    "\n",
    "# print(dfcpy.shape)\n",
    "# touches_random_sample = dfcpy.loc[((dfcpy['lower_vs_actual_percdiff']==0))]\n",
    "# index_to_remove = touches_random_sample.sample(int(touches_random_sample.shape[0]//2)).index\n",
    "# dfcpy.drop(index=index_to_remove, inplace=True)\n",
    "# print(dfcpy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update original data frame with removed rows (need this to match clusters later)\n",
    "df_clean_original = df_clean_original.loc[df_clean_original.index.isin(dfcpy.index)]\n",
    "df_clean_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfcpy.skew().sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply log transform to help distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_log = [\n",
    "    col for col in dfcpy.columns if len(dfcpy[col].value_counts()) != 2\n",
    "]\n",
    "dfcpy[columns_to_log] = dfcpy[columns_to_log].apply(lambda x: np.log(x+1))\n",
    "\n",
    "plt_boxplot(dfcpy, plt_name='raw', fig_hight=10, fig_width=20, fntsze=10)\n",
    "plt_distplot(dfcpy, plt_name='raw', fig_hight=20, fig_width=30, fntsze=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORMALIZE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize with Min-max\n",
    "df_norm = dfcpy.copy()\n",
    "# get only columns that are not 0 or 1 (ie have range of values)\n",
    "columns_to_norm = [\n",
    "    col for col in df_norm.columns if len(df_norm[col].value_counts()) != 2\n",
    "]\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_norm[columns_to_norm] = min_max_scaler.fit_transform(df_norm[columns_to_norm])\n",
    "\n",
    "\n",
    "# # standardize with mean\n",
    "# df_stand = dfcpy.copy()\n",
    "# # get only columns that are not 0 or 1 (ie have range of values)\n",
    "# columns_to_stand = [\n",
    "#     col for col in df_stand.columns if len(df_stand[col].value_counts()) != 2\n",
    "# ]\n",
    "# stand_scaler = preprocessing.StandardScaler()\n",
    "# df_stand[columns_to_stand] = stand_scaler.fit_transform(df_stand[columns_to_stand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.describe().round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_boxplot(df_norm, plt_name='raw', fig_hight=15, fig_width=20, fntsze=10)\n",
    "plt_distplot(df_norm, plt_name='raw', fig_hight=15, fig_width=20, fntsze=10)\n",
    "# plt_histo(df_norm, plt_name='raw', fig_hight=15, fig_width=20, fntsze=10, bns=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = df_norm.copy()\n",
    "# features_df = df_stand.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START PCA: Determine how many features to use for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********Start PCA *******************\n",
    "use_PCA = False\n",
    "\n",
    "if use_PCA:\n",
    "    pca = PCA()\n",
    "    df_pca = pca.fit_transform(features_df)\n",
    "    var1 = variance_table(features_df, pca.explained_variance_, \"Explained Variance\")\n",
    "    var2 = variance_table(\n",
    "        features_df,\n",
    "        (pca.explained_variance_ratio_ * 100),\n",
    "        \"Percentage of Explained Variance\",\n",
    "    )\n",
    "    var3 = variance_table(\n",
    "        features_df,\n",
    "        (pca.explained_variance_ratio_.cumsum() * 100),\n",
    "        \"Cumulative Percentage of Explained Variance\",\n",
    "    )\n",
    "else:\n",
    "    data = features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    print(var1.head(5))\n",
    "    print(var2.head(5))\n",
    "    print(var3.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    variance_bars_plt()\n",
    "    percent_variance_plt_ratio(\n",
    "        [pca.explained_variance_ratio_, pca.explained_variance_ratio_.cumsum()],\n",
    "        [\"Percentage of Explained Variance\", \"Cumulative Percentage of Explained Variance\"],\n",
    "        [\"Components\", \"Components\"],\n",
    "        [0, 0.8],\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally it is recommended to use at least 80% of the variance when applying PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    # with more than 80% of the variance\n",
    "    pca1 = PCA(n_components=4, random_state=18)\n",
    "    df_pca1 = pca1.fit_transform(features_df)\n",
    "    df_pca1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START: Determine how many Cluster to use\n",
    "\n",
    "Silhouette: closer to 1 the better\n",
    "    shows how similar each sample is to other points in its own cluster relative to other clusters. Positive values == closer to point in own cluster\n",
    "    0 values indicate point is on the border\n",
    "    neg values indicate samples are not clustered well and point should be in another cluster\n",
    "\n",
    "Calinski Harabasz: the higher the value the better the clustering:\n",
    "    measures the relationship between variations within its own cluster and variations between clusters \n",
    "    lower values can indicate the clusters are overlapping\n",
    "\n",
    "Davies Bouldin: the smaller this value the better:\n",
    "    is a measurement of similarity between data samples in a cluster. The value measures the similarity between each cluster and its closet neighbors\n",
    "    the smaller this value shows that an item in a cluster is less like another cluster, ie. good clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot metrics to find number of cluster\n",
    "plt_kelbow_metric(\n",
    "    data,\n",
    "    KMeans(random_state=18),\n",
    "    (2, 9),\n",
    "    [\"distortion\", \"silhouette\", \"calinski_harabasz\"],\n",
    "    \"kmeans\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clst_scores_df = metrics_table(\n",
    "    range(2, 8),\n",
    "    data,\n",
    "    \"kmeans\",\n",
    "    [\"inertia\", \"silhouette\", \"calinski_harabasz\", \"davies_bouldin\"],\n",
    ")\n",
    "clst_scores_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette: closer to 1 the better\n",
    "\n",
    "Inertia: the lower this value the better\n",
    "\n",
    "Calinski Harabasz: the higher the value the better the clustering:\n",
    "\n",
    "Davies Bouldin: the smaller this value the better:\n",
    "\n",
    "Silhouette analysis can be used visually see how each cluster relates to another. \n",
    "values on the x-axis closer +1 are far away from neighboring cluster\n",
    "values closer to zero are on or near the boarder line between neighboring clusters\n",
    "negative values  indicate it may be in the wrong cluster\n",
    "\n",
    "The dotted line represents average Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_clust_try = [2, 3, 4]\n",
    "for i in range(len(n_clust_try)):\n",
    "    vis = Silhouette_plot(\n",
    "        KMeans(n_clusters=n_clust_try[i], random_state=18), data\n",
    "    ).show()\n",
    "    vis.get_figure().savefig(\n",
    "        f\"trials/v_{version}/scatter_images/silhouettvis_cluster-{n_clust_try[i]}.png\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column index / name for plot\n",
    "col_index_name = [(features_df.columns.get_loc(c), c) for c in features_df]\n",
    "\n",
    "# selecting which columns to plot vs each other.\n",
    "# xycol_names = [('actual_vs_expected_percdiff', 'touches_count_freq'),\n",
    "#                     ('model_build_freq', 'touches_count_freq'),\n",
    "#                     ('model_build_freq', 'noAlert_freq'),\n",
    "#                     ('noAlert_freq', 'touches_count_freq'),\n",
    "#                     ]\n",
    "\n",
    "# xycol_names = [\n",
    "#     (\"actual_vs_expected_percdiff\", \"actiontype_Quick Watch Set_freq\"),\n",
    "#     (\"upper_vs_actual_percdiff\", \"model_save_freq\"),\n",
    "#     (\"FRQ_freq\", \"model_save_freq\"),\n",
    "#     (\"actiontype_Note Added_freq\", \"model_build_freq\"),\n",
    "# ]\n",
    "\n",
    "\n",
    "xycol_names = [\n",
    "    (\"upper_vs_actual_percdiff\", \"in_alert_freq\"),\n",
    "    (\"upper_vs_actual_percdiff\", \"touches_count_freq\"),\n",
    "    (\"upper_vs_actual_percdiff\", \"in_alert_freq\"),\n",
    "    (\"touches_count_freq\", \"model_build_freq\"),\n",
    "    (\"touches_count_freq\", \"in_alert_freq\"),\n",
    "    (\"in_alert_freq\", \"model_build_freq\"),\n",
    "]\n",
    "\n",
    "\n",
    "# convert column name tuples to indexes\n",
    "\n",
    "xycol = columnsnames_to_index(col_index_name, xycol_names)\n",
    "\n",
    "# calculate Kmeans with PCA or original data, then plot with original data\n",
    "\n",
    "n_clust = 2\n",
    "df2, kmeans2 = apply_kmeans(n_clust, data, features_df)\n",
    "plot_general(df2, xycol, f\"clust_kmeans-{n_clust}\", figsiz=(30, 40), fntsze=14)\n",
    "\n",
    "n_clust = 3\n",
    "df3, kmeans3 = apply_kmeans(n_clust, data, features_df)\n",
    "plot_general(df3, xycol, f\"clust_kmeans-{n_clust}\", figsiz=(30, 40), fntsze=14)\n",
    "\n",
    "n_clust = 4\n",
    "df4, kmeans4 = apply_kmeans(n_clust, data, features_df)\n",
    "plot_general(df4, xycol, f\"clust_kmeans-{n_clust}\", figsiz=(30, 40), fntsze=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(df3[0:],hue='cluster')\n",
    "fig.savefig(f\"trials/v_{version}/scatter_images/pairplot_kmeans_3.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df3,x=\"upper_vs_actual_percdiff\",y=\"touches_count_freq\",z=\"in_alert_freq\",\n",
    "                    color='cluster',\n",
    "                    title=f'KMeans 3')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    # Apply PCA  components for preview\n",
    "    n_comp_2 = 2\n",
    "    pca2 = PCA(n_components=n_comp_2, random_state=18)\n",
    "    df_pca2 = pca2.fit_transform(features_df)\n",
    "\n",
    "    n_comp_3 = 3\n",
    "    pca3 = PCA(n_components=n_comp_3, random_state=18)\n",
    "    df_pca3 = pca3.fit_transform(features_df)\n",
    "\n",
    "\n",
    "    df_pca2_kmn = pd.DataFrame(data=df_pca2, columns=[f\"PCA{i+1}\" for i in range(n_comp_2)])\n",
    "    df_pca3_kmn = pd.DataFrame(data=df_pca3, columns=[f\"PCA{i+1}\" for i in range(n_comp_3)])\n",
    "\n",
    "    n_clust = 3\n",
    "    df_kmeans_pca2, kmeans_pca2 = apply_kmeans(n_clust, df_pca2, df_pca2_kmn)\n",
    "    df_kmeans_pca3, kmeans_pca3 = apply_kmeans(n_clust, df_pca3, df_pca3_kmn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    plot_general(df_kmeans_pca2, [(0, 1)], f\"kmeans_PCA_clust-{n_comp_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    fig = px.scatter_3d(df_kmeans_pca3,x=\"PCA1\",y=\"PCA2\",z=\"PCA3\",\n",
    "                        color='cluster',\n",
    "                        title=f'KMeans {n_comp_3} ')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means model\n",
    "final_k_clusters = 3\n",
    "\n",
    "final_df_kmn = metrics_table(\n",
    "    range(final_k_clusters, final_k_clusters + 1),\n",
    "    data,\n",
    "    \"kmeans\",\n",
    "    [\"inertia\", \"silhouette\", \"calinski_harabasz\", \"davies_bouldin\"],\n",
    ")\n",
    "\n",
    "final_df_kmn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********START DBSCAN METHOD*******************\n",
    "# NearestNeighbors\n",
    "\n",
    "# Final K-Means model\n",
    "nbrs = NearestNeighbors(n_neighbors=3).fit(data)\n",
    "\n",
    "# Calculating the distances and indices of the k nearest neighbors\n",
    "distances, indices = nbrs.kneighbors(data)\n",
    "\n",
    "# Plot\n",
    "plt.plot(sorted(distances[:, 1]), \"r-\")\n",
    "plt.title(\n",
    "    \"DISTANCES BETWEEN THE K NEAREST NEIGHBORS OF EACH POINT\",\n",
    "    fontsize=11,\n",
    "    color=\"dimgrey\",\n",
    "    fontweight=\"bold\",\n",
    "    y=1,\n",
    ")\n",
    "plt.xlabel(\n",
    "    \"INDICES  K NEAREST NEIGHBORS\",\n",
    "    color=\"dimgrey\",\n",
    "    labelpad=5,\n",
    "    fontweight=\"bold\",\n",
    "    fontsize=8,\n",
    ")\n",
    "plt.ylabel(\n",
    "    \"DISTANCE TO NEAREST NEIGHBOR\", color=\"dimgrey\", fontweight=\"bold\", fontsize=8\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_lst = np.linspace(0.1, 0.2, 6)\n",
    "# eps_lst = [.08, .1, .12, .13, .15, .18, .2, .23, .25, .275]\n",
    "# eps_lst = [.005, .01, .02, .04, .05, .06, .08, .1]\n",
    "# eps_lst = [.25, .3, .3125, .325, .35, .375, .4, .5, .55]\n",
    "eps_lst = [.03, .04, .05, .06, .07, .1, .13, .15]\n",
    "min_sample = [2, 3, 5, 10, 20, 40, 50, 60, 70]\n",
    "print(eps_lst, min_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means model\n",
    "# Defines the parameter search space\n",
    "param_grid = {\"eps\": eps_lst, \"min_samples\": min_sample}\n",
    "\n",
    "# Create the DBSCAN object\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# Creates the GridSearchCV object\n",
    "grid_search = GridSearchCV(dbscan, param_grid, scoring=silhouette_score, n_jobs=-1)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Displays the best parameters found\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of DBSCAN\n",
    "dbscan = DBSCAN(eps=0.03, min_samples=10, n_jobs=-1).fit(data)\n",
    "# Getting the cluster labels\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.02, 0.1, 20)\n",
    "# np.linspace(0.1, 0.3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = []\n",
    "min_samples = []\n",
    "silhouette = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "n_clusters = []\n",
    "metrics_dbscan = []\n",
    "\n",
    "for eps in np.linspace(0.02, 0.12, 20):\n",
    "    for min_sample in range(2, 60, 5):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_sample, n_jobs=-1)\n",
    "        dbscan.fit(data)\n",
    "        pred = dbscan.labels_\n",
    "\n",
    "        if len(np.unique(pred)) > 1:\n",
    "            silhouette = silhouette_score(data, pred)\n",
    "            calinski_harabasz = calinski_harabasz_score(data, pred)\n",
    "            davies_bouldin = davies_bouldin_score(data, pred)\n",
    "            metrics_dbscan.append(\n",
    "                (\n",
    "                    eps,\n",
    "                    min_sample,\n",
    "                    silhouette,\n",
    "                    calinski_harabasz,\n",
    "                    davies_bouldin,\n",
    "                    len(set(dbscan.labels_)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "df_dbscan = pd.DataFrame(\n",
    "    metrics_dbscan,\n",
    "    columns=[\n",
    "        \"Eps\",\n",
    "        \"Min Samples\",\n",
    "        \"Silhouette\",\n",
    "        \"Calinski Harabasz\",\n",
    "        \"Davies Bouldin\",\n",
    "        \"Number of Clusters\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbscan.sort_values(\"Silhouette\", ascending=False).head(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick EPS 0.173684, Min Samples 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_coefs = []\n",
    "\n",
    "min_sample_points = [5, 10, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 42, 60, 70]\n",
    "for min_points in min_sample_points:\n",
    "    db = DBSCAN(eps=0.100, min_samples=min_points, n_jobs=-1)\n",
    "    db.fit(data)\n",
    "    silhouette_coefs.append(silhouette_score(data, db.labels_))\n",
    "\n",
    "plt.figure(figsize=(6, 3), dpi=120)\n",
    "plt.plot(\n",
    "    min_sample_points, silhouette_coefs, color=\"r\", marker=\".\"\n",
    ")\n",
    "plt.title(\n",
    "    \"SILHOUETTE SCORE - MINIMUM CLUSTERS SAMPLES\",\n",
    "    fontsize=9,\n",
    "    color=\"dimgrey\",\n",
    "    fontweight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.xlabel(\n",
    "    \"MINIMUM SAMPLES OF EACH CLUSTERS\",\n",
    "    color=\"dimgrey\",\n",
    "    labelpad=5,\n",
    "    fontweight=\"bold\",\n",
    "    fontsize=6,\n",
    ")\n",
    "plt.ylabel(\"SILHOUETTE SCORE\", color=\"dimgrey\", fontweight=\"bold\", fontsize=6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN instance\n",
    "dbscan = DBSCAN(eps=0.045678, min_samples=30, n_jobs=-1).fit(data)\n",
    "\n",
    "# Getting the cluster labels\n",
    "dbscan_labels = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present\n",
    "n_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise_ = list(dbscan_labels).count(-1)\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "df_bscan = features_df.copy()\n",
    "# Append DBSCAN clusters result to Dataframe\n",
    "df_bscan[\"cluster\"] = dbscan_labels\n",
    "\n",
    "# Quantity of each cluster\n",
    "df_bscan[\"cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_general(df_bscan, xycol, f\"clust_dbscan{n_clusters_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(df_bscan[0:],hue='cluster')\n",
    "fig.savefig(f\"trials/v_{version}/scatter_images/pairplot_DB.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    # Plot DBscan clusters with the 2 PCAs\n",
    "    df_pca2_db = df_pca2.copy()\n",
    "    df_pca3_db = df_pca3.copy()\n",
    "\n",
    "    # Dataframe for two components\n",
    "    df_pca2_db = pd.DataFrame(data=df_pca2, columns=[f\"PCA{i+1}\" for i in range(n_comp_2)])\n",
    "    df_pca3_db = pd.DataFrame(data=df_pca3, columns=[f\"PCA{i+1}\" for i in range(n_comp_3)])\n",
    "\n",
    "    # Append cluster labels to the Dataframe\n",
    "    df_pca2_db = pd.concat([df_pca2_db, pd.DataFrame({\"cluster\": dbscan_labels})], axis=1)\n",
    "    df_pca3_db = pd.concat([df_pca3_db, pd.DataFrame({\"cluster\": dbscan_labels})], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    plot_general(df_pca2_db, [(0, 1)], f\"dbscan_PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    fig = px.scatter_3d(df_pca3_db,x=\"PCA1\",y=\"PCA2\",z=\"PCA3\",\n",
    "                        color='cluster',\n",
    "                        title=f'KMeans {n_clust} ')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final DBSCAN model\n",
    "# Metrics\n",
    "metrics = []\n",
    "\n",
    "dbscan = DBSCAN(eps=0.18, min_samples=25).fit(data)\n",
    "pred = dbscan.labels_\n",
    "\n",
    "model = \"DBSCAN\"\n",
    "n_clusters = 3\n",
    "silhouette = silhouette_score(data, pred)\n",
    "calinski_harabasz = calinski_harabasz_score(data, pred)\n",
    "davies_bouldin = davies_bouldin_score(data, pred)\n",
    "\n",
    "metrics.append((model, n_clusters, silhouette, calinski_harabasz, davies_bouldin))\n",
    "\n",
    "final_df_db = pd.DataFrame(\n",
    "    metrics,\n",
    "    columns=[\n",
    "        \"Model\",\n",
    "        \"Clusters\",\n",
    "        \"Silhouette\",\n",
    "        \"Calinski Harabasz\",\n",
    "        \"Davies Bouldin\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "final_df_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********START Hierarchical Clustering METHOD*******************\n",
    "dend = linkage(data, \"ward\")\n",
    "plt.axhline(y=16, color=\"black\", linestyle=\"--\")\n",
    "dendrogram(dend)\n",
    "\n",
    "plt.savefig(f\"trials/v_{version}/scatter_images/Dendo.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_kelbow_metric(\n",
    "    data,\n",
    "    \"\",\n",
    "    \"\",\n",
    "    [\"distortion\", \"silhouette\", \"calinski_harabasz\"],\n",
    "    \"AgglomerativeClustering\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_scores_df = metrics_table(\n",
    "    range(2, 8), data, \"hc\", [\"silhouette\", \"calinski_harabasz\", \"davies_bouldin\"]\n",
    ")\n",
    "hc_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclusters = 3\n",
    "hc = AgglomerativeClustering(n_clusters=nclusters)\n",
    "\n",
    "# Fit the model to the data\n",
    "hc.fit(data)\n",
    "\n",
    "# Prints the labels for each example\n",
    "hc_labels = hc.labels_\n",
    "\n",
    "df_hc = features_df.copy()\n",
    "df_hc[\"cluster\"] = hc_labels\n",
    "\n",
    "plot_general(df_hc, xycol, f\"Clust_HC{nclusters}\", (20, 30), 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.pairplot(df_hc[0:],hue='cluster')\n",
    "fig.savefig(f\"trials/v_{version}/scatter_images/pairplot_HC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter_3d(df_hc,x=\"actual_vs_expected_percdiff\",y=\"touches_count_freq\",z=\"noAlert_freq\",\n",
    "                    color='cluster',\n",
    "                    title=f'KMeans 3 ')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    # Plot DBscan clusters with the 2 PCAs\n",
    "    df_pca2_hc = df_pca2.copy()\n",
    "    df_pca3_hc = df_pca3.copy()\n",
    "\n",
    "    # Dataframe for two components\n",
    "    df_pca2_hc = pd.DataFrame(data=df_pca2, columns=[f\"PCA{i+1}\" for i in range(n_comp_2)])\n",
    "    df_pca3_hc = pd.DataFrame(data=df_pca3, columns=[f\"PCA{i+1}\" for i in range(n_comp_3)])\n",
    "\n",
    "    # Append cluster labels to the Dataframe\n",
    "    df_pca2_hc = pd.concat([df_pca2_hc, pd.DataFrame({\"cluster\": hc_labels})], axis=1)\n",
    "    df_pca3_hc = pd.concat([df_pca3_hc, pd.DataFrame({\"cluster\": hc_labels})], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:\n",
    "    plot_general(df_pca2_hc, [(0, 1)], f\"dbscan_HC_PCA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_PCA:    \n",
    "    fig = px.scatter_3d(df_pca3_hc,x=\"PCA1\",y=\"PCA2\",z=\"PCA3\",\n",
    "                        color='cluster',\n",
    "                        title=f'KMeans {n_clust} ')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Hierarchical Clustering Model\n",
    "final_hc_clusters = 3\n",
    "\n",
    "final_df_hc = metrics_table(\n",
    "    range(final_hc_clusters, final_hc_clusters + 1),\n",
    "    data,\n",
    "    \"hc\",\n",
    "    [\"silhouette\", \"calinski_harabasz\", \"davies_bouldin\"],\n",
    ")\n",
    "\n",
    "final_df_hc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********Final Results**************\n",
    "# Comparison table between algorithms\n",
    "# df_comp = pd.concat([final_df_kmn, final_df_db, final_df_hc])\n",
    "df_comp = pd.concat([final_df_kmn, final_df_hc])\n",
    "df_comp.sort_values(\"Silhouette\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_lables = dbscan.labels_\n",
    "# final_lables = kmeans4.labels_\n",
    "final_lables = kmeans3.labels_\n",
    "# final_lables = kmeans2.labels_\n",
    "\n",
    "# final_lables = hc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_original.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_use = [\n",
    "    \"currently_active_False\",\n",
    "    \"currently_active_True\",\n",
    "    \"model_save_freq\",\n",
    "    \"model_build_freq\",\n",
    "    \"model_score_avg\",\n",
    "    \"noAlert_freq\",\n",
    "    \"model_age\",\n",
    "    \"actual_vs_expected_percdiff\",\n",
    "    \"upper_vs_actual_percdiff\",\n",
    "    \"lower_vs_actual_percdiff\",\n",
    "    'case_count',\n",
    "    \"case_count_freq\",\n",
    "    \"touches_count_freq\",\n",
    "    \"modeltype_APR\",\n",
    "    \"modeltype_Fixed Limit\",\n",
    "    \"modeltype_Rolling Average\",\n",
    "    \"site_FHR-CC\",\n",
    "    \"site_FHR-PB\",\n",
    "    \"site_Pipelines and Terminals\",\n",
    "    \"site_unknown\",\n",
    "    \"position_class_group_EP\",\n",
    "    \"position_class_group_FP-COMP_VALVE_PIPE_OTHER\",\n",
    "    \"position_class_group_FP-EXCH\",\n",
    "    \"position_class_group_FP-FIRED\",\n",
    "    \"position_class_group_FP-tank-vessel\",\n",
    "    \"position_class_group_IP-LOOPS_XMTR_OTHER\",\n",
    "    \"position_class_group_IP-VALVE\",\n",
    "    \"position_class_group_MP-COMP\",\n",
    "    \"position_class_group_MP-FAN_TURBN_GRBX_OTHER\",\n",
    "    \"position_class_group_MP-PUMP\",\n",
    "    \"position_class_group_OP_P\",\n",
    "    'position_class_group_unknown'\n",
    "]\n",
    "\n",
    "df_clean_original = df_clean_original[col_to_use].copy()\n",
    "\n",
    "# Append the final result of the clusters to the original Dataframe\n",
    "df_clean_original['cluster'] = final_lables\n",
    "\n",
    "\n",
    "case_count_df = df_clean_original.groupby(\"cluster\", group_keys=False)[\n",
    "        [\"case_count\"]\n",
    "    ].sum()\n",
    "case_count_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "plt.rcParams[\"xtick.labelsize\"] = 8\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "total = float(df_clean_original.shape[0])\n",
    "cores = [\"#8da0cb\", \"#fc8d62\", \"#66c2a5\"]\n",
    "sns.set_palette(sns.color_palette(cores))\n",
    "ax = sns.countplot(\n",
    "    x=\"cluster\",\n",
    "    data=df_clean_original,\n",
    "    order=df_clean_original[\"cluster\"].value_counts().index,\n",
    ")\n",
    "sns.despine()\n",
    "plt.title(\"CLUSTERS DISTRIBUTION\", fontsize=9, color=\"dimgrey\", fontweight=\"bold\")\n",
    "for p in ax.patches:\n",
    "    percentage = \"{:.1f}%\".format(100 * p.get_height() / total)\n",
    "    x = p.get_x() + p.get_width()\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y), ha=\"center\")\n",
    "plt.xlabel(\"CLUSTERS\", color=\"dimgrey\", labelpad=5, fontweight=\"bold\", fontsize=7)\n",
    "plt.ylabel(\"COUNT\", color=\"dimgrey\", fontweight=\"bold\", fontsize=7)\n",
    "plt.savefig(f\"trials/v_{version}/results/bar-cluster_distribution.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting columns by clusters\n",
    "plt.figure(figsize=(20, 120))\n",
    "x = 1\n",
    "for d in df_clean_original:\n",
    "    if not d == \"cluster\":\n",
    "        plt.subplot(((df_clean_original.shape[1] // 2) + 1), 2, x)\n",
    "        sns.kdeplot(data=df_clean_original, x=d, hue=\"cluster\", palette=\"Set2\")\n",
    "        x += 1\n",
    "\n",
    "plt.savefig(f\"trials/v_{version}/results/kdeplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_clean_original:\n",
    "    if not c == \"cluster\":\n",
    "        grid = sns.FacetGrid(df_clean_original, col=\"cluster\").map(plt.hist, c)\n",
    "        plt.savefig(f\"trials/v_{version}/results/hist_{c}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of clusters by columns\n",
    "plt.figure(figsize=(20, 100))\n",
    "x = 1\n",
    "for d in df_clean_original:\n",
    "    if not d == \"cluster\":\n",
    "        plt.subplot(((df_clean_original.shape[1] // 2) + 1), 3, x)\n",
    "        sns.stripplot(data=df_clean_original, x=\"cluster\", y=d, palette=\"Set2\")\n",
    "        x += 1\n",
    "\n",
    "plt.savefig(f\"trials/v_{version}/results/stripplot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_plt = df_clean_original.columns.tolist()\n",
    "\n",
    "for i in range(len(col_to_plt)):\n",
    "    if not col_to_plt[i] == \"cluster\":\n",
    "        plot_bars_box(df_clean_original, col_to_plt[i], f\"kmeans_clust-{n_clust}_{i}\")\n",
    "\n",
    "plot_bars_box(case_count_df, 'case_count', f\"kmeans_clust-{n_clust}_caseCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# sns.jointplot(x=df_clean_original[i], y=data[\"Spent\"], hue =data[\"Clusters\"], kind=\"kde\", palette=pal)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# sns.jointplot(x=df_clean_original[\"model_age\"], y=df_clean_original[\"model_score_avg\"], hue=df_clean_original[\"cluster\"], kind=\"kde\")\n",
    "\n",
    "\n",
    "\n",
    "# col_to_plt = df_clean_original.columns.tolist()\n",
    "\n",
    "# for i in range(len(col_to_plt)):\n",
    "#     if not col_to_plt[i] == \"cluster\":\n",
    "#         plt.figure()\n",
    "#         sns.jointplot(x=df_clean_original[i], y=df_clean_original[\"model_score_avg\"], hue =data[\"Clusters\"], kind=\"kde\")\n",
    "        # plot_bars_box(df_clean_original, col_to_plt[i], f\"kmeans_clust-{n_clust}_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfddfb6c7bd0c938935be39c4b0e4a8fb77352881edfeed0e8c2bea19a1c4275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
