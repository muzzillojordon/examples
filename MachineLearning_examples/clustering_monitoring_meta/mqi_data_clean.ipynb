{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in files that were created from mqi_data_import.ipynb\n",
    "model_actions = pd.read_pickle(\"mqi_data_combined/model_actions_combined.pkl\")\n",
    "model_state = pd.read_pickle(\"mqi_data_combined/model_state_combined.pkl\")\n",
    "model_cases = pd.read_pickle(\"mqi_data_combined/model_cases_combined.pkl\")\n",
    "model_info = pd.read_pickle(\"mqi_data_combined/model_info_combined.pkl\")\n",
    "\n",
    "#read in file that was created from other_data_stuff.ipynb\n",
    "model_info_addedcols = pd.read_pickle(\n",
    "    \"mqi_data_clean/model_info_clean_added_col_3.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********START MODEL INFO *******************\n",
    "model_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_modelinfo_deactivate_rows():\n",
    "    \"\"\"    sets 'active' field in table. this value will be used for all other tables to know when a model was active or not.\n",
    "    active status will == false for all past records if model is not currently active.\n",
    "    case 1: current ingestion_ts active == true\n",
    "        action make all entries active == true\n",
    "    case 2: current ingestion_ts active == false\n",
    "        2.1: there is an active == true before any null values\n",
    "            action: make all entries after the active == true ==> active == true\n",
    "            make all entries before the active == true ==> active == false\n",
    "        2.2: there are no active == true before any null values\n",
    "            we know that the model is currently deactivate or deleted, there must be a save in order to make a deactivate or deleted, \n",
    "            therefore last save date is point where model was deactivate or deleted\n",
    "            action find last save point and drop any rows after that ingestion point\n",
    "    Returns:\n",
    "        list, dataframe: list of indexs of rows to remove from model_info Df were a model was not active, DF containing current value of active for each model\n",
    "    \"\"\"\n",
    "    # NOTE: older dates are less than newer dates: ie. dates more in past < dates closer to now\n",
    "    \n",
    "    #make all \"active\" = null rows false, we do not know for sure the state but if set to false it will fall into group of models to determine current state\n",
    "    model_info[\"active\"].fillna(False, inplace=True)\n",
    "    # separate out CASE 1: all rows that currently active, we know all these will models will have active == true\n",
    "    model_info[\"current_active_value\"] = model_info.groupby(\"modelid\")[\"active\"].transform(\n",
    "        \"first\"\n",
    "    )\n",
    "    # DF for only models who are not active now\n",
    "    cur_set_false = model_info.loc[model_info[\"current_active_value\"] != True].copy()\n",
    "\n",
    "    # keep only required columns, we made all null rows == False, (this will let us pull out True rows with .max())\n",
    "    cur_set_false = cur_set_false[[\"modelid\", \"lastsavetime\", \"ingestion_ts\", \"active\"]]\n",
    "\n",
    "    # add column for every row with the newest(nearest to now) last save date for each model ID\n",
    "    # cur_set_false will be helpfull for applying same deactivate criteria to all tables\n",
    "    cur_set_false[\"newest_lastsavedate\"] = cur_set_false.groupby(\"modelid\")[\n",
    "        \"lastsavetime\"\n",
    "    ].transform(\"max\")\n",
    "\n",
    "    cur_set_false_unique = cur_set_false.groupby(\"modelid\")[\n",
    "        [\"newest_lastsavedate\"]\n",
    "    ].max()\n",
    "\n",
    "    return (\n",
    "        cur_set_false[\n",
    "            cur_set_false[\"ingestion_ts\"] >= cur_set_false[\"newest_lastsavedate\"]\n",
    "        ].index.tolist(),\n",
    "        cur_set_false_unique,\n",
    "    )\n",
    "\n",
    "\n",
    "# sort ingestion date and reset index\n",
    "model_info.sort_values(by=\"ingestion_ts\", inplace=True, ascending=False)\n",
    "model_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# drop all indexes that ingestion date >= last save date (we know model is currently not active and save must be done to deactivate so date of last save is deactivation point)\n",
    "print(model_info.shape)\n",
    "(index_to_drop, cur_deactivate_df) = drop_modelinfo_deactivate_rows()\n",
    "model_info.drop(index=index_to_drop, inplace=True)\n",
    "\n",
    "#any rows left are active models, make all rows status active\n",
    "model_info['active'] = True\n",
    "\n",
    "\n",
    "#add Model model_info_addedcols to Model Info\n",
    "model_info = model_info.merge(\n",
    "        model_info_addedcols[\n",
    "            [\n",
    "                \"position_code\",\n",
    "                \"position_description\",\n",
    "                \"site\",\n",
    "                \"position_class\",\n",
    "            ]\n",
    "        ],\n",
    "        left_on=\"modelid\",\n",
    "        right_on=\"modelid\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "print(model_info.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modinfo_clean_unbuiltmodels():\n",
    "    \"\"\"\n",
    "    Looking at the rows with no lastbuildtime and comparing them to model state table, it looks these are not good models,\n",
    "    most the values for actual / expected ..etc are 0. the belief is that they are models that were added but had no data to backfill and were never fully trained\n",
    "    action: remove model IDs that have no build status \n",
    "    Returns:\n",
    "        List: of indexes of models with null build dates to drop\n",
    "    \"\"\"\n",
    "    #check null build date for model info\n",
    "    modinfo_null_build = model_info.loc[pd.isna(model_info[\"lastbuildtime\"])]\n",
    "    modinfo_null_build_lst = modinfo_null_build[\"modelid\"].unique()\n",
    "    \n",
    "    return modinfo_null_build.index.tolist()\n",
    "\n",
    "\n",
    "# First check rows that are missing build status, if the model has any build status, if model has a build status use the newest last build\n",
    "model_info[\"maxLastBuild\"] = model_info.groupby(\"modelid\")[\"lastbuildtime\"].transform(\n",
    "    \"max\"\n",
    ")\n",
    "model_info[\"lastbuildtime\"] = model_info[\"lastbuildtime\"].fillna(\n",
    "    model_info[\"maxLastBuild\"]\n",
    ")\n",
    "model_info.drop(columns=[\"maxLastBuild\"], inplace=True)\n",
    "\n",
    "\n",
    "#any models that are left with no build status have never had one and remove them\n",
    "print(model_info.shape)\n",
    "model_info.drop(index=modinfo_clean_unbuiltmodels(), inplace=True)\n",
    "print(model_info.shape)\n",
    "print(f\"after drop null last build date: {model_info['lastbuildtime'].isna().sum()}\")\n",
    "\n",
    "# ***********END MODEL INFO*******************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********START MODEL STATES *******************\n",
    "model_state.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows where model ID not found model info table\n",
    "print(model_state.shape)\n",
    "model_state = model_state.loc[\n",
    "    model_state[\"modelid\"].isin(model_info[\"modelid\"].unique().tolist())\n",
    "]\n",
    "print(model_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add deactivate date column, date from model_info table: cur_deactivate_df[\"newest_lastsavedate\"] is the date model became deactivate\n",
    "model_state[\"deleted_date\"] = np.nan\n",
    "model_state[\"deleted_date\"] = model_state[\"modelid\"].map(\n",
    "    cur_deactivate_df[\"newest_lastsavedate\"]\n",
    ")\n",
    "\n",
    "print(model_state.shape)\n",
    "# note null vales in compare will be false. adding 1 day to deleted date BC data table in datalake have slightly different ingestion times\n",
    "index_to_drop = model_state[\n",
    "    model_state[\"ingestion_ts\"] >= (model_state[\"deleted_date\"] - pd.Timedelta(1, unit='d'))\n",
    "].index.tolist()\n",
    "model_state.drop(index=index_to_drop, inplace=True)\n",
    "print(model_state.shape)\n",
    "\n",
    "# drop deleted_date column, we do not need it anymore\n",
    "model_state.drop(columns=[\"deleted_date\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure data type correct\n",
    "model_state[\"activealerts\"] = model_state[\"activealerts\"].astype(\"string\")\n",
    "\n",
    "# fill active alert values with no active alerts with a value\n",
    "model_state[\"activealerts\"] = model_state[\"activealerts\"].replace(\n",
    "    r\"^\\s*$\", \"noAlert\", regex=True\n",
    ")\n",
    "\n",
    "# check how close number of records are between info and state (should be pretty close)\n",
    "print(f\"Difference between model ID in model state vs model info: {len(model_state['modelid'].unique()) - len(model_info['modelid'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows in Difference between model ID in model state vs model info. looking at rows they are single rows, that have all zeros for actual and expected values.\n",
    "print(model_info.shape)\n",
    "model_info = model_info.loc[\n",
    "    model_info[\"modelid\"].isin(model_state[\"modelid\"].unique().tolist())\n",
    "]\n",
    "print(model_info.shape)\n",
    "print(f\"Difference between model ID in model state vs model info: {len(model_state['modelid'].unique()) - len(model_info['modelid'].unique())}\")\n",
    "\n",
    "# ***********END MODEL STATES *******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = pd.read_pickle(\"MQI_dataLake_raw/model_actions_compress_1-2.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********START MODEL ACTIONS*******************\n",
    "model_actions.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_actions.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows not found model info table\n",
    "print(model_actions.shape)\n",
    "model_actions = model_actions.loc[\n",
    "    model_actions[\"modelid\"].isin(model_info[\"modelid\"].unique().tolist())\n",
    "]\n",
    "print(model_actions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Types\n",
    "model_actions[[\"actionnote\"]] = model_actions[[\"actionnote\"]].astype(\"string\")\n",
    "\n",
    "# add empty string for rows with no comments\n",
    "model_actions.loc[pd.isna(model_actions[\"actionnote\"]), [\"actionnote\"]] = \"\"\n",
    "model_actions.isna().sum()\n",
    "# ***********END MODEL ACTIONS*******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP ANY ACTIONS WHEN MODEL WAS NOT ACTIVE\n",
    "# add deactivate date column, date from model_info table: cur_deactivate_df[\"newest_lastsavedate\"] is the date model became deactivate\n",
    "model_actions[\"deleted_date\"] = np.nan\n",
    "model_actions[\"deleted_date\"] = model_actions[\"modelid\"].map(\n",
    "    cur_deactivate_df[\"newest_lastsavedate\"]\n",
    ")\n",
    "\n",
    "print(model_actions.shape)\n",
    "# note null vales in compare will be false. adding 1 day to deleted date BC data table in datalake have slightly different ingestion times\n",
    "index_to_drop = model_actions[\n",
    "    model_actions['changedate'] >= (model_actions[\"deleted_date\"] - pd.Timedelta(1, unit='d'))\n",
    "].index.tolist()\n",
    "model_actions.drop(index=index_to_drop, inplace=True)\n",
    "print(model_actions.shape)\n",
    "\n",
    "# drop deleted_date column, we do not need it anymore\n",
    "model_actions.drop(columns=[\"deleted_date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***********START MODEL CASES *******************\n",
    "model_cases.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove cases we know were written from non Asset 360 models (1 month after last comment by CHRISTOPHER M CHRISMAN)\n",
    "print(model_cases.shape)\n",
    "model_cases = model_cases.loc[~((model_cases['casemgmt_created_by_name']=='CHRISTOPHER M CHRISMAN') & (model_cases['casemgmt_date_created'] > '2022-03-24T04:00:00.000'))]\n",
    "print(model_cases.shape)\n",
    "# ***********END MODEL CASES *******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!not efficient method to do this, but only need it one time, could make better if needed to run repeatedly\n",
    "'''\n",
    "    UPDATE MODEL FOR CHANGE MODEL TYPES\n",
    "    it was found that the model type may change over time (ie. starts as a fixed limit, then at some point changes to APR).\n",
    "    the consequence of this is a model score or other frequencies will not be accurate represented due to age being incorrect.\n",
    "    1333 models have changed types, for now remove these rows. but in future create function to find change date and add as new model\n",
    "    \n",
    "'''\n",
    "def check_status_change(model_info):\n",
    "    \"\"\"\n",
    "        CHECK HOW MANY MODEL IDs HAVE CHANGED TYPES\n",
    "    Returns:\n",
    "        dataframe, list: df model ids and count of unique model types, list of counts \n",
    "    \"\"\"\n",
    "    modelinfo_type = pd.DataFrame()\n",
    "    modelinfo_type[[\"modeltype_count\"]] = model_info.groupby(\"modelid\", group_keys=False)[[\"modeltype\"]].nunique()\n",
    "\n",
    "    #get list of model types\n",
    "    modeltype_lst = model_info['modeltype'].value_counts().index.tolist()\n",
    "\n",
    "    return modelinfo_type, modeltype_lst\n",
    "\n",
    "\n",
    "def get__dates_of_changes(modelinfo_type, modeltype_lst):\n",
    "    \"\"\"GET START AND END DATES FOR EACH MODEL TYPE CHANGE\n",
    "\n",
    "    Returns:\n",
    "        dataframe: of model id and each type change dates\n",
    "    \"\"\"\n",
    "    #get list of model IDs that have multiple model types\n",
    "    modelinfo_multtype = modelinfo_type.loc[modelinfo_type['modeltype_count']>1]\n",
    "    modinfo_multtype_lst = modelinfo_multtype.index.unique().tolist()\n",
    "\n",
    "    #get all models records that have multiple IDs\n",
    "    modinfo_multtyp_df = model_info.loc[model_info['modelid'].isin(modinfo_multtype_lst), ['modelid', 'modelname', 'modeltype', 'lastbuildtime', 'lastsavetime', 'ingestion_ts']]\n",
    "    modinfo_multtyp_df.sort_values(by=\"ingestion_ts\", inplace=True, ascending=False)\n",
    "\n",
    "    #for each model type, split out and get when model type started and ended for each type, then add to a new column modelType_start and modelType_end\n",
    "    for i in range(len(modeltype_lst)):\n",
    "        #find ingestion data when model type started and when model type was change\n",
    "        modinfo_multtyp_df.loc[modinfo_multtyp_df['modeltype']==modeltype_lst[i], f\"{modeltype_lst[i]}_start\"] = modinfo_multtyp_df.loc[\n",
    "                    modinfo_multtyp_df['modeltype']==modeltype_lst[i]].groupby(\"modelid\")[\"ingestion_ts\"].transform(\"min\")\n",
    "\n",
    "        modinfo_multtyp_df.loc[modinfo_multtyp_df['modeltype']==modeltype_lst[i], f\"{modeltype_lst[i]}_end\"] = modinfo_multtyp_df.loc[\n",
    "                    modinfo_multtyp_df['modeltype']==modeltype_lst[i]].groupby(\"modelid\")[\"ingestion_ts\"].transform(\"max\")\n",
    "\n",
    "    #group all models back together to get model ID to start and end time of each of its model types\n",
    "    modelinfo_type_updated = pd.DataFrame()\n",
    "    modelinfo_type_updated = modinfo_multtyp_df.groupby(\"modelid\", group_keys=False).first()\n",
    "\n",
    "    return modelinfo_type_updated\n",
    "\n",
    "\n",
    "def create_new_modelid(modelinfo_type_updated):\n",
    "    \"\"\"CREATE NEW MODEL ID FOR EACH MODEL TYPE CHANGE\n",
    "    loop each model type, check to see if row has value for start date if no, go to next model type, if yes create new model ID and add it to DF\n",
    "    \"\"\"\n",
    "    new_modid_toadd = pd.DataFrame()\n",
    "    for index, row in modelinfo_type_updated.iterrows():\n",
    "        for modtype in modeltype_lst:\n",
    "            if not pd.isna(row[f'{modtype}_start']):\n",
    "                newrow = pd.Series({'modeltype_start':row[f'{modtype}_start'] , 'modeltype_end': row[f'{modtype}_end'], 'old_modid': index})\n",
    "                newrow.name = f\"{index}_{modtype}\"\n",
    "                new_modid_toadd = pd.concat([new_modid_toadd, newrow.to_frame().T])\n",
    "\n",
    "    return new_modid_toadd\n",
    "\n",
    "def update_with_new_modelid(new_modid_toadd, df, time_offset=0):\n",
    "    \"\"\"Change model_info to new model IDs\n",
    "        get old model ID rows for each new model ID and change old model ID for those rows to new model ID\n",
    "    Args:\n",
    "        new_modid_toadd (_type_): _description_\n",
    "        df (_type_): _description_\n",
    "        time_offset (int, optional): _description_. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: same dataframe passed but with updated model ids.\n",
    "    \"\"\"\n",
    "    for index, row in new_modid_toadd.iterrows():\n",
    "        df.loc[(df['modelid']== row['old_modid']) & \n",
    "            (df['ingestion_ts'] >= row['modeltype_start']-pd.Timedelta(time_offset, unit='d')) & \n",
    "            (df['ingestion_ts'] <= row['modeltype_end']+pd.Timedelta(time_offset, unit='d')), 'modelid'] = index\n",
    "    return df\n",
    "\n",
    "\n",
    "#get models with changed types and list of all types of each model\n",
    "modelinfo_type, modeltype_lst = check_status_change(model_info)\n",
    "print(modelinfo_type['modeltype_count'].value_counts())\n",
    "\n",
    "#get start and end date of every type change for each model\n",
    "modelinfo_type_updated = get__dates_of_changes(modelinfo_type, modeltype_lst)\n",
    "\n",
    "#get dataframe of all new model IDs that need to be added\n",
    "new_modid_toadd = create_new_modelid(modelinfo_type_updated)\n",
    "\n",
    "#update model_info model_states, model_actions table with new model ids, and remove old model ids.\n",
    "model_info = update_with_new_modelid(new_modid_toadd, model_info.copy(), 0)\n",
    "model_state = update_with_new_modelid(new_modid_toadd, model_state.copy(), 1)\n",
    "model_actions = update_with_new_modelid(new_modid_toadd, model_actions.copy(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any model ids in model_states and model_actions not in model_info\n",
    "print(f\"model_info: {len(model_info['modelid'].unique())}, model ids in model States: {len(model_state['modelid'].unique())}, model ids in model actions: {len(model_actions['modelid'].unique())}\")\n",
    "\n",
    "model_state = model_state.loc[\n",
    "    model_state[\"modelid\"].isin(model_info[\"modelid\"].unique().tolist())\n",
    "]\n",
    "model_actions = model_actions.loc[\n",
    "    model_actions[\"modelid\"].isin(model_info[\"modelid\"].unique().tolist())\n",
    "]\n",
    "print(f\"model_info: {len(model_info['modelid'].unique())}, model ids in model States: {len(model_state['modelid'].unique())}, model ids in model actions: {len(model_actions['modelid'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small difference about 15 models, drop them to make sure everything lines up\n",
    "print(model_info.shape)\n",
    "model_info = model_info.loc[\n",
    "    model_info[\"modelid\"].isin(model_state[\"modelid\"].unique().tolist())\n",
    "]\n",
    "print(model_info.shape)\n",
    "print(f\"Difference between model ID in model state vs model info: {len(model_state['modelid'].unique()) - len(model_info['modelid'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info.to_pickle(\"mqi_data_clean/model_info_clean_cpy.pkl\")\n",
    "model_actions.to_pickle(\"mqi_data_clean/model_actions_clean_cpy.pkl\")\n",
    "model_state.to_pickle(\"mqi_data_clean/model_state_clean_cpy.pkl\")\n",
    "model_cases.to_pickle(\"mqi_data_clean/model_cases_clean.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfddfb6c7bd0c938935be39c4b0e4a8fb77352881edfeed0e8c2bea19a1c4275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
