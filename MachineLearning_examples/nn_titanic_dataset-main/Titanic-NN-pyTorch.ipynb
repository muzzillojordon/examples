{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor, optim\n",
    "from torch.utils.data import TensorDataset, dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  cuda\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"CUDA Available: \", dev)\n",
    "\n",
    "\n",
    "path = Path(os.getcwd()).parent.parent\n",
    "path = path/'data/Titanic'\n",
    "path\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#read Data\n",
    "data = pd.read_csv(path/'train.csv')\n",
    "data.head()\n",
    "dataCopy = data.copy()\n",
    "dataCopy.isna().sum()\n",
    "\n",
    "#fill null columns:\n",
    "modes = dataCopy.mode().iloc[0]\n",
    "dataCopy.fillna(modes, inplace=True)\n",
    "dataCopy.isna().sum()\n",
    "\n",
    "#create Dummy Columsn for non numaric columns:\n",
    "#get non-numeric Columns:\n",
    "dataCopy.describe(include=[object])\n",
    "\n",
    "dataCopy['LogFare'] = np.log(dataCopy['Fare']+1)\n",
    "dataCopy.head()\n",
    "\n",
    "dataCopy = pd.get_dummies(dataCopy, columns=[\"Sex\",\"Pclass\",\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indepn_col = dataCopy[['Age', 'SibSp', 'Parch', 'LogFare', 'Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n",
    "dpen_Col = dataCopy[['Survived']]\n",
    "# indepn_col.head()\n",
    "# dpen_Col.head()\n",
    "indepn_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n",
      "torch.Size([596, 12]) torch.Size([596, 1])\n",
      "torch.Size([295, 12]) torch.Size([295, 1])\n"
     ]
    }
   ],
   "source": [
    "#split into training data and validation data with SKlearn\n",
    "indepenTrain, indepenVal, dpenTrain, dpenVal = train_test_split(indepn_col, dpen_Col, test_size=0.33, random_state=42)\n",
    "\n",
    "#put pandasframes into Tensors:\n",
    "\n",
    "indepenTrainTens = torch.tensor(indepenTrain.values, device=dev, dtype=torch.float32)\n",
    "dpenTrainTens = torch.tensor(dpenTrain.values, device=dev, dtype=torch.float32)\n",
    "indepenValTens = torch.tensor(indepenVal.values, device=dev, dtype=torch.float32)\n",
    "dpenValTens = torch.tensor(dpenVal.values, device=dev, dtype=torch.float32)\n",
    "\n",
    "print(indepenTrainTens.dtype, dpenTrainTens.dtype)\n",
    "print(indepenTrainTens.shape, dpenTrainTens.shape)\n",
    "print(indepenValTens.shape, dpenValTens.shape)\n",
    "\n",
    "#put tensors into pytorch datasets\n",
    "train_ds = TensorDataset(indepenTrainTens, dpenTrainTens)\n",
    "valid_ds = TensorDataset(indepenValTens, dpenValTens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempx, TempY = train_ds[0:5]\n",
    "# print(tempx.dtype, TempY.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(12, 50)\n",
    "        self.layer2 = nn.Linear(50, 25)\n",
    "        self.layer3 = nn.Linear(25, 1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        layer1_out = F.relu(self.layer1(xb))\n",
    "        layer2_out = F.relu(self.layer2(layer1_out))\n",
    "        layer3_out = self.layer3(layer2_out)\n",
    "        sigmoid_out = self.sigmoid1(layer3_out)\n",
    "\n",
    "        return sigmoid_out\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    loss_list = []\n",
    "    validate_loss_list = []\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        running_num = 0\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            losses, nums = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            running_loss += losses*nums\n",
    "            running_num += nums\n",
    "        \n",
    "        trainingLoss = running_loss/running_num\n",
    "        loss_list.append(trainingLoss)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "           losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        \n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        validate_loss_list.append(val_loss)\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch} trainginLoss: {trainingLoss} ValidationLoss:{val_loss}')\n",
    "\n",
    "\n",
    "    return validate_loss_list, loss_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 trainginLoss: 0.738819633154261 ValidationLoss:0.721544521946018\n",
      "Epoch: 1 trainginLoss: 0.712376222114435 ValidationLoss:0.7029225022105847\n",
      "Epoch: 2 trainginLoss: 0.6946079066935802 ValidationLoss:0.691653627864385\n",
      "Epoch: 3 trainginLoss: 0.6834909047856427 ValidationLoss:0.6841449814327693\n",
      "Epoch: 4 trainginLoss: 0.6761639862252562 ValidationLoss:0.6795274423340619\n",
      "Epoch: 5 trainginLoss: 0.670770641541321 ValidationLoss:0.6768393362982799\n",
      "Epoch: 6 trainginLoss: 0.6676551363612181 ValidationLoss:0.6750326601125426\n",
      "Epoch: 7 trainginLoss: 0.6651903890923365 ValidationLoss:0.6739583872132382\n",
      "Epoch: 8 trainginLoss: 0.6637296644633248 ValidationLoss:0.6733471143043648\n",
      "Epoch: 9 trainginLoss: 0.6627753965006579 ValidationLoss:0.672955351764873\n",
      "Epoch: 10 trainginLoss: 0.6619730759787079 ValidationLoss:0.6727242665775751\n",
      "Epoch: 11 trainginLoss: 0.6615109431663616 ValidationLoss:0.6725636478197776\n",
      "Epoch: 12 trainginLoss: 0.6612976537454849 ValidationLoss:0.6724779405836332\n",
      "Epoch: 13 trainginLoss: 0.6607487425708131 ValidationLoss:0.672431002228947\n",
      "Epoch: 14 trainginLoss: 0.6604554521157437 ValidationLoss:0.6724019577947714\n",
      "Epoch: 15 trainginLoss: 0.6602908312874353 ValidationLoss:0.6724014712592302\n",
      "Epoch: 16 trainginLoss: 0.6600218227245663 ValidationLoss:0.6724064572382782\n",
      "Epoch: 17 trainginLoss: 0.6598587448164921 ValidationLoss:0.6724235312413361\n",
      "Epoch: 18 trainginLoss: 0.6597270333526919 ValidationLoss:0.6724752135195975\n",
      "Epoch: 19 trainginLoss: 0.659735594419825 ValidationLoss:0.6723987793518325\n",
      "Epoch: 20 trainginLoss: 0.6595246751836482 ValidationLoss:0.6723446062055685\n",
      "Epoch: 21 trainginLoss: 0.6593722281039961 ValidationLoss:0.6723158951533043\n",
      "Epoch: 22 trainginLoss: 0.6592190797696977 ValidationLoss:0.6722304253254907\n",
      "Epoch: 23 trainginLoss: 0.6591617985059751 ValidationLoss:0.6722033801725356\n",
      "Epoch: 24 trainginLoss: 0.6590303798649935 ValidationLoss:0.6720461899951353\n",
      "Epoch: 25 trainginLoss: 0.6588976075185226 ValidationLoss:0.6719625923593165\n",
      "Epoch: 26 trainginLoss: 0.6587427262491827 ValidationLoss:0.6719214386859182\n",
      "Epoch: 27 trainginLoss: 0.6588556570494735 ValidationLoss:0.6719530269251032\n",
      "Epoch: 28 trainginLoss: 0.6586812978622897 ValidationLoss:0.6717834189786749\n",
      "Epoch: 29 trainginLoss: 0.6586156631476127 ValidationLoss:0.6716424564183768\n",
      "Epoch: 30 trainginLoss: 0.6586458074966534 ValidationLoss:0.6717630467172396\n",
      "Epoch: 31 trainginLoss: 0.6585156361528691 ValidationLoss:0.6716436153751308\n",
      "Epoch: 32 trainginLoss: 0.6584090562474808 ValidationLoss:0.6715081560409675\n",
      "Epoch: 33 trainginLoss: 0.6582938240678519 ValidationLoss:0.6714797189680196\n",
      "Epoch: 34 trainginLoss: 0.6582525176489913 ValidationLoss:0.6714561230045254\n",
      "Epoch: 35 trainginLoss: 0.6582426332787379 ValidationLoss:0.6714432283983393\n",
      "Epoch: 36 trainginLoss: 0.6582451226727274 ValidationLoss:0.6712979633929366\n",
      "Epoch: 37 trainginLoss: 0.6582080701853605 ValidationLoss:0.6713337950787301\n",
      "Epoch: 38 trainginLoss: 0.6581572214228995 ValidationLoss:0.6711139325368203\n",
      "Epoch: 39 trainginLoss: 0.6579823761978405 ValidationLoss:0.671053077608852\n",
      "Epoch: 40 trainginLoss: 0.6578843733608323 ValidationLoss:0.671023886082536\n",
      "Epoch: 41 trainginLoss: 0.6578531381267829 ValidationLoss:0.670911864709046\n",
      "Epoch: 42 trainginLoss: 0.6580892133232731 ValidationLoss:0.6707748651504517\n",
      "Epoch: 43 trainginLoss: 0.6578444978534775 ValidationLoss:0.6708753725229684\n",
      "Epoch: 44 trainginLoss: 0.65774614058885 ValidationLoss:0.6708617786229667\n",
      "Epoch: 45 trainginLoss: 0.6576754690816738 ValidationLoss:0.670813775062561\n",
      "Epoch: 46 trainginLoss: 0.6578247267127837 ValidationLoss:0.6706580612619044\n",
      "Epoch: 47 trainginLoss: 0.6575294961065254 ValidationLoss:0.6706781288324777\n",
      "Epoch: 48 trainginLoss: 0.6574652963036659 ValidationLoss:0.6706016762781951\n",
      "Epoch: 49 trainginLoss: 0.6573676790967083 ValidationLoss:0.6705484493304107\n",
      "Epoch: 50 trainginLoss: 0.6573833767199676 ValidationLoss:0.6704310449503236\n",
      "Epoch: 51 trainginLoss: 0.6572677181071083 ValidationLoss:0.6704074392884465\n",
      "Epoch: 52 trainginLoss: 0.6573254702075216 ValidationLoss:0.6704338059586994\n",
      "Epoch: 53 trainginLoss: 0.6571954420748973 ValidationLoss:0.670269734576597\n",
      "Epoch: 54 trainginLoss: 0.6571632087630713 ValidationLoss:0.6702386862140591\n",
      "Epoch: 55 trainginLoss: 0.6572200263906646 ValidationLoss:0.6701351141525527\n",
      "Epoch: 56 trainginLoss: 0.6571334516442062 ValidationLoss:0.6701155834278818\n",
      "Epoch: 57 trainginLoss: 0.6570746334607169 ValidationLoss:0.669947283752894\n",
      "Epoch: 58 trainginLoss: 0.656932958420491 ValidationLoss:0.6699317447209763\n",
      "Epoch: 59 trainginLoss: 0.6568546839208411 ValidationLoss:0.6699150869401834\n",
      "Epoch: 60 trainginLoss: 0.6568328230172996 ValidationLoss:0.6698035187640432\n",
      "Epoch: 61 trainginLoss: 0.6567720790837435 ValidationLoss:0.6696834879406428\n",
      "Epoch: 62 trainginLoss: 0.6567233284847849 ValidationLoss:0.6696341611571232\n",
      "Epoch: 63 trainginLoss: 0.6566423133715688 ValidationLoss:0.6695171010696281\n",
      "Epoch: 64 trainginLoss: 0.6565766090514676 ValidationLoss:0.6694452984858368\n",
      "Epoch: 65 trainginLoss: 0.6565400101994509 ValidationLoss:0.6693287901959177\n",
      "Epoch: 66 trainginLoss: 0.6565068572159581 ValidationLoss:0.6692046593811553\n",
      "Epoch: 67 trainginLoss: 0.6564727349569334 ValidationLoss:0.6692253539117716\n",
      "Epoch: 68 trainginLoss: 0.6563075776868219 ValidationLoss:0.6692061735411822\n",
      "Epoch: 69 trainginLoss: 0.6563907629691514 ValidationLoss:0.6692638591184454\n",
      "Epoch: 70 trainginLoss: 0.6562420745824007 ValidationLoss:0.669177012120263\n",
      "Epoch: 71 trainginLoss: 0.6563612454689589 ValidationLoss:0.669244667433076\n",
      "Epoch: 72 trainginLoss: 0.6560955511643582 ValidationLoss:0.6692349741014384\n",
      "Epoch: 73 trainginLoss: 0.656026597391039 ValidationLoss:0.669224950418634\n",
      "Epoch: 74 trainginLoss: 0.6559667543276845 ValidationLoss:0.6691222546464306\n",
      "Epoch: 75 trainginLoss: 0.6560653764129485 ValidationLoss:0.6690400956040722\n",
      "Epoch: 76 trainginLoss: 0.6559191110150125 ValidationLoss:0.6690128247616655\n",
      "Epoch: 77 trainginLoss: 0.6559818615049324 ValidationLoss:0.669140060271247\n",
      "Epoch: 78 trainginLoss: 0.6558144908623408 ValidationLoss:0.6690696209163989\n",
      "Epoch: 79 trainginLoss: 0.6557109615946776 ValidationLoss:0.6689801735393072\n",
      "Epoch: 80 trainginLoss: 0.6557485077205121 ValidationLoss:0.668817386788837\n",
      "Epoch: 81 trainginLoss: 0.6558484311071818 ValidationLoss:0.6688114097562887\n",
      "Epoch: 82 trainginLoss: 0.6555940976878941 ValidationLoss:0.6686901836071985\n",
      "Epoch: 83 trainginLoss: 0.6554775998096338 ValidationLoss:0.6685856354438653\n",
      "Epoch: 84 trainginLoss: 0.6556776389979676 ValidationLoss:0.6685036760265545\n",
      "Epoch: 85 trainginLoss: 0.6554841031164131 ValidationLoss:0.6684376696408805\n",
      "Epoch: 86 trainginLoss: 0.6553951269828233 ValidationLoss:0.6683958863807937\n",
      "Epoch: 87 trainginLoss: 0.6553471368431245 ValidationLoss:0.6683252748796495\n",
      "Epoch: 88 trainginLoss: 0.6553172181116654 ValidationLoss:0.668255687164048\n",
      "Epoch: 89 trainginLoss: 0.6552960224599647 ValidationLoss:0.6682498347961296\n",
      "Epoch: 90 trainginLoss: 0.6556531954931732 ValidationLoss:0.6682087631548865\n",
      "Epoch: 91 trainginLoss: 0.6550934550746176 ValidationLoss:0.6680481003502668\n",
      "Epoch: 92 trainginLoss: 0.655187483201891 ValidationLoss:0.668029361458148\n",
      "Epoch: 93 trainginLoss: 0.6551589701799738 ValidationLoss:0.667888611252025\n",
      "Epoch: 94 trainginLoss: 0.6550794111802274 ValidationLoss:0.6677888260049335\n",
      "Epoch: 95 trainginLoss: 0.654907719000874 ValidationLoss:0.6676219380508035\n",
      "Epoch: 96 trainginLoss: 0.6548674950663675 ValidationLoss:0.6674884735527685\n",
      "Epoch: 97 trainginLoss: 0.6550007866533011 ValidationLoss:0.6674424118914847\n",
      "Epoch: 98 trainginLoss: 0.6547597710718245 ValidationLoss:0.6674228241888144\n",
      "Epoch: 99 trainginLoss: 0.6546570718688453 ValidationLoss:0.6673507563138412\n",
      "Epoch: 100 trainginLoss: 0.6545886073336505 ValidationLoss:0.6673432719909539\n",
      "Epoch: 101 trainginLoss: 0.6545217789259533 ValidationLoss:0.6673273092609341\n",
      "Epoch: 102 trainginLoss: 0.6545550023149324 ValidationLoss:0.6673351859642287\n",
      "Epoch: 103 trainginLoss: 0.6545415500666472 ValidationLoss:0.6673520300347926\n",
      "Epoch: 104 trainginLoss: 0.6543711379870473 ValidationLoss:0.6672736089108354\n",
      "Epoch: 105 trainginLoss: 0.6543372069429231 ValidationLoss:0.667177457324529\n",
      "Epoch: 106 trainginLoss: 0.6543951918614791 ValidationLoss:0.6671928912906323\n",
      "Epoch: 107 trainginLoss: 0.654399964633404 ValidationLoss:0.6671859636145123\n",
      "Epoch: 108 trainginLoss: 0.6542126820391456 ValidationLoss:0.6672331167479693\n",
      "Epoch: 109 trainginLoss: 0.6543141451458003 ValidationLoss:0.6670451412766667\n",
      "Epoch: 110 trainginLoss: 0.654033307261115 ValidationLoss:0.6669938905764434\n",
      "Epoch: 111 trainginLoss: 0.6540575599510398 ValidationLoss:0.6669082134456957\n",
      "Epoch: 112 trainginLoss: 0.6540595477059383 ValidationLoss:0.6668003902596943\n",
      "Epoch: 113 trainginLoss: 0.6541330742355961 ValidationLoss:0.6666207964137449\n",
      "Epoch: 114 trainginLoss: 0.6539187607349165 ValidationLoss:0.6666054881225199\n",
      "Epoch: 115 trainginLoss: 0.6539778765415986 ValidationLoss:0.6664858456385337\n",
      "Epoch: 116 trainginLoss: 0.653757354157083 ValidationLoss:0.6664646445694616\n",
      "Epoch: 117 trainginLoss: 0.6537825649216671 ValidationLoss:0.6664459113347329\n",
      "Epoch: 118 trainginLoss: 0.6537120446262744 ValidationLoss:0.6663437196763895\n",
      "Epoch: 119 trainginLoss: 0.6536939424156343 ValidationLoss:0.6662832377320629\n",
      "Epoch: 120 trainginLoss: 0.6536568207228743 ValidationLoss:0.6662172028573893\n",
      "Epoch: 121 trainginLoss: 0.6535806131842953 ValidationLoss:0.6661489145230439\n",
      "Epoch: 122 trainginLoss: 0.6537057105326812 ValidationLoss:0.6659903073714951\n",
      "Epoch: 123 trainginLoss: 0.6534458578832998 ValidationLoss:0.6660244040570017\n",
      "Epoch: 124 trainginLoss: 0.6533983777833465 ValidationLoss:0.6660037095263853\n",
      "Epoch: 125 trainginLoss: 0.653349151147292 ValidationLoss:0.6660334187038874\n",
      "Epoch: 126 trainginLoss: 0.653309929290874 ValidationLoss:0.6660862409462363\n",
      "Epoch: 127 trainginLoss: 0.6533236675614479 ValidationLoss:0.6660415235212294\n",
      "Epoch: 128 trainginLoss: 0.6532412251370066 ValidationLoss:0.665940093185942\n",
      "Epoch: 129 trainginLoss: 0.6531213385946799 ValidationLoss:0.665909037549617\n",
      "Epoch: 130 trainginLoss: 0.6530654426389093 ValidationLoss:0.665892470489114\n",
      "Epoch: 131 trainginLoss: 0.6530760754674874 ValidationLoss:0.6658155839321978\n",
      "Epoch: 132 trainginLoss: 0.6530332809326632 ValidationLoss:0.665779197216034\n",
      "Epoch: 133 trainginLoss: 0.653285854614821 ValidationLoss:0.6655897995172921\n",
      "Epoch: 134 trainginLoss: 0.6529412237589791 ValidationLoss:0.6656410453683239\n",
      "Epoch: 135 trainginLoss: 0.652886362683853 ValidationLoss:0.6656628600621628\n",
      "Epoch: 136 trainginLoss: 0.6528969819113712 ValidationLoss:0.6656099198228221\n",
      "Epoch: 137 trainginLoss: 0.6527520210150904 ValidationLoss:0.6655302975137355\n",
      "Epoch: 138 trainginLoss: 0.6527086224331952 ValidationLoss:0.6654733084015927\n",
      "Epoch: 139 trainginLoss: 0.6526577444684586 ValidationLoss:0.6654199365842141\n",
      "Epoch: 140 trainginLoss: 0.6526567911941733 ValidationLoss:0.6654533511501247\n",
      "Epoch: 141 trainginLoss: 0.6525861912925771 ValidationLoss:0.6654763318724551\n",
      "Epoch: 142 trainginLoss: 0.6525627862687079 ValidationLoss:0.6653715226609828\n",
      "Epoch: 143 trainginLoss: 0.6525030580143001 ValidationLoss:0.6652267965219789\n",
      "Epoch: 144 trainginLoss: 0.6524554363033116 ValidationLoss:0.6650980838274552\n",
      "Epoch: 145 trainginLoss: 0.6524823047970766 ValidationLoss:0.6651353712809288\n",
      "Epoch: 146 trainginLoss: 0.6524485113636759 ValidationLoss:0.6650602061869735\n",
      "Epoch: 147 trainginLoss: 0.6523230099838052 ValidationLoss:0.6650163939443685\n",
      "Epoch: 148 trainginLoss: 0.6523144745186671 ValidationLoss:0.6649489018876674\n",
      "Epoch: 149 trainginLoss: 0.6522781808904353 ValidationLoss:0.6648553472454265\n",
      "Epoch: 150 trainginLoss: 0.6522339470434508 ValidationLoss:0.664733587079129\n",
      "Epoch: 151 trainginLoss: 0.6521680291067034 ValidationLoss:0.6646910188561779\n",
      "Epoch: 152 trainginLoss: 0.6522078410091016 ValidationLoss:0.6645716681318768\n",
      "Epoch: 153 trainginLoss: 0.65219295424903 ValidationLoss:0.6645583896313684\n",
      "Epoch: 154 trainginLoss: 0.6519719394261405 ValidationLoss:0.6644980293209269\n",
      "Epoch: 155 trainginLoss: 0.6520466652492549 ValidationLoss:0.6643843424522271\n",
      "Epoch: 156 trainginLoss: 0.6519054978485875 ValidationLoss:0.6643994345503338\n",
      "Epoch: 157 trainginLoss: 0.6518622744803461 ValidationLoss:0.6643604145211689\n",
      "Epoch: 158 trainginLoss: 0.6518235366616473 ValidationLoss:0.6643887125839621\n",
      "Epoch: 159 trainginLoss: 0.6519616794266156 ValidationLoss:0.664348935878883\n",
      "Epoch: 160 trainginLoss: 0.6517038925382115 ValidationLoss:0.6643215401697967\n",
      "Epoch: 161 trainginLoss: 0.6516579409573702 ValidationLoss:0.6642889552197214\n",
      "Epoch: 162 trainginLoss: 0.6517061467138713 ValidationLoss:0.6643107521331917\n",
      "Epoch: 163 trainginLoss: 0.6517483500026217 ValidationLoss:0.6643508616140333\n",
      "Epoch: 164 trainginLoss: 0.6515723550879715 ValidationLoss:0.6641772518723698\n",
      "Epoch: 165 trainginLoss: 0.651740550994873 ValidationLoss:0.6639524003206674\n",
      "Epoch: 166 trainginLoss: 0.6515641092454028 ValidationLoss:0.6639348567542384\n",
      "Epoch: 167 trainginLoss: 0.6515037525420221 ValidationLoss:0.6638320569264686\n",
      "Epoch: 168 trainginLoss: 0.6514129642672186 ValidationLoss:0.6638595686120502\n",
      "Epoch: 169 trainginLoss: 0.6513504165931036 ValidationLoss:0.6637700200080872\n",
      "Epoch: 170 trainginLoss: 0.6513848448759757 ValidationLoss:0.6637574428218906\n",
      "Epoch: 171 trainginLoss: 0.6513498189465311 ValidationLoss:0.663819100897191\n",
      "Epoch: 172 trainginLoss: 0.6511484548549524 ValidationLoss:0.6637192158375756\n",
      "Epoch: 173 trainginLoss: 0.6512322077815165 ValidationLoss:0.6635206388214887\n",
      "Epoch: 174 trainginLoss: 0.6512169293909265 ValidationLoss:0.6635541384502993\n",
      "Epoch: 175 trainginLoss: 0.6511945724487305 ValidationLoss:0.6635532702429819\n",
      "Epoch: 176 trainginLoss: 0.6509644161134758 ValidationLoss:0.6635260707240994\n",
      "Epoch: 177 trainginLoss: 0.6510551139812342 ValidationLoss:0.6635153728016352\n",
      "Epoch: 178 trainginLoss: 0.6508578174066224 ValidationLoss:0.6634602130469629\n",
      "Epoch: 179 trainginLoss: 0.6510387339047937 ValidationLoss:0.6634538711127588\n",
      "Epoch: 180 trainginLoss: 0.6509493237373812 ValidationLoss:0.6634236723689709\n",
      "Epoch: 181 trainginLoss: 0.6507639812943119 ValidationLoss:0.6632899379326125\n",
      "Epoch: 182 trainginLoss: 0.6508207093149223 ValidationLoss:0.6631315097970477\n",
      "Epoch: 183 trainginLoss: 0.6505974291155002 ValidationLoss:0.6630767895003497\n",
      "Epoch: 184 trainginLoss: 0.650706135186573 ValidationLoss:0.6631620980925479\n",
      "Epoch: 185 trainginLoss: 0.6506604320250902 ValidationLoss:0.6631226476976427\n",
      "Epoch: 186 trainginLoss: 0.6505166368196474 ValidationLoss:0.6631259334289421\n",
      "Epoch: 187 trainginLoss: 0.6504424414378684 ValidationLoss:0.6630816049494985\n",
      "Epoch: 188 trainginLoss: 0.6504108929794107 ValidationLoss:0.6629723603442564\n",
      "Epoch: 189 trainginLoss: 0.6504676330009563 ValidationLoss:0.662909362073672\n",
      "Epoch: 190 trainginLoss: 0.6504725957876883 ValidationLoss:0.6627441650730068\n",
      "Epoch: 191 trainginLoss: 0.6503783240414306 ValidationLoss:0.6626683103836188\n",
      "Epoch: 192 trainginLoss: 0.6503244706448292 ValidationLoss:0.6626177252349207\n",
      "Epoch: 193 trainginLoss: 0.6502453364781885 ValidationLoss:0.6625719884694633\n",
      "Epoch: 194 trainginLoss: 0.6502187352052471 ValidationLoss:0.6624912015462325\n",
      "Epoch: 195 trainginLoss: 0.6500363593933566 ValidationLoss:0.6624490388369156\n",
      "Epoch: 196 trainginLoss: 0.6500120542993482 ValidationLoss:0.6623953128265122\n",
      "Epoch: 197 trainginLoss: 0.6499786684977128 ValidationLoss:0.6622967370485855\n",
      "Epoch: 198 trainginLoss: 0.6499642873770438 ValidationLoss:0.6622435907186088\n",
      "Epoch: 199 trainginLoss: 0.6499705394642465 ValidationLoss:0.6623079960629091\n",
      "Epoch: 200 trainginLoss: 0.6499425424825425 ValidationLoss:0.6622538946442685\n",
      "Epoch: 201 trainginLoss: 0.6498240064454559 ValidationLoss:0.6622647085432279\n",
      "Epoch: 202 trainginLoss: 0.6497286242126619 ValidationLoss:0.6621970994997832\n",
      "Epoch: 203 trainginLoss: 0.6497016585113218 ValidationLoss:0.662144280894328\n",
      "Epoch: 204 trainginLoss: 0.649791664725182 ValidationLoss:0.6620469610569841\n",
      "Epoch: 205 trainginLoss: 0.6495835849102711 ValidationLoss:0.6620567756184077\n",
      "Epoch: 206 trainginLoss: 0.6498578766848417 ValidationLoss:0.6618345923342948\n",
      "Epoch: 207 trainginLoss: 0.6495625024673922 ValidationLoss:0.6616973590042632\n",
      "Epoch: 208 trainginLoss: 0.6495551014906608 ValidationLoss:0.6617021255573984\n",
      "Epoch: 209 trainginLoss: 0.6494102774050412 ValidationLoss:0.6616941579317642\n",
      "Epoch: 210 trainginLoss: 0.6494132256347861 ValidationLoss:0.6615341730036978\n",
      "Epoch: 211 trainginLoss: 0.6493675428748931 ValidationLoss:0.6616013326887357\n",
      "Epoch: 212 trainginLoss: 0.6493717476025523 ValidationLoss:0.6615466073407965\n",
      "Epoch: 213 trainginLoss: 0.6492817793916535 ValidationLoss:0.6615701632984614\n",
      "Epoch: 214 trainginLoss: 0.6492269443185538 ValidationLoss:0.6615184854652922\n",
      "Epoch: 215 trainginLoss: 0.6492707813346146 ValidationLoss:0.6615097443936235\n",
      "Epoch: 216 trainginLoss: 0.649164508253136 ValidationLoss:0.6615148016961955\n",
      "Epoch: 217 trainginLoss: 0.6491020669873129 ValidationLoss:0.661444115638733\n",
      "Epoch: 218 trainginLoss: 0.6491342690967074 ValidationLoss:0.6613076280739347\n",
      "Epoch: 219 trainginLoss: 0.6491253372006769 ValidationLoss:0.6612444653349407\n",
      "Epoch: 220 trainginLoss: 0.6488880067063658 ValidationLoss:0.6611100364539583\n",
      "Epoch: 221 trainginLoss: 0.648906969384059 ValidationLoss:0.6610054961705611\n",
      "Epoch: 222 trainginLoss: 0.6489456507983624 ValidationLoss:0.6609263506986327\n",
      "Epoch: 223 trainginLoss: 0.6488015767711921 ValidationLoss:0.6607682862524259\n",
      "Epoch: 224 trainginLoss: 0.6487769288504683 ValidationLoss:0.6607461595939378\n",
      "Epoch: 225 trainginLoss: 0.6487575093371756 ValidationLoss:0.6606830962633683\n",
      "Epoch: 226 trainginLoss: 0.6487082142957905 ValidationLoss:0.6605850654133295\n",
      "Epoch: 227 trainginLoss: 0.6485546555295086 ValidationLoss:0.6606459567102335\n",
      "Epoch: 228 trainginLoss: 0.6487513480570493 ValidationLoss:0.6605919140880391\n",
      "Epoch: 229 trainginLoss: 0.648546858521916 ValidationLoss:0.6604517789210304\n",
      "Epoch: 230 trainginLoss: 0.6486121160071968 ValidationLoss:0.6604157193232391\n",
      "Epoch: 231 trainginLoss: 0.6483416753327287 ValidationLoss:0.6604372093233012\n",
      "Epoch: 232 trainginLoss: 0.6485553947871163 ValidationLoss:0.6602805913504908\n",
      "Epoch: 233 trainginLoss: 0.6484223100162992 ValidationLoss:0.660391097957805\n",
      "Epoch: 234 trainginLoss: 0.6485052580801433 ValidationLoss:0.6602208022343911\n",
      "Epoch: 235 trainginLoss: 0.6481518333390255 ValidationLoss:0.6601247233859563\n",
      "Epoch: 236 trainginLoss: 0.648209271414968 ValidationLoss:0.6601333460565341\n",
      "Epoch: 237 trainginLoss: 0.6480552162100005 ValidationLoss:0.6601438386965606\n",
      "Epoch: 238 trainginLoss: 0.6480119200360855 ValidationLoss:0.6600858975264986\n",
      "Epoch: 239 trainginLoss: 0.6479798123340479 ValidationLoss:0.6600297293420565\n",
      "Epoch: 240 trainginLoss: 0.6481842242631336 ValidationLoss:0.6599618473295438\n",
      "Epoch: 241 trainginLoss: 0.6478348282359591 ValidationLoss:0.6598889391301042\n",
      "Epoch: 242 trainginLoss: 0.6477975353298572 ValidationLoss:0.6597869074950784\n",
      "Epoch: 243 trainginLoss: 0.647812454892485 ValidationLoss:0.6596955899464882\n",
      "Epoch: 244 trainginLoss: 0.6476842457016042 ValidationLoss:0.6596333808818106\n",
      "Epoch: 245 trainginLoss: 0.6479242476040885 ValidationLoss:0.6595840607659291\n",
      "Epoch: 246 trainginLoss: 0.6475890534836174 ValidationLoss:0.659511163073071\n",
      "Epoch: 247 trainginLoss: 0.6475847899513757 ValidationLoss:0.6594218021732265\n",
      "Epoch: 248 trainginLoss: 0.6477493443745095 ValidationLoss:0.659477041737508\n",
      "Epoch: 249 trainginLoss: 0.6475353492986435 ValidationLoss:0.6595003835225509\n",
      "Epoch: 250 trainginLoss: 0.6474978723782021 ValidationLoss:0.6595410538932024\n",
      "Epoch: 251 trainginLoss: 0.6473984582312156 ValidationLoss:0.6595452951172651\n",
      "Epoch: 252 trainginLoss: 0.6473325134923794 ValidationLoss:0.6594401012032719\n",
      "Epoch: 253 trainginLoss: 0.6473891451054772 ValidationLoss:0.6594298312219523\n",
      "Epoch: 254 trainginLoss: 0.6473304929349246 ValidationLoss:0.6593761381456408\n",
      "Epoch: 255 trainginLoss: 0.6472955094087844 ValidationLoss:0.6593746094380395\n",
      "Epoch: 256 trainginLoss: 0.6471320710726233 ValidationLoss:0.6592411651449689\n",
      "Epoch: 257 trainginLoss: 0.6471444972409498 ValidationLoss:0.6591398469472336\n",
      "Epoch: 258 trainginLoss: 0.647190624435476 ValidationLoss:0.6591177453429012\n",
      "Epoch: 259 trainginLoss: 0.6471470518400205 ValidationLoss:0.6590578560101784\n",
      "Epoch: 260 trainginLoss: 0.6469897571826141 ValidationLoss:0.6589377746743671\n",
      "Epoch: 261 trainginLoss: 0.6469268930838412 ValidationLoss:0.6588636232634723\n",
      "Epoch: 262 trainginLoss: 0.6469520582448716 ValidationLoss:0.6588373584262396\n",
      "Epoch: 263 trainginLoss: 0.6468108552414299 ValidationLoss:0.6588166653099706\n",
      "Epoch: 264 trainginLoss: 0.6466890685510316 ValidationLoss:0.6587962485976139\n",
      "Epoch: 265 trainginLoss: 0.6467516298262065 ValidationLoss:0.6586379608865511\n",
      "Epoch: 266 trainginLoss: 0.6465778902873097 ValidationLoss:0.6585150019597199\n",
      "Epoch: 267 trainginLoss: 0.6469014130982776 ValidationLoss:0.6584488480778063\n",
      "Epoch: 268 trainginLoss: 0.6465965285397216 ValidationLoss:0.6585519554251331\n",
      "Epoch: 269 trainginLoss: 0.6467621126430947 ValidationLoss:0.6585991014868526\n",
      "Epoch: 270 trainginLoss: 0.6464955374698511 ValidationLoss:0.6584718744633562\n",
      "Epoch: 271 trainginLoss: 0.6464668276325968 ValidationLoss:0.6584786920224206\n",
      "Epoch: 272 trainginLoss: 0.6464311340511245 ValidationLoss:0.6582881470858041\n",
      "Epoch: 273 trainginLoss: 0.6463800660715807 ValidationLoss:0.6580791148088746\n",
      "Epoch: 274 trainginLoss: 0.6461970446093771 ValidationLoss:0.658076802350707\n",
      "Epoch: 275 trainginLoss: 0.6463409742252939 ValidationLoss:0.6581334538378958\n",
      "Epoch: 276 trainginLoss: 0.646199417194264 ValidationLoss:0.6581345105575303\n",
      "Epoch: 277 trainginLoss: 0.6461524111312508 ValidationLoss:0.658131241192252\n",
      "Epoch: 278 trainginLoss: 0.6461087817313688 ValidationLoss:0.6580389257204735\n",
      "Epoch: 279 trainginLoss: 0.6460615604515844 ValidationLoss:0.6581203282889673\n",
      "Epoch: 280 trainginLoss: 0.6459220895831217 ValidationLoss:0.6578955561427747\n",
      "Epoch: 281 trainginLoss: 0.6459929659062584 ValidationLoss:0.6579221159724866\n",
      "Epoch: 282 trainginLoss: 0.645808441926969 ValidationLoss:0.6578147678051964\n",
      "Epoch: 283 trainginLoss: 0.6457638048485621 ValidationLoss:0.6576497583066003\n",
      "Epoch: 284 trainginLoss: 0.6457597373315952 ValidationLoss:0.657623410022865\n",
      "Epoch: 285 trainginLoss: 0.6458242519589878 ValidationLoss:0.6575301738108619\n",
      "Epoch: 286 trainginLoss: 0.6457826287154383 ValidationLoss:0.6573554333993944\n",
      "Epoch: 287 trainginLoss: 0.6456351340217078 ValidationLoss:0.6573010984113661\n",
      "Epoch: 288 trainginLoss: 0.6457549405577999 ValidationLoss:0.6572332265013355\n",
      "Epoch: 289 trainginLoss: 0.6454230226926355 ValidationLoss:0.6571346579971961\n",
      "Epoch: 290 trainginLoss: 0.6453603574093556 ValidationLoss:0.6570631554571249\n",
      "Epoch: 291 trainginLoss: 0.6454008765668677 ValidationLoss:0.6570919578358279\n",
      "Epoch: 292 trainginLoss: 0.6453776267550936 ValidationLoss:0.6570746167231414\n",
      "Epoch: 293 trainginLoss: 0.6453789168556264 ValidationLoss:0.6569662189079544\n",
      "Epoch: 294 trainginLoss: 0.6452023971000774 ValidationLoss:0.6569185434761694\n",
      "Epoch: 295 trainginLoss: 0.6451959013938904 ValidationLoss:0.6567308025845027\n",
      "Epoch: 296 trainginLoss: 0.6452566469275711 ValidationLoss:0.6567670907004405\n",
      "Epoch: 297 trainginLoss: 0.6450974085186952 ValidationLoss:0.6566819302106308\n",
      "Epoch: 298 trainginLoss: 0.644997525935205 ValidationLoss:0.6566961617793067\n",
      "Epoch: 299 trainginLoss: 0.6450301664787651 ValidationLoss:0.6567390098410137\n",
      "Epoch: 300 trainginLoss: 0.644821013380217 ValidationLoss:0.6566481487225678\n",
      "Epoch: 301 trainginLoss: 0.644949921425557 ValidationLoss:0.656506985526974\n",
      "Epoch: 302 trainginLoss: 0.6448077987504486 ValidationLoss:0.6564844149654194\n",
      "Epoch: 303 trainginLoss: 0.6447618847725375 ValidationLoss:0.6564060661752346\n",
      "Epoch: 304 trainginLoss: 0.6446630242686944 ValidationLoss:0.6562721317097292\n",
      "Epoch: 305 trainginLoss: 0.6446146016952975 ValidationLoss:0.656132533186573\n",
      "Epoch: 306 trainginLoss: 0.6446261413945448 ValidationLoss:0.6560570710796421\n",
      "Epoch: 307 trainginLoss: 0.6444768301592577 ValidationLoss:0.6559093673350447\n",
      "Epoch: 308 trainginLoss: 0.6445175129295195 ValidationLoss:0.6559337078514745\n",
      "Epoch: 309 trainginLoss: 0.6443498142613661 ValidationLoss:0.6558665883743157\n",
      "Epoch: 310 trainginLoss: 0.6443065400891657 ValidationLoss:0.655770962117082\n",
      "Epoch: 311 trainginLoss: 0.6444257841814285 ValidationLoss:0.6556901188220008\n",
      "Epoch: 312 trainginLoss: 0.6443100775648284 ValidationLoss:0.6557445475610636\n",
      "Epoch: 313 trainginLoss: 0.6442738515418648 ValidationLoss:0.6556844749693144\n",
      "Epoch: 314 trainginLoss: 0.6441740237626453 ValidationLoss:0.6555484535330433\n",
      "Epoch: 315 trainginLoss: 0.6441888985217817 ValidationLoss:0.655433824304807\n",
      "Epoch: 316 trainginLoss: 0.6440293740906171 ValidationLoss:0.6554736854666371\n",
      "Epoch: 317 trainginLoss: 0.6441658375247213 ValidationLoss:0.6554024781210948\n",
      "Epoch: 318 trainginLoss: 0.6439066937305783 ValidationLoss:0.6553185283127477\n",
      "Epoch: 319 trainginLoss: 0.6439286894446251 ValidationLoss:0.6552736179303315\n",
      "Epoch: 320 trainginLoss: 0.6437881412922136 ValidationLoss:0.6552302322145236\n",
      "Epoch: 321 trainginLoss: 0.6438207162306613 ValidationLoss:0.655251993365207\n",
      "Epoch: 322 trainginLoss: 0.6437641290210238 ValidationLoss:0.6551446605536897\n",
      "Epoch: 323 trainginLoss: 0.6437083914775976 ValidationLoss:0.6551131333334971\n",
      "Epoch: 324 trainginLoss: 0.6437934173033542 ValidationLoss:0.6550312236203986\n",
      "Epoch: 325 trainginLoss: 0.6435870972255733 ValidationLoss:0.6549177248599165\n",
      "Epoch: 326 trainginLoss: 0.643480080486144 ValidationLoss:0.6549566412376145\n",
      "Epoch: 327 trainginLoss: 0.6434430243984964 ValidationLoss:0.6549462831626505\n",
      "Epoch: 328 trainginLoss: 0.6433762931983743 ValidationLoss:0.6548693871094009\n",
      "Epoch: 329 trainginLoss: 0.6433397027470121 ValidationLoss:0.6546866057282787\n",
      "Epoch: 330 trainginLoss: 0.6433800306896236 ValidationLoss:0.654566236269676\n",
      "Epoch: 331 trainginLoss: 0.6434689004149213 ValidationLoss:0.6544851798122212\n",
      "Epoch: 332 trainginLoss: 0.6430863914073713 ValidationLoss:0.6543882723581993\n",
      "Epoch: 333 trainginLoss: 0.643254976144573 ValidationLoss:0.6544651619458602\n",
      "Epoch: 334 trainginLoss: 0.6433093379807953 ValidationLoss:0.6543412988468752\n",
      "Epoch: 335 trainginLoss: 0.6429836906042675 ValidationLoss:0.6542122283224332\n",
      "Epoch: 336 trainginLoss: 0.6429923088758583 ValidationLoss:0.6542624522063691\n",
      "Epoch: 337 trainginLoss: 0.6428560906608632 ValidationLoss:0.6541133678565592\n",
      "Epoch: 338 trainginLoss: 0.642816549179538 ValidationLoss:0.653966212878793\n",
      "Epoch: 339 trainginLoss: 0.6427170942293717 ValidationLoss:0.6538912833747217\n",
      "Epoch: 340 trainginLoss: 0.6426733508206054 ValidationLoss:0.6538081152964447\n",
      "Epoch: 341 trainginLoss: 0.6427069966425032 ValidationLoss:0.6538565593250727\n",
      "Epoch: 342 trainginLoss: 0.642590949999406 ValidationLoss:0.6537102141622769\n",
      "Epoch: 343 trainginLoss: 0.6426538108979296 ValidationLoss:0.6538174101861857\n",
      "Epoch: 344 trainginLoss: 0.6423910948254118 ValidationLoss:0.6537315625255391\n",
      "Epoch: 345 trainginLoss: 0.6423671037558741 ValidationLoss:0.6535784115225581\n",
      "Epoch: 346 trainginLoss: 0.6424998188978873 ValidationLoss:0.6534128611370669\n",
      "Epoch: 347 trainginLoss: 0.6425366293663947 ValidationLoss:0.6535510422819751\n",
      "Epoch: 348 trainginLoss: 0.6421768977338036 ValidationLoss:0.653476893295676\n",
      "Epoch: 349 trainginLoss: 0.6422033045915949 ValidationLoss:0.653446874174021\n",
      "Epoch: 350 trainginLoss: 0.6420620583847865 ValidationLoss:0.6534073754892511\n",
      "Epoch: 351 trainginLoss: 0.6420424400560008 ValidationLoss:0.6532461166381835\n",
      "Epoch: 352 trainginLoss: 0.6420436041467141 ValidationLoss:0.653331684055975\n",
      "Epoch: 353 trainginLoss: 0.641953070691768 ValidationLoss:0.6532703314797353\n",
      "Epoch: 354 trainginLoss: 0.641927249879645 ValidationLoss:0.653162177336418\n",
      "Epoch: 355 trainginLoss: 0.6419577126534993 ValidationLoss:0.653006132982545\n",
      "Epoch: 356 trainginLoss: 0.641817488926369 ValidationLoss:0.6529378371723628\n",
      "Epoch: 357 trainginLoss: 0.6419580578804016 ValidationLoss:0.6527763431355105\n",
      "Epoch: 358 trainginLoss: 0.6416629604845239 ValidationLoss:0.6527653829526093\n",
      "Epoch: 359 trainginLoss: 0.6416281613727544 ValidationLoss:0.6527329630770926\n",
      "Epoch: 360 trainginLoss: 0.6415050061757133 ValidationLoss:0.652727288108761\n",
      "Epoch: 361 trainginLoss: 0.6414699834465181 ValidationLoss:0.6525249848931522\n",
      "Epoch: 362 trainginLoss: 0.6413792567765153 ValidationLoss:0.6525144536616438\n",
      "Epoch: 363 trainginLoss: 0.641302035958975 ValidationLoss:0.652458035541793\n",
      "Epoch: 364 trainginLoss: 0.6413241704838388 ValidationLoss:0.6523488978208122\n",
      "Epoch: 365 trainginLoss: 0.6412968519550042 ValidationLoss:0.6521778522911719\n",
      "Epoch: 366 trainginLoss: 0.6413263358525781 ValidationLoss:0.6522255440889779\n",
      "Epoch: 367 trainginLoss: 0.6410970763872134 ValidationLoss:0.6520826184143454\n",
      "Epoch: 368 trainginLoss: 0.6410919295861417 ValidationLoss:0.6519835393307573\n",
      "Epoch: 369 trainginLoss: 0.641113921699908 ValidationLoss:0.6520461929046502\n",
      "Epoch: 370 trainginLoss: 0.6410295171225631 ValidationLoss:0.6520895268957494\n",
      "Epoch: 371 trainginLoss: 0.6411191325059673 ValidationLoss:0.6521691009149713\n",
      "Epoch: 372 trainginLoss: 0.6408558783915219 ValidationLoss:0.6519736298060013\n",
      "Epoch: 373 trainginLoss: 0.6408839505790864 ValidationLoss:0.6519466444597406\n",
      "Epoch: 374 trainginLoss: 0.6406722400812495 ValidationLoss:0.6518977456173655\n",
      "Epoch: 375 trainginLoss: 0.6407537596337747 ValidationLoss:0.6516958559973766\n",
      "Epoch: 376 trainginLoss: 0.6406065565627693 ValidationLoss:0.6515519495737755\n",
      "Epoch: 377 trainginLoss: 0.640710933496488 ValidationLoss:0.6515023482047906\n",
      "Epoch: 378 trainginLoss: 0.6404079734879052 ValidationLoss:0.6513785927982654\n",
      "Epoch: 379 trainginLoss: 0.6406549403331424 ValidationLoss:0.6513776530653743\n",
      "Epoch: 380 trainginLoss: 0.6404327070153 ValidationLoss:0.6514820294865107\n",
      "Epoch: 381 trainginLoss: 0.6402324870128759 ValidationLoss:0.6514024494057995\n",
      "Epoch: 382 trainginLoss: 0.6402488402872277 ValidationLoss:0.6512266730858107\n",
      "Epoch: 383 trainginLoss: 0.6402156673021765 ValidationLoss:0.6512683324894663\n",
      "Epoch: 384 trainginLoss: 0.6402246067987992 ValidationLoss:0.6512539113982249\n",
      "Epoch: 385 trainginLoss: 0.6404852427092175 ValidationLoss:0.6512641039945312\n",
      "Epoch: 386 trainginLoss: 0.6399547837724622 ValidationLoss:0.6511596590785657\n",
      "Epoch: 387 trainginLoss: 0.6399537864947479 ValidationLoss:0.6510471731929456\n",
      "Epoch: 388 trainginLoss: 0.6400252508637089 ValidationLoss:0.6508682901576414\n",
      "Epoch: 389 trainginLoss: 0.6400251592565703 ValidationLoss:0.6508315953157716\n",
      "Epoch: 390 trainginLoss: 0.6398148036643163 ValidationLoss:0.650596674620095\n",
      "Epoch: 391 trainginLoss: 0.639904251434659 ValidationLoss:0.6504790340439748\n",
      "Epoch: 392 trainginLoss: 0.6396638991048672 ValidationLoss:0.6504603284900471\n",
      "Epoch: 393 trainginLoss: 0.639553316487562 ValidationLoss:0.6503791419126219\n",
      "Epoch: 394 trainginLoss: 0.6396325518620894 ValidationLoss:0.6502250127873178\n",
      "Epoch: 395 trainginLoss: 0.6395083197811305 ValidationLoss:0.650073164600437\n",
      "Epoch: 396 trainginLoss: 0.6395822767443304 ValidationLoss:0.6500942599975457\n",
      "Epoch: 397 trainginLoss: 0.6394516989688745 ValidationLoss:0.6500417844723847\n",
      "Epoch: 398 trainginLoss: 0.6393414595783157 ValidationLoss:0.6500949934377509\n",
      "Epoch: 399 trainginLoss: 0.6391578316688538 ValidationLoss:0.6500903002286361\n",
      "Epoch: 400 trainginLoss: 0.6391833508574722 ValidationLoss:0.6499427706508313\n",
      "Epoch: 401 trainginLoss: 0.63903342397421 ValidationLoss:0.6498416423797607\n",
      "Epoch: 402 trainginLoss: 0.6392495400153551 ValidationLoss:0.6497090937727589\n",
      "Epoch: 403 trainginLoss: 0.6389441314159624 ValidationLoss:0.649737474069757\n",
      "Epoch: 404 trainginLoss: 0.6388502108970745 ValidationLoss:0.6496077125355348\n",
      "Epoch: 405 trainginLoss: 0.6388673830352374 ValidationLoss:0.6494088106236215\n",
      "Epoch: 406 trainginLoss: 0.6389183930102611 ValidationLoss:0.6492213146161225\n",
      "Epoch: 407 trainginLoss: 0.6389740365463615 ValidationLoss:0.6493178917189776\n",
      "Epoch: 408 trainginLoss: 0.6388749936283035 ValidationLoss:0.649059992927616\n",
      "Epoch: 409 trainginLoss: 0.6385824352302807 ValidationLoss:0.6490074594142073\n",
      "Epoch: 410 trainginLoss: 0.6386542796288561 ValidationLoss:0.6489712975792966\n",
      "Epoch: 411 trainginLoss: 0.6384999360014129 ValidationLoss:0.6488494628566807\n",
      "Epoch: 412 trainginLoss: 0.6384875702377933 ValidationLoss:0.6488530084238214\n",
      "Epoch: 413 trainginLoss: 0.6382681579397829 ValidationLoss:0.6487937341302128\n",
      "Epoch: 414 trainginLoss: 0.6382496720992479 ValidationLoss:0.6486752049397614\n",
      "Epoch: 415 trainginLoss: 0.6382243829285539 ValidationLoss:0.6486783603490409\n",
      "Epoch: 416 trainginLoss: 0.6381065349450847 ValidationLoss:0.6486578337216782\n",
      "Epoch: 417 trainginLoss: 0.6383479701592618 ValidationLoss:0.648414092548823\n",
      "Epoch: 418 trainginLoss: 0.6379819120336699 ValidationLoss:0.6483559216483165\n",
      "Epoch: 419 trainginLoss: 0.6380982907026406 ValidationLoss:0.6484800403401003\n",
      "Epoch: 420 trainginLoss: 0.6379688174132533 ValidationLoss:0.6483802249876119\n",
      "Epoch: 421 trainginLoss: 0.6378661402119886 ValidationLoss:0.6484877160039999\n",
      "Epoch: 422 trainginLoss: 0.6379884985469332 ValidationLoss:0.6483229416911885\n",
      "Epoch: 423 trainginLoss: 0.6378885279565849 ValidationLoss:0.648218460002188\n",
      "Epoch: 424 trainginLoss: 0.6376668122790804 ValidationLoss:0.6480511198609562\n",
      "Epoch: 425 trainginLoss: 0.6375553479930699 ValidationLoss:0.6479596511792328\n",
      "Epoch: 426 trainginLoss: 0.6374842392518216 ValidationLoss:0.6478716569431757\n",
      "Epoch: 427 trainginLoss: 0.6375371181724856 ValidationLoss:0.6477855872299711\n",
      "Epoch: 428 trainginLoss: 0.6375950750888594 ValidationLoss:0.6476001264685292\n",
      "Epoch: 429 trainginLoss: 0.637393526982941 ValidationLoss:0.6475683676994453\n",
      "Epoch: 430 trainginLoss: 0.6373518917384564 ValidationLoss:0.6476340562610303\n",
      "Epoch: 431 trainginLoss: 0.6372887732198574 ValidationLoss:0.647652314274998\n",
      "Epoch: 432 trainginLoss: 0.6372285309253923 ValidationLoss:0.6474947531344527\n",
      "Epoch: 433 trainginLoss: 0.6370094694547205 ValidationLoss:0.6473924390340255\n",
      "Epoch: 434 trainginLoss: 0.6371483294755821 ValidationLoss:0.6472829723762253\n",
      "Epoch: 435 trainginLoss: 0.6369657144450501 ValidationLoss:0.6473169112609605\n",
      "Epoch: 436 trainginLoss: 0.6371041296312473 ValidationLoss:0.6472636491565381\n",
      "Epoch: 437 trainginLoss: 0.6368085733996142 ValidationLoss:0.6470527828749963\n",
      "Epoch: 438 trainginLoss: 0.6368039974430263 ValidationLoss:0.6469648908760588\n",
      "Epoch: 439 trainginLoss: 0.6367065230471975 ValidationLoss:0.6468887605909573\n",
      "Epoch: 440 trainginLoss: 0.6366034494150405 ValidationLoss:0.6467512132757801\n",
      "Epoch: 441 trainginLoss: 0.6365747651797813 ValidationLoss:0.6467357679948968\n",
      "Epoch: 442 trainginLoss: 0.6364455451101265 ValidationLoss:0.6467241034669391\n",
      "Epoch: 443 trainginLoss: 0.6364008756291947 ValidationLoss:0.6466623011281935\n",
      "Epoch: 444 trainginLoss: 0.6363262822163985 ValidationLoss:0.6465066556203163\n",
      "Epoch: 445 trainginLoss: 0.6362950873854977 ValidationLoss:0.646452718872135\n",
      "Epoch: 446 trainginLoss: 0.6364074917447647 ValidationLoss:0.6462794023045039\n",
      "Epoch: 447 trainginLoss: 0.636417540127799 ValidationLoss:0.6460662033598302\n",
      "Epoch: 448 trainginLoss: 0.6362186198266561 ValidationLoss:0.6460511969307722\n",
      "Epoch: 449 trainginLoss: 0.6361869305572254 ValidationLoss:0.6461590112265894\n",
      "Epoch: 450 trainginLoss: 0.6362701498422046 ValidationLoss:0.6460907350152226\n",
      "Epoch: 451 trainginLoss: 0.6358994081516394 ValidationLoss:0.6461322356078585\n",
      "Epoch: 452 trainginLoss: 0.6358427293348632 ValidationLoss:0.645974677902157\n",
      "Epoch: 453 trainginLoss: 0.6359846011904262 ValidationLoss:0.6458423226566637\n",
      "Epoch: 454 trainginLoss: 0.6357331655969556 ValidationLoss:0.6459079132241717\n",
      "Epoch: 455 trainginLoss: 0.6356163464936634 ValidationLoss:0.6457765338784557\n",
      "Epoch: 456 trainginLoss: 0.6355809577359449 ValidationLoss:0.6458005947581792\n",
      "Epoch: 457 trainginLoss: 0.6354479549715183 ValidationLoss:0.6456901439165665\n",
      "Epoch: 458 trainginLoss: 0.6354299963720693 ValidationLoss:0.6457197730824099\n",
      "Epoch: 459 trainginLoss: 0.6354348399495119 ValidationLoss:0.6457557278164362\n",
      "Epoch: 460 trainginLoss: 0.6352740578203393 ValidationLoss:0.64550060720767\n",
      "Epoch: 461 trainginLoss: 0.6353502329563935 ValidationLoss:0.6454741817409709\n",
      "Epoch: 462 trainginLoss: 0.6353080728710098 ValidationLoss:0.6453149342941026\n",
      "Epoch: 463 trainginLoss: 0.6350917144109739 ValidationLoss:0.6452439823393095\n",
      "Epoch: 464 trainginLoss: 0.6350826357035029 ValidationLoss:0.6452660025176356\n",
      "Epoch: 465 trainginLoss: 0.6350657807900602 ValidationLoss:0.6450953978603169\n",
      "Epoch: 466 trainginLoss: 0.6351127792524811 ValidationLoss:0.64474134121911\n",
      "Epoch: 467 trainginLoss: 0.6349942568164544 ValidationLoss:0.6447779675661507\n",
      "Epoch: 468 trainginLoss: 0.6346876017199267 ValidationLoss:0.6447271941071849\n",
      "Epoch: 469 trainginLoss: 0.6346998462741007 ValidationLoss:0.6446527101225772\n",
      "Epoch: 470 trainginLoss: 0.634772439931063 ValidationLoss:0.6444295877117222\n",
      "Epoch: 471 trainginLoss: 0.634688351778376 ValidationLoss:0.6444185709549208\n",
      "Epoch: 472 trainginLoss: 0.6345088961940484 ValidationLoss:0.6444155161663637\n",
      "Epoch: 473 trainginLoss: 0.6344396599987209 ValidationLoss:0.6444496579089407\n",
      "Epoch: 474 trainginLoss: 0.6345391749535632 ValidationLoss:0.6442922969995919\n",
      "Epoch: 475 trainginLoss: 0.6343018732614966 ValidationLoss:0.6442167393231796\n",
      "Epoch: 476 trainginLoss: 0.6344261745478483 ValidationLoss:0.6442488322823735\n",
      "Epoch: 477 trainginLoss: 0.6342501248289275 ValidationLoss:0.6442952794543767\n",
      "Epoch: 478 trainginLoss: 0.6341261743699145 ValidationLoss:0.6439826237953316\n",
      "Epoch: 479 trainginLoss: 0.6339604918588728 ValidationLoss:0.6439364504005949\n",
      "Epoch: 480 trainginLoss: 0.6338977409689218 ValidationLoss:0.6438397136785217\n",
      "Epoch: 481 trainginLoss: 0.6340506892876338 ValidationLoss:0.6437978857654636\n",
      "Epoch: 482 trainginLoss: 0.6338896711400691 ValidationLoss:0.6437766406495692\n",
      "Epoch: 483 trainginLoss: 0.6337687969207764 ValidationLoss:0.643637123754469\n",
      "Epoch: 484 trainginLoss: 0.633740544719184 ValidationLoss:0.643554490905697\n",
      "Epoch: 485 trainginLoss: 0.6337030521975268 ValidationLoss:0.6434299206329605\n",
      "Epoch: 486 trainginLoss: 0.6334784382941739 ValidationLoss:0.6433668069920297\n",
      "Epoch: 487 trainginLoss: 0.6336423778693948 ValidationLoss:0.6432531981144921\n",
      "Epoch: 488 trainginLoss: 0.6333486161776037 ValidationLoss:0.6431677858708269\n",
      "Epoch: 489 trainginLoss: 0.6333062668774752 ValidationLoss:0.6430190177287085\n",
      "Epoch: 490 trainginLoss: 0.6331823076177764 ValidationLoss:0.6428746997299841\n",
      "Epoch: 491 trainginLoss: 0.6331940769349169 ValidationLoss:0.6426936048572346\n",
      "Epoch: 492 trainginLoss: 0.6331848810183122 ValidationLoss:0.6425358911692086\n",
      "Epoch: 493 trainginLoss: 0.6331784441167077 ValidationLoss:0.6424491765135426\n",
      "Epoch: 494 trainginLoss: 0.6329844706010499 ValidationLoss:0.6424245983867322\n",
      "Epoch: 495 trainginLoss: 0.6329127602129174 ValidationLoss:0.6424266079724845\n",
      "Epoch: 496 trainginLoss: 0.6329727296861226 ValidationLoss:0.642503599797265\n",
      "Epoch: 497 trainginLoss: 0.6328386012339752 ValidationLoss:0.6423073978747351\n",
      "Epoch: 498 trainginLoss: 0.6329035966988378 ValidationLoss:0.6421811406895266\n",
      "Epoch: 499 trainginLoss: 0.6326084252971931 ValidationLoss:0.6420861965518887\n",
      "Epoch: 500 trainginLoss: 0.6325256788490603 ValidationLoss:0.6420835634409371\n",
      "Epoch: 501 trainginLoss: 0.6326177728256123 ValidationLoss:0.6420651306540279\n",
      "Epoch: 502 trainginLoss: 0.6324220711752873 ValidationLoss:0.6418586205627959\n",
      "Epoch: 503 trainginLoss: 0.6325609339963669 ValidationLoss:0.6417944683866986\n",
      "Epoch: 504 trainginLoss: 0.6322887739879173 ValidationLoss:0.6416373784259214\n",
      "Epoch: 505 trainginLoss: 0.6321165961707198 ValidationLoss:0.6415574835518659\n",
      "Epoch: 506 trainginLoss: 0.6321156116940031 ValidationLoss:0.641465142823882\n",
      "Epoch: 507 trainginLoss: 0.6322916078087467 ValidationLoss:0.6412303762920832\n",
      "Epoch: 508 trainginLoss: 0.6320885799875195 ValidationLoss:0.6412250947144071\n",
      "Epoch: 509 trainginLoss: 0.6318620015310761 ValidationLoss:0.6411972884404458\n",
      "Epoch: 510 trainginLoss: 0.6318372824048036 ValidationLoss:0.6411606400699938\n",
      "Epoch: 511 trainginLoss: 0.6317824929352575 ValidationLoss:0.6411967350264727\n",
      "Epoch: 512 trainginLoss: 0.631843043653757 ValidationLoss:0.6410962306847007\n",
      "Epoch: 513 trainginLoss: 0.6316718431127152 ValidationLoss:0.640977002604533\n",
      "Epoch: 514 trainginLoss: 0.6316689020835313 ValidationLoss:0.6410529215457076\n",
      "Epoch: 515 trainginLoss: 0.631420915158803 ValidationLoss:0.6409519438016212\n",
      "Epoch: 516 trainginLoss: 0.6314628520267922 ValidationLoss:0.6408708715842942\n",
      "Epoch: 517 trainginLoss: 0.6314934396103724 ValidationLoss:0.6407066995814695\n",
      "Epoch: 518 trainginLoss: 0.6313276974946861 ValidationLoss:0.6408060647673526\n",
      "Epoch: 519 trainginLoss: 0.631630569896442 ValidationLoss:0.6405676851838322\n",
      "Epoch: 520 trainginLoss: 0.6316653690082115 ValidationLoss:0.6404455756737014\n",
      "Epoch: 521 trainginLoss: 0.631093891275009 ValidationLoss:0.6402715949688927\n",
      "Epoch: 522 trainginLoss: 0.6310730840535772 ValidationLoss:0.6401811229980598\n",
      "Epoch: 523 trainginLoss: 0.6309315842270051 ValidationLoss:0.6402313173827479\n",
      "Epoch: 524 trainginLoss: 0.6307527134882523 ValidationLoss:0.6400476148573019\n",
      "Epoch: 525 trainginLoss: 0.6306556711260904 ValidationLoss:0.6399037920822531\n",
      "Epoch: 526 trainginLoss: 0.6309411469721954 ValidationLoss:0.6399759044081478\n",
      "Epoch: 527 trainginLoss: 0.6305545248441248 ValidationLoss:0.6398882174895981\n",
      "Epoch: 528 trainginLoss: 0.6305902868309277 ValidationLoss:0.6396612254239745\n",
      "Epoch: 529 trainginLoss: 0.6304370756917352 ValidationLoss:0.639719355510453\n",
      "Epoch: 530 trainginLoss: 0.6303861553236942 ValidationLoss:0.6395610710321846\n",
      "Epoch: 531 trainginLoss: 0.6302926000332673 ValidationLoss:0.6394096479577533\n",
      "Epoch: 532 trainginLoss: 0.6301972306014707 ValidationLoss:0.6392615310216354\n",
      "Epoch: 533 trainginLoss: 0.6302281594116416 ValidationLoss:0.6393614439641016\n",
      "Epoch: 534 trainginLoss: 0.6300358048221409 ValidationLoss:0.6393312533022993\n",
      "Epoch: 535 trainginLoss: 0.6299979910754517 ValidationLoss:0.6391556081125291\n",
      "Epoch: 536 trainginLoss: 0.6299927242650282 ValidationLoss:0.6390112575838122\n",
      "Epoch: 537 trainginLoss: 0.6298995590049948 ValidationLoss:0.6389163924475848\n",
      "Epoch: 538 trainginLoss: 0.6298504551785105 ValidationLoss:0.6388151415323807\n",
      "Epoch: 539 trainginLoss: 0.6298237634985239 ValidationLoss:0.6385940679049088\n",
      "Epoch: 540 trainginLoss: 0.6298291663195463 ValidationLoss:0.6385704713352656\n",
      "Epoch: 541 trainginLoss: 0.6295726383292435 ValidationLoss:0.6386081366215722\n",
      "Epoch: 542 trainginLoss: 0.6294888662011832 ValidationLoss:0.6383586439035707\n",
      "Epoch: 543 trainginLoss: 0.629393120740084 ValidationLoss:0.6383131946547557\n",
      "Epoch: 544 trainginLoss: 0.6294136279381362 ValidationLoss:0.6380451800459522\n",
      "Epoch: 545 trainginLoss: 0.6293739392453392 ValidationLoss:0.6381407759957395\n",
      "Epoch: 546 trainginLoss: 0.6291942768448952 ValidationLoss:0.63792354151354\n",
      "Epoch: 547 trainginLoss: 0.6290356608845243 ValidationLoss:0.6378516708390187\n",
      "Epoch: 548 trainginLoss: 0.629022971495686 ValidationLoss:0.6376927191928282\n",
      "Epoch: 549 trainginLoss: 0.6289218792179286 ValidationLoss:0.6376184156385519\n",
      "Epoch: 550 trainginLoss: 0.6290320190007255 ValidationLoss:0.6374925431558641\n",
      "Epoch: 551 trainginLoss: 0.6290066022200872 ValidationLoss:0.6376105474213423\n",
      "Epoch: 552 trainginLoss: 0.6287754682086458 ValidationLoss:0.6374236577648228\n",
      "Epoch: 553 trainginLoss: 0.6287294054191385 ValidationLoss:0.63729620202113\n",
      "Epoch: 554 trainginLoss: 0.6285434233262235 ValidationLoss:0.6371839468762026\n",
      "Epoch: 555 trainginLoss: 0.6286555284621732 ValidationLoss:0.6369729537074849\n",
      "Epoch: 556 trainginLoss: 0.628375220618792 ValidationLoss:0.6370382900965416\n",
      "Epoch: 557 trainginLoss: 0.6283596170028584 ValidationLoss:0.6369229987516242\n",
      "Epoch: 558 trainginLoss: 0.6283505586969772 ValidationLoss:0.6367605847827459\n",
      "Epoch: 559 trainginLoss: 0.6282991906940537 ValidationLoss:0.6368821598715702\n",
      "Epoch: 560 trainginLoss: 0.6281566051828781 ValidationLoss:0.6367254600686542\n",
      "Epoch: 561 trainginLoss: 0.6280556733176212 ValidationLoss:0.6367715613316681\n",
      "Epoch: 562 trainginLoss: 0.6279070821384456 ValidationLoss:0.6365831625663628\n",
      "Epoch: 563 trainginLoss: 0.6280910712760567 ValidationLoss:0.6365864097061804\n",
      "Epoch: 564 trainginLoss: 0.627870278070437 ValidationLoss:0.6362747410596428\n",
      "Epoch: 565 trainginLoss: 0.6279952198066967 ValidationLoss:0.6362318602658934\n",
      "Epoch: 566 trainginLoss: 0.6277515752203513 ValidationLoss:0.6360317915172901\n",
      "Epoch: 567 trainginLoss: 0.6276473210962027 ValidationLoss:0.6360115687725908\n",
      "Epoch: 568 trainginLoss: 0.6274203482090227 ValidationLoss:0.6360723966259068\n",
      "Epoch: 569 trainginLoss: 0.627357690526335 ValidationLoss:0.6358965255446353\n",
      "Epoch: 570 trainginLoss: 0.6274469598827747 ValidationLoss:0.6357296325392642\n",
      "Epoch: 571 trainginLoss: 0.6272382396179558 ValidationLoss:0.6355570522405334\n",
      "Epoch: 572 trainginLoss: 0.6270959181273543 ValidationLoss:0.6354660694882021\n",
      "Epoch: 573 trainginLoss: 0.6270402989931555 ValidationLoss:0.635441335985216\n",
      "Epoch: 574 trainginLoss: 0.6270102066481673 ValidationLoss:0.6355305835352105\n",
      "Epoch: 575 trainginLoss: 0.6268670170899205 ValidationLoss:0.6354825468386633\n",
      "Epoch: 576 trainginLoss: 0.6268380992364564 ValidationLoss:0.6353993937120599\n",
      "Epoch: 577 trainginLoss: 0.6267628429720066 ValidationLoss:0.6352069066742719\n",
      "Epoch: 578 trainginLoss: 0.6267134807253844 ValidationLoss:0.6350229540113675\n",
      "Epoch: 579 trainginLoss: 0.6270792128255703 ValidationLoss:0.6347314757815862\n",
      "Epoch: 580 trainginLoss: 0.6265435638843767 ValidationLoss:0.6348494368084406\n",
      "Epoch: 581 trainginLoss: 0.6268268919631139 ValidationLoss:0.6348172181743686\n",
      "Epoch: 582 trainginLoss: 0.6264908661778341 ValidationLoss:0.6348703020710056\n",
      "Epoch: 583 trainginLoss: 0.6262583636597499 ValidationLoss:0.6346354559316474\n",
      "Epoch: 584 trainginLoss: 0.6263206797158158 ValidationLoss:0.6343680195889231\n",
      "Epoch: 585 trainginLoss: 0.6261158721558999 ValidationLoss:0.6341961274712773\n",
      "Epoch: 586 trainginLoss: 0.6260437089324797 ValidationLoss:0.6342113393848225\n",
      "Epoch: 587 trainginLoss: 0.6259712800883607 ValidationLoss:0.6341493497460575\n",
      "Epoch: 588 trainginLoss: 0.6258100455239315 ValidationLoss:0.6341059834270154\n",
      "Epoch: 589 trainginLoss: 0.6258369908236817 ValidationLoss:0.6341151431455451\n",
      "Epoch: 590 trainginLoss: 0.6259737354797005 ValidationLoss:0.6341520163972499\n",
      "Epoch: 591 trainginLoss: 0.6256008332207699 ValidationLoss:0.6340336441993714\n",
      "Epoch: 592 trainginLoss: 0.6258274684816398 ValidationLoss:0.6338703961695655\n",
      "Epoch: 593 trainginLoss: 0.6255818405407387 ValidationLoss:0.6337637363854102\n",
      "Epoch: 594 trainginLoss: 0.6255212830217093 ValidationLoss:0.6334581558987246\n",
      "Epoch: 595 trainginLoss: 0.6252779648608009 ValidationLoss:0.6333637187036417\n",
      "Epoch: 596 trainginLoss: 0.6255419898353167 ValidationLoss:0.6333383576344636\n",
      "Epoch: 597 trainginLoss: 0.6251350225218191 ValidationLoss:0.6333779654260409\n",
      "Epoch: 598 trainginLoss: 0.6250293706887521 ValidationLoss:0.633333980430991\n",
      "Epoch: 599 trainginLoss: 0.6249749268461394 ValidationLoss:0.6330009601883969\n",
      "Epoch: 600 trainginLoss: 0.6248795958173355 ValidationLoss:0.6329829502913912\n",
      "Epoch: 601 trainginLoss: 0.6249540084160414 ValidationLoss:0.6328996549218388\n",
      "Epoch: 602 trainginLoss: 0.6248098259804232 ValidationLoss:0.6328259530714002\n",
      "Epoch: 603 trainginLoss: 0.624773551953719 ValidationLoss:0.632716942439645\n",
      "Epoch: 604 trainginLoss: 0.6247963661315458 ValidationLoss:0.6326905672833071\n",
      "Epoch: 605 trainginLoss: 0.6246222681647179 ValidationLoss:0.6326013348870358\n",
      "Epoch: 606 trainginLoss: 0.6243663302203953 ValidationLoss:0.6324925739886397\n",
      "Epoch: 607 trainginLoss: 0.62426015034618 ValidationLoss:0.6323344450885967\n",
      "Epoch: 608 trainginLoss: 0.6242093451871168 ValidationLoss:0.6322840935092862\n",
      "Epoch: 609 trainginLoss: 0.6245088929297941 ValidationLoss:0.6319470975358608\n",
      "Epoch: 610 trainginLoss: 0.6241173852210077 ValidationLoss:0.6318167577355595\n",
      "Epoch: 611 trainginLoss: 0.6238932645560911 ValidationLoss:0.6317183973425525\n",
      "Epoch: 612 trainginLoss: 0.6240185783213417 ValidationLoss:0.6316748051320092\n",
      "Epoch: 613 trainginLoss: 0.6238538682860816 ValidationLoss:0.6315259252564381\n",
      "Epoch: 614 trainginLoss: 0.6238022334623656 ValidationLoss:0.6315437858387575\n",
      "Epoch: 615 trainginLoss: 0.6237094586327572 ValidationLoss:0.6314981781830222\n",
      "Epoch: 616 trainginLoss: 0.6236576257936106 ValidationLoss:0.6316490553193174\n",
      "Epoch: 617 trainginLoss: 0.6237829131568038 ValidationLoss:0.6313676397679215\n",
      "Epoch: 618 trainginLoss: 0.6236107609416014 ValidationLoss:0.6313101946297338\n",
      "Epoch: 619 trainginLoss: 0.6232171326675671 ValidationLoss:0.6310919593956511\n",
      "Epoch: 620 trainginLoss: 0.6231423556404626 ValidationLoss:0.630929748688714\n",
      "Epoch: 621 trainginLoss: 0.6232485255138986 ValidationLoss:0.6307402578450866\n",
      "Epoch: 622 trainginLoss: 0.623158210876004 ValidationLoss:0.630617163140895\n",
      "Epoch: 623 trainginLoss: 0.6229455159014503 ValidationLoss:0.6304407388477002\n",
      "Epoch: 624 trainginLoss: 0.6228431263225991 ValidationLoss:0.630402952129558\n",
      "Epoch: 625 trainginLoss: 0.6229723671938749 ValidationLoss:0.6303347684569278\n",
      "Epoch: 626 trainginLoss: 0.6225928212172233 ValidationLoss:0.6303200442912215\n",
      "Epoch: 627 trainginLoss: 0.6226997103467083 ValidationLoss:0.6301702461000216\n",
      "Epoch: 628 trainginLoss: 0.622540066306223 ValidationLoss:0.6301098286095312\n",
      "Epoch: 629 trainginLoss: 0.6227345186591948 ValidationLoss:0.6302311012300394\n",
      "Epoch: 630 trainginLoss: 0.6227579828876777 ValidationLoss:0.6303175091743469\n",
      "Epoch: 631 trainginLoss: 0.622438780413378 ValidationLoss:0.6302568692271993\n",
      "Epoch: 632 trainginLoss: 0.6223157548264369 ValidationLoss:0.6299424787699166\n",
      "Epoch: 633 trainginLoss: 0.6221485121938206 ValidationLoss:0.629539468126782\n",
      "Epoch: 634 trainginLoss: 0.6219679413225827 ValidationLoss:0.6295300800921553\n",
      "Epoch: 635 trainginLoss: 0.6219770580330151 ValidationLoss:0.6293704657231347\n",
      "Epoch: 636 trainginLoss: 0.6218437908479831 ValidationLoss:0.6294215438729626\n",
      "Epoch: 637 trainginLoss: 0.6217940429713102 ValidationLoss:0.6293086662130841\n",
      "Epoch: 638 trainginLoss: 0.6220260442503347 ValidationLoss:0.6290910765276116\n",
      "Epoch: 639 trainginLoss: 0.6216106006763126 ValidationLoss:0.6288038265907159\n",
      "Epoch: 640 trainginLoss: 0.6216855645179749 ValidationLoss:0.628679344209574\n",
      "Epoch: 641 trainginLoss: 0.6213276802293406 ValidationLoss:0.6286612825878596\n",
      "Epoch: 642 trainginLoss: 0.6212316023423368 ValidationLoss:0.6285097079761958\n",
      "Epoch: 643 trainginLoss: 0.6213215101485284 ValidationLoss:0.6283747018393824\n",
      "Epoch: 644 trainginLoss: 0.6210814490414306 ValidationLoss:0.6284564169786744\n",
      "Epoch: 645 trainginLoss: 0.6209668461108367 ValidationLoss:0.628258836067329\n",
      "Epoch: 646 trainginLoss: 0.6211318761710353 ValidationLoss:0.6282504592911672\n",
      "Epoch: 647 trainginLoss: 0.6207710104500688 ValidationLoss:0.6281064348705744\n",
      "Epoch: 648 trainginLoss: 0.6208713890722134 ValidationLoss:0.6281323596582574\n",
      "Epoch: 649 trainginLoss: 0.620623068921518 ValidationLoss:0.6280246152716168\n",
      "Epoch: 650 trainginLoss: 0.6205338163663877 ValidationLoss:0.6279236809682038\n",
      "Epoch: 651 trainginLoss: 0.6204937432436335 ValidationLoss:0.62786542924784\n",
      "Epoch: 652 trainginLoss: 0.6206028953494641 ValidationLoss:0.6276956885547961\n",
      "Epoch: 653 trainginLoss: 0.6203657996734516 ValidationLoss:0.627398915614112\n",
      "Epoch: 654 trainginLoss: 0.6201614917524709 ValidationLoss:0.6272471197580887\n",
      "Epoch: 655 trainginLoss: 0.6202343350288851 ValidationLoss:0.6272811323909436\n",
      "Epoch: 656 trainginLoss: 0.620020831991362 ValidationLoss:0.6272699398509527\n",
      "Epoch: 657 trainginLoss: 0.619882824036899 ValidationLoss:0.6272723311084812\n",
      "Epoch: 658 trainginLoss: 0.6199295948815826 ValidationLoss:0.6272329968921209\n",
      "Epoch: 659 trainginLoss: 0.619713248822513 ValidationLoss:0.6268738354666759\n",
      "Epoch: 660 trainginLoss: 0.6196684857342867 ValidationLoss:0.6265502834724168\n",
      "Epoch: 661 trainginLoss: 0.6194672736545537 ValidationLoss:0.6265385585316157\n",
      "Epoch: 662 trainginLoss: 0.619466367983978 ValidationLoss:0.626492057210308\n",
      "Epoch: 663 trainginLoss: 0.6194600630926606 ValidationLoss:0.6265788045980163\n",
      "Epoch: 664 trainginLoss: 0.6192337278551703 ValidationLoss:0.6264537940591068\n",
      "Epoch: 665 trainginLoss: 0.6191698636784649 ValidationLoss:0.6264320480621467\n",
      "Epoch: 666 trainginLoss: 0.6190027806582867 ValidationLoss:0.6262282474566314\n",
      "Epoch: 667 trainginLoss: 0.6189404638021584 ValidationLoss:0.6259909128738662\n",
      "Epoch: 668 trainginLoss: 0.6188494155070926 ValidationLoss:0.6258225568270279\n",
      "Epoch: 669 trainginLoss: 0.6188772312746752 ValidationLoss:0.625956700413914\n",
      "Epoch: 670 trainginLoss: 0.6189265915211415 ValidationLoss:0.6256889793832423\n",
      "Epoch: 671 trainginLoss: 0.6186875732953117 ValidationLoss:0.6256881867424916\n",
      "Epoch: 672 trainginLoss: 0.618671978880095 ValidationLoss:0.6256837083121477\n",
      "Epoch: 673 trainginLoss: 0.618458036608344 ValidationLoss:0.6256005733700122\n",
      "Epoch: 674 trainginLoss: 0.6185841912391202 ValidationLoss:0.6255824891187377\n",
      "Epoch: 675 trainginLoss: 0.6185791924495825 ValidationLoss:0.6254540952585511\n",
      "Epoch: 676 trainginLoss: 0.6182719277055472 ValidationLoss:0.6251353241629519\n",
      "Epoch: 677 trainginLoss: 0.6180498608006727 ValidationLoss:0.6249285988888498\n",
      "Epoch: 678 trainginLoss: 0.6179113236049678 ValidationLoss:0.6249170050782672\n",
      "Epoch: 679 trainginLoss: 0.6178599340003609 ValidationLoss:0.6245516754813113\n",
      "Epoch: 680 trainginLoss: 0.6177385613422266 ValidationLoss:0.6244736527992507\n",
      "Epoch: 681 trainginLoss: 0.6181304286790374 ValidationLoss:0.6240895507699352\n",
      "Epoch: 682 trainginLoss: 0.6175455419009164 ValidationLoss:0.624087949526512\n",
      "Epoch: 683 trainginLoss: 0.6175077017521698 ValidationLoss:0.6239812267028679\n",
      "Epoch: 684 trainginLoss: 0.6177223920822144 ValidationLoss:0.6241918440592491\n",
      "Epoch: 685 trainginLoss: 0.6172267330572909 ValidationLoss:0.6239767563545098\n",
      "Epoch: 686 trainginLoss: 0.6171610795411487 ValidationLoss:0.6237074512546346\n",
      "Epoch: 687 trainginLoss: 0.6173126725542465 ValidationLoss:0.6235831834502139\n",
      "Epoch: 688 trainginLoss: 0.6171527576926571 ValidationLoss:0.6233342069690511\n",
      "Epoch: 689 trainginLoss: 0.6170299609235469 ValidationLoss:0.6232360300371202\n",
      "Epoch: 690 trainginLoss: 0.6168569442409797 ValidationLoss:0.6231643309027461\n",
      "Epoch: 691 trainginLoss: 0.6173039474743325 ValidationLoss:0.6229314228235665\n",
      "Epoch: 692 trainginLoss: 0.6168299293358054 ValidationLoss:0.6231096588959129\n",
      "Epoch: 693 trainginLoss: 0.6166447169028673 ValidationLoss:0.623087711051359\n",
      "Epoch: 694 trainginLoss: 0.6167184962522263 ValidationLoss:0.6231713792025033\n",
      "Epoch: 695 trainginLoss: 0.61650523083322 ValidationLoss:0.6230327000052241\n",
      "Epoch: 696 trainginLoss: 0.6162726939124549 ValidationLoss:0.622703878354218\n",
      "Epoch: 697 trainginLoss: 0.6161487662552187 ValidationLoss:0.622711118197037\n",
      "Epoch: 698 trainginLoss: 0.6161456280106666 ValidationLoss:0.6225949085364908\n",
      "Epoch: 699 trainginLoss: 0.6161087847396032 ValidationLoss:0.6225133815054166\n",
      "Epoch: 700 trainginLoss: 0.6157632718150248 ValidationLoss:0.6223921983929004\n",
      "Epoch: 701 trainginLoss: 0.615856577886031 ValidationLoss:0.6223499389018042\n",
      "Epoch: 702 trainginLoss: 0.61569421043332 ValidationLoss:0.6219676365286617\n",
      "Epoch: 703 trainginLoss: 0.6155764580573011 ValidationLoss:0.6216912661568593\n",
      "Epoch: 704 trainginLoss: 0.6155215129756287 ValidationLoss:0.6215035727468587\n",
      "Epoch: 705 trainginLoss: 0.6153986506014062 ValidationLoss:0.6213986273539268\n",
      "Epoch: 706 trainginLoss: 0.615445930685773 ValidationLoss:0.6212740516258498\n",
      "Epoch: 707 trainginLoss: 0.6151248992689504 ValidationLoss:0.6211405560121698\n",
      "Epoch: 708 trainginLoss: 0.6152461123146467 ValidationLoss:0.6211611978078293\n",
      "Epoch: 709 trainginLoss: 0.6149746007567284 ValidationLoss:0.6211215926428972\n",
      "Epoch: 710 trainginLoss: 0.6148076957504222 ValidationLoss:0.6209691694227316\n",
      "Epoch: 711 trainginLoss: 0.6146925379765914 ValidationLoss:0.6208051115779554\n",
      "Epoch: 712 trainginLoss: 0.6148074661325288 ValidationLoss:0.6205480613950956\n",
      "Epoch: 713 trainginLoss: 0.614756680174962 ValidationLoss:0.6204041072877787\n",
      "Epoch: 714 trainginLoss: 0.6146044171096494 ValidationLoss:0.6204376372240358\n",
      "Epoch: 715 trainginLoss: 0.6143630739026422 ValidationLoss:0.6204599342103732\n",
      "Epoch: 716 trainginLoss: 0.6142819443805105 ValidationLoss:0.6202638411926011\n",
      "Epoch: 717 trainginLoss: 0.6142723468326082 ValidationLoss:0.61997374498238\n",
      "Epoch: 718 trainginLoss: 0.6141007986644771 ValidationLoss:0.6199436660540306\n",
      "Epoch: 719 trainginLoss: 0.6142854990575137 ValidationLoss:0.6196053201869383\n",
      "Epoch: 720 trainginLoss: 0.6140567232298371 ValidationLoss:0.6197239621210906\n",
      "Epoch: 721 trainginLoss: 0.6139408702018277 ValidationLoss:0.6198232780068608\n",
      "Epoch: 722 trainginLoss: 0.6137757365335554 ValidationLoss:0.6196111517437434\n",
      "Epoch: 723 trainginLoss: 0.6138612391964701 ValidationLoss:0.6194188895872084\n",
      "Epoch: 724 trainginLoss: 0.6135701485128211 ValidationLoss:0.6195374177674116\n",
      "Epoch: 725 trainginLoss: 0.6133833923595864 ValidationLoss:0.6193540952973446\n",
      "Epoch: 726 trainginLoss: 0.613549729321627 ValidationLoss:0.6192718398773064\n",
      "Epoch: 727 trainginLoss: 0.6133625535356918 ValidationLoss:0.6191561913086197\n",
      "Epoch: 728 trainginLoss: 0.613043332259927 ValidationLoss:0.6190803404581748\n",
      "Epoch: 729 trainginLoss: 0.6130475537888955 ValidationLoss:0.6188484163607582\n",
      "Epoch: 730 trainginLoss: 0.61286988594388 ValidationLoss:0.6187285102019876\n",
      "Epoch: 731 trainginLoss: 0.6126925721264526 ValidationLoss:0.6185841968504049\n",
      "Epoch: 732 trainginLoss: 0.6127919054671422 ValidationLoss:0.6186451451253083\n",
      "Epoch: 733 trainginLoss: 0.6125829411833078 ValidationLoss:0.6183428742117801\n",
      "Epoch: 734 trainginLoss: 0.612559624966359 ValidationLoss:0.6179441559112678\n",
      "Epoch: 735 trainginLoss: 0.6122790082989124 ValidationLoss:0.6178497870089644\n",
      "Epoch: 736 trainginLoss: 0.6127685332458291 ValidationLoss:0.6175580283342782\n",
      "Epoch: 737 trainginLoss: 0.6124159017665274 ValidationLoss:0.6175382874779782\n",
      "Epoch: 738 trainginLoss: 0.6120135400119244 ValidationLoss:0.6175151800705214\n",
      "Epoch: 739 trainginLoss: 0.6119478076896412 ValidationLoss:0.6174377980878798\n",
      "Epoch: 740 trainginLoss: 0.6118117250852136 ValidationLoss:0.6172529362015805\n",
      "Epoch: 741 trainginLoss: 0.6117172097199716 ValidationLoss:0.6171192732907957\n",
      "Epoch: 742 trainginLoss: 0.6118556969117799 ValidationLoss:0.6171773839805086\n",
      "Epoch: 743 trainginLoss: 0.6114739231615258 ValidationLoss:0.617092531010256\n",
      "Epoch: 744 trainginLoss: 0.6115204899903112 ValidationLoss:0.6171569870690168\n",
      "Epoch: 745 trainginLoss: 0.611461977430638 ValidationLoss:0.6169548422603284\n",
      "Epoch: 746 trainginLoss: 0.6113414212361278 ValidationLoss:0.6170505117561857\n",
      "Epoch: 747 trainginLoss: 0.6114736511403283 ValidationLoss:0.6170568193419504\n",
      "Epoch: 748 trainginLoss: 0.6110734599548698 ValidationLoss:0.6166264689574807\n",
      "Epoch: 749 trainginLoss: 0.6115195271152778 ValidationLoss:0.6160182356834412\n",
      "Epoch: 750 trainginLoss: 0.6109934829225476 ValidationLoss:0.6162069181264457\n",
      "Epoch: 751 trainginLoss: 0.6107571049024595 ValidationLoss:0.6162509708081262\n",
      "Epoch: 752 trainginLoss: 0.6109112913176518 ValidationLoss:0.6160468309612598\n",
      "Epoch: 753 trainginLoss: 0.6106955145029414 ValidationLoss:0.6159055459297309\n",
      "Epoch: 754 trainginLoss: 0.6105184763069921 ValidationLoss:0.6156914531174352\n",
      "Epoch: 755 trainginLoss: 0.6101550783887005 ValidationLoss:0.6156261874457537\n",
      "Epoch: 756 trainginLoss: 0.6101198168408951 ValidationLoss:0.6154460361448385\n",
      "Epoch: 757 trainginLoss: 0.6099473762032169 ValidationLoss:0.615313743938834\n",
      "Epoch: 758 trainginLoss: 0.6100138509833573 ValidationLoss:0.615216534622645\n",
      "Epoch: 759 trainginLoss: 0.6099113829983961 ValidationLoss:0.6149545253333398\n",
      "Epoch: 760 trainginLoss: 0.6098495573805482 ValidationLoss:0.6146219429323229\n",
      "Epoch: 761 trainginLoss: 0.6097783358305092 ValidationLoss:0.6143987217191923\n",
      "Epoch: 762 trainginLoss: 0.6097571205772809 ValidationLoss:0.6143224742452977\n",
      "Epoch: 763 trainginLoss: 0.6096408975204365 ValidationLoss:0.6143032647795597\n",
      "Epoch: 764 trainginLoss: 0.6093265334231741 ValidationLoss:0.6141308481410398\n",
      "Epoch: 765 trainginLoss: 0.6092643885804503 ValidationLoss:0.6139773659786936\n",
      "Epoch: 766 trainginLoss: 0.6092518284016808 ValidationLoss:0.6138817082017155\n",
      "Epoch: 767 trainginLoss: 0.6089423030814869 ValidationLoss:0.6139217483795295\n",
      "Epoch: 768 trainginLoss: 0.6094745893606404 ValidationLoss:0.6136275364180742\n",
      "Epoch: 769 trainginLoss: 0.6088506063358896 ValidationLoss:0.6135711728516272\n",
      "Epoch: 770 trainginLoss: 0.6086346151044705 ValidationLoss:0.6134820861331487\n",
      "Epoch: 771 trainginLoss: 0.6086615508034725 ValidationLoss:0.6134390909793013\n",
      "Epoch: 772 trainginLoss: 0.6084439234445559 ValidationLoss:0.6134562506514081\n",
      "Epoch: 773 trainginLoss: 0.6083087405102365 ValidationLoss:0.6134265424841542\n",
      "Epoch: 774 trainginLoss: 0.6081727187105473 ValidationLoss:0.6133484646425409\n",
      "Epoch: 775 trainginLoss: 0.6081113755302942 ValidationLoss:0.613006439451444\n",
      "Epoch: 776 trainginLoss: 0.6081855989142553 ValidationLoss:0.6128668235520185\n",
      "Epoch: 777 trainginLoss: 0.6079119915930217 ValidationLoss:0.6124618299936844\n",
      "Epoch: 778 trainginLoss: 0.607908033284565 ValidationLoss:0.6125926581479735\n",
      "Epoch: 779 trainginLoss: 0.6076013881888166 ValidationLoss:0.6123151763010833\n",
      "Epoch: 780 trainginLoss: 0.6075677603683216 ValidationLoss:0.6123522758483887\n",
      "Epoch: 781 trainginLoss: 0.6073786496316027 ValidationLoss:0.6121973734790996\n",
      "Epoch: 782 trainginLoss: 0.6073243870031113 ValidationLoss:0.6120485469446344\n",
      "Epoch: 783 trainginLoss: 0.6072634662557769 ValidationLoss:0.6116928979501886\n",
      "Epoch: 784 trainginLoss: 0.6074579394103696 ValidationLoss:0.6114230018551067\n",
      "Epoch: 785 trainginLoss: 0.6069925543445869 ValidationLoss:0.6113560581611375\n",
      "Epoch: 786 trainginLoss: 0.6070495647872054 ValidationLoss:0.6111269269959402\n",
      "Epoch: 787 trainginLoss: 0.6069781408213929 ValidationLoss:0.6110811524471994\n",
      "Epoch: 788 trainginLoss: 0.6065607814980833 ValidationLoss:0.6109996098583027\n",
      "Epoch: 789 trainginLoss: 0.6065366348164194 ValidationLoss:0.6109491891780142\n",
      "Epoch: 790 trainginLoss: 0.6063587373535105 ValidationLoss:0.610688772848097\n",
      "Epoch: 791 trainginLoss: 0.6063540461879449 ValidationLoss:0.6106080374475252\n",
      "Epoch: 792 trainginLoss: 0.6066373566653105 ValidationLoss:0.6102470230248015\n",
      "Epoch: 793 trainginLoss: 0.6062104069946597 ValidationLoss:0.6101113931607391\n",
      "Epoch: 794 trainginLoss: 0.606252284258004 ValidationLoss:0.6101161221326408\n",
      "Epoch: 795 trainginLoss: 0.605943408588435 ValidationLoss:0.6101081702668788\n",
      "Epoch: 796 trainginLoss: 0.6057048527986412 ValidationLoss:0.6099056274203931\n",
      "Epoch: 797 trainginLoss: 0.6055667596375383 ValidationLoss:0.6098894828456943\n",
      "Epoch: 798 trainginLoss: 0.605512876238599 ValidationLoss:0.6096259509102773\n",
      "Epoch: 799 trainginLoss: 0.6057384206144601 ValidationLoss:0.6099484530545898\n",
      "Epoch: 800 trainginLoss: 0.605346411266583 ValidationLoss:0.6095173084129721\n",
      "Epoch: 801 trainginLoss: 0.6053676841243002 ValidationLoss:0.6091181251962307\n",
      "Epoch: 802 trainginLoss: 0.605204754627791 ValidationLoss:0.6090670557345375\n",
      "Epoch: 803 trainginLoss: 0.6050674063247322 ValidationLoss:0.6091653179314177\n",
      "Epoch: 804 trainginLoss: 0.6052233164742489 ValidationLoss:0.6088526699502589\n",
      "Epoch: 805 trainginLoss: 0.6045952063278864 ValidationLoss:0.6087537341198679\n",
      "Epoch: 806 trainginLoss: 0.6046857257817415 ValidationLoss:0.608493933839313\n",
      "Epoch: 807 trainginLoss: 0.6046878439468025 ValidationLoss:0.6083326309414233\n",
      "Epoch: 808 trainginLoss: 0.6045285167310062 ValidationLoss:0.608381522105912\n",
      "Epoch: 809 trainginLoss: 0.6044128257156218 ValidationLoss:0.6081777354418221\n",
      "Epoch: 810 trainginLoss: 0.6043571093738479 ValidationLoss:0.6079097026485508\n",
      "Epoch: 811 trainginLoss: 0.6043540787376813 ValidationLoss:0.6076875601784658\n",
      "Epoch: 812 trainginLoss: 0.6046099814792607 ValidationLoss:0.6074684383505482\n",
      "Epoch: 813 trainginLoss: 0.603986590500646 ValidationLoss:0.6075734320333448\n",
      "Epoch: 814 trainginLoss: 0.6039128699558693 ValidationLoss:0.6073671126769761\n",
      "Epoch: 815 trainginLoss: 0.6036704454646015 ValidationLoss:0.6075161414631343\n",
      "Epoch: 816 trainginLoss: 0.6036115796774025 ValidationLoss:0.6072357527280258\n",
      "Epoch: 817 trainginLoss: 0.6040931064010466 ValidationLoss:0.6070387644282842\n",
      "Epoch: 818 trainginLoss: 0.6033703388783755 ValidationLoss:0.6068788174855507\n",
      "Epoch: 819 trainginLoss: 0.6033715137699306 ValidationLoss:0.6069691457990872\n",
      "Epoch: 820 trainginLoss: 0.6032187338643427 ValidationLoss:0.6065749770503933\n",
      "Epoch: 821 trainginLoss: 0.6028871308237114 ValidationLoss:0.6063714009220317\n",
      "Epoch: 822 trainginLoss: 0.6029729303097565 ValidationLoss:0.6061677253852457\n",
      "Epoch: 823 trainginLoss: 0.6030801890680454 ValidationLoss:0.605945769609031\n",
      "Epoch: 824 trainginLoss: 0.602656343639297 ValidationLoss:0.6059099272146063\n",
      "Epoch: 825 trainginLoss: 0.6024556055965039 ValidationLoss:0.6057945291874772\n",
      "Epoch: 826 trainginLoss: 0.6024371297567482 ValidationLoss:0.6060203467385243\n",
      "Epoch: 827 trainginLoss: 0.602329761389918 ValidationLoss:0.6059920112965471\n",
      "Epoch: 828 trainginLoss: 0.6021935403747046 ValidationLoss:0.6055657839370986\n",
      "Epoch: 829 trainginLoss: 0.6019948652926708 ValidationLoss:0.6055821948132273\n",
      "Epoch: 830 trainginLoss: 0.6016858292106014 ValidationLoss:0.6052571046150337\n",
      "Epoch: 831 trainginLoss: 0.6018268542001711 ValidationLoss:0.6051992933628922\n",
      "Epoch: 832 trainginLoss: 0.6015361075433309 ValidationLoss:0.6048522326905849\n",
      "Epoch: 833 trainginLoss: 0.6013343838237276 ValidationLoss:0.6047907558538146\n",
      "Epoch: 834 trainginLoss: 0.6011876375883217 ValidationLoss:0.6046848921452539\n",
      "Epoch: 835 trainginLoss: 0.6011192022554026 ValidationLoss:0.6045940195099783\n",
      "Epoch: 836 trainginLoss: 0.6009572568355791 ValidationLoss:0.604378832598864\n",
      "Epoch: 837 trainginLoss: 0.6010573462351857 ValidationLoss:0.6045052207122414\n",
      "Epoch: 838 trainginLoss: 0.6006925886109371 ValidationLoss:0.6043702570058531\n",
      "Epoch: 839 trainginLoss: 0.6008724602276847 ValidationLoss:0.6040244589417668\n",
      "Epoch: 840 trainginLoss: 0.6004824134327421 ValidationLoss:0.6038900629948761\n",
      "Epoch: 841 trainginLoss: 0.6003716720030612 ValidationLoss:0.603618456347514\n",
      "Epoch: 842 trainginLoss: 0.6002832843153268 ValidationLoss:0.6034743175668231\n",
      "Epoch: 843 trainginLoss: 0.6006640475868379 ValidationLoss:0.6032994872432644\n",
      "Epoch: 844 trainginLoss: 0.5999253532230454 ValidationLoss:0.6032641798762952\n",
      "Epoch: 845 trainginLoss: 0.599808626126923 ValidationLoss:0.6030788536799156\n",
      "Epoch: 846 trainginLoss: 0.5997112889417866 ValidationLoss:0.6029314109834574\n",
      "Epoch: 847 trainginLoss: 0.599714254372872 ValidationLoss:0.6028232548196437\n",
      "Epoch: 848 trainginLoss: 0.5996803457304936 ValidationLoss:0.6026182514126018\n",
      "Epoch: 849 trainginLoss: 0.5993421529763497 ValidationLoss:0.6024400727223542\n",
      "Epoch: 850 trainginLoss: 0.5992731223970451 ValidationLoss:0.6022938265638836\n",
      "Epoch: 851 trainginLoss: 0.5992343225735146 ValidationLoss:0.6019807922638069\n",
      "Epoch: 852 trainginLoss: 0.5988990132440657 ValidationLoss:0.601951698731568\n",
      "Epoch: 853 trainginLoss: 0.5990603358153529 ValidationLoss:0.6016042476993496\n",
      "Epoch: 854 trainginLoss: 0.5990282985188017 ValidationLoss:0.6013796929585732\n",
      "Epoch: 855 trainginLoss: 0.5994131621098359 ValidationLoss:0.601324785765955\n",
      "Epoch: 856 trainginLoss: 0.598597831774078 ValidationLoss:0.6013713941735737\n",
      "Epoch: 857 trainginLoss: 0.5984972355349752 ValidationLoss:0.6011448051969884\n",
      "Epoch: 858 trainginLoss: 0.5983547160289432 ValidationLoss:0.6009290838645677\n",
      "Epoch: 859 trainginLoss: 0.5981702396533634 ValidationLoss:0.600828103493836\n",
      "Epoch: 860 trainginLoss: 0.5979754988779158 ValidationLoss:0.6008327150748948\n",
      "Epoch: 861 trainginLoss: 0.5979248949345326 ValidationLoss:0.6005856813010523\n",
      "Epoch: 862 trainginLoss: 0.5977071255645496 ValidationLoss:0.600390284546351\n",
      "Epoch: 863 trainginLoss: 0.5975126208074941 ValidationLoss:0.6004110178704989\n",
      "Epoch: 864 trainginLoss: 0.5974382006081959 ValidationLoss:0.6001793693687956\n",
      "Epoch: 865 trainginLoss: 0.5972140475407542 ValidationLoss:0.6000912082397332\n",
      "Epoch: 866 trainginLoss: 0.5970970260216886 ValidationLoss:0.6000221711094097\n",
      "Epoch: 867 trainginLoss: 0.5973694964543285 ValidationLoss:0.5995487906165042\n",
      "Epoch: 868 trainginLoss: 0.5971832047372856 ValidationLoss:0.599608521340257\n",
      "Epoch: 869 trainginLoss: 0.5969457818357736 ValidationLoss:0.5996802263340708\n",
      "Epoch: 870 trainginLoss: 0.5966268109795231 ValidationLoss:0.5992942016003495\n",
      "Epoch: 871 trainginLoss: 0.59646218175056 ValidationLoss:0.5992582589893017\n",
      "Epoch: 872 trainginLoss: 0.596307066462984 ValidationLoss:0.5992004131866714\n",
      "Epoch: 873 trainginLoss: 0.5963313419546857 ValidationLoss:0.5989217584416018\n",
      "Epoch: 874 trainginLoss: 0.5964231147062058 ValidationLoss:0.5985739119982315\n",
      "Epoch: 875 trainginLoss: 0.596275200779806 ValidationLoss:0.5984496565188392\n",
      "Epoch: 876 trainginLoss: 0.5957593197790568 ValidationLoss:0.5983706417730299\n",
      "Epoch: 877 trainginLoss: 0.5956412345771022 ValidationLoss:0.5982704136331203\n",
      "Epoch: 878 trainginLoss: 0.5959433441194112 ValidationLoss:0.5980790158449594\n",
      "Epoch: 879 trainginLoss: 0.5954417262301349 ValidationLoss:0.5978862449274225\n",
      "Epoch: 880 trainginLoss: 0.5952520330480281 ValidationLoss:0.5978202704655922\n",
      "Epoch: 881 trainginLoss: 0.5951404559532268 ValidationLoss:0.5976474856926223\n",
      "Epoch: 882 trainginLoss: 0.5951808375000154 ValidationLoss:0.5974221853886621\n",
      "Epoch: 883 trainginLoss: 0.595176394353777 ValidationLoss:0.597206408694639\n",
      "Epoch: 884 trainginLoss: 0.5948165899155123 ValidationLoss:0.5971682314145363\n",
      "Epoch: 885 trainginLoss: 0.5945686057910023 ValidationLoss:0.5969949609142239\n",
      "Epoch: 886 trainginLoss: 0.5945037315355851 ValidationLoss:0.5969387937400301\n",
      "Epoch: 887 trainginLoss: 0.5945043991876129 ValidationLoss:0.596946878756507\n",
      "Epoch: 888 trainginLoss: 0.594245239392223 ValidationLoss:0.5967482700186261\n",
      "Epoch: 889 trainginLoss: 0.594384900675524 ValidationLoss:0.596613451181832\n",
      "Epoch: 890 trainginLoss: 0.5940657386043727 ValidationLoss:0.5962457105264826\n",
      "Epoch: 891 trainginLoss: 0.5942208738935074 ValidationLoss:0.5962818117465003\n",
      "Epoch: 892 trainginLoss: 0.5938698737413292 ValidationLoss:0.5960879612777192\n",
      "Epoch: 893 trainginLoss: 0.5936655266172934 ValidationLoss:0.5960044230444956\n",
      "Epoch: 894 trainginLoss: 0.5935588731061692 ValidationLoss:0.5955453015990176\n",
      "Epoch: 895 trainginLoss: 0.5935571409711902 ValidationLoss:0.5955705576023813\n",
      "Epoch: 896 trainginLoss: 0.5934333721263296 ValidationLoss:0.5951853097495386\n",
      "Epoch: 897 trainginLoss: 0.5930067540815213 ValidationLoss:0.5952089760263087\n",
      "Epoch: 898 trainginLoss: 0.5928402503064815 ValidationLoss:0.5950110079878468\n",
      "Epoch: 899 trainginLoss: 0.5928714431372265 ValidationLoss:0.5950782303082741\n",
      "Epoch: 900 trainginLoss: 0.5926547882540915 ValidationLoss:0.5947637735787085\n",
      "Epoch: 901 trainginLoss: 0.5926947517683042 ValidationLoss:0.5949616533214763\n",
      "Epoch: 902 trainginLoss: 0.5923917269546713 ValidationLoss:0.594559386221029\n",
      "Epoch: 903 trainginLoss: 0.5923465386333081 ValidationLoss:0.5941607760170758\n",
      "Epoch: 904 trainginLoss: 0.5920402355642127 ValidationLoss:0.5940958615076743\n",
      "Epoch: 905 trainginLoss: 0.5919250745901325 ValidationLoss:0.5938451615430541\n",
      "Epoch: 906 trainginLoss: 0.5923808197047087 ValidationLoss:0.593628139819129\n",
      "Epoch: 907 trainginLoss: 0.5917923354462489 ValidationLoss:0.5934279215537895\n",
      "Epoch: 908 trainginLoss: 0.5916105032767225 ValidationLoss:0.5933510855092841\n",
      "Epoch: 909 trainginLoss: 0.5914485874592058 ValidationLoss:0.5932138669288765\n",
      "Epoch: 910 trainginLoss: 0.5915940067912108 ValidationLoss:0.593346597582607\n",
      "Epoch: 911 trainginLoss: 0.5912116749974705 ValidationLoss:0.5929902951596147\n",
      "Epoch: 912 trainginLoss: 0.5910918712615967 ValidationLoss:0.5931455618244107\n",
      "Epoch: 913 trainginLoss: 0.5909201882829602 ValidationLoss:0.5928805615942357\n",
      "Epoch: 914 trainginLoss: 0.5908002413359265 ValidationLoss:0.5925458999003395\n",
      "Epoch: 915 trainginLoss: 0.5906455336801157 ValidationLoss:0.5924534575413849\n",
      "Epoch: 916 trainginLoss: 0.5904031675933992 ValidationLoss:0.5921987093101113\n",
      "Epoch: 917 trainginLoss: 0.591024937645701 ValidationLoss:0.5920459555367292\n",
      "Epoch: 918 trainginLoss: 0.5903895713338916 ValidationLoss:0.5919372281785739\n",
      "Epoch: 919 trainginLoss: 0.5902081263945407 ValidationLoss:0.5916015679553404\n",
      "Epoch: 920 trainginLoss: 0.5901333329661581 ValidationLoss:0.5914438817460658\n",
      "Epoch: 921 trainginLoss: 0.5902635623144623 ValidationLoss:0.5911900744599811\n",
      "Epoch: 922 trainginLoss: 0.5900825822113345 ValidationLoss:0.5911625082209959\n",
      "Epoch: 923 trainginLoss: 0.5896240344783604 ValidationLoss:0.5908951167332924\n",
      "Epoch: 924 trainginLoss: 0.589737147692866 ValidationLoss:0.5909835457801819\n",
      "Epoch: 925 trainginLoss: 0.5898164498725994 ValidationLoss:0.5910138045327138\n",
      "Epoch: 926 trainginLoss: 0.589551542429316 ValidationLoss:0.5908381949036808\n",
      "Epoch: 927 trainginLoss: 0.5893848398387832 ValidationLoss:0.5904947537486837\n",
      "Epoch: 928 trainginLoss: 0.5888653209545468 ValidationLoss:0.5903697143166752\n",
      "Epoch: 929 trainginLoss: 0.5888725931212406 ValidationLoss:0.5901653059458328\n",
      "Epoch: 930 trainginLoss: 0.5887034127376224 ValidationLoss:0.5899470064599636\n",
      "Epoch: 931 trainginLoss: 0.5883552000026575 ValidationLoss:0.5898473822464377\n",
      "Epoch: 932 trainginLoss: 0.5883565025041567 ValidationLoss:0.5896422598321559\n",
      "Epoch: 933 trainginLoss: 0.5882960129904267 ValidationLoss:0.5893057136212365\n",
      "Epoch: 934 trainginLoss: 0.588096401835448 ValidationLoss:0.5892687306565754\n",
      "Epoch: 935 trainginLoss: 0.5879394972084353 ValidationLoss:0.5892778669373464\n",
      "Epoch: 936 trainginLoss: 0.5877544075850673 ValidationLoss:0.5891311623282351\n",
      "Epoch: 937 trainginLoss: 0.5879043232674567 ValidationLoss:0.5892368142887697\n",
      "Epoch: 938 trainginLoss: 0.5875883950483078 ValidationLoss:0.5885555820950007\n",
      "Epoch: 939 trainginLoss: 0.5874487077629806 ValidationLoss:0.5884577256138042\n",
      "Epoch: 940 trainginLoss: 0.5872188146482378 ValidationLoss:0.5882722882901208\n",
      "Epoch: 941 trainginLoss: 0.5872049435673145 ValidationLoss:0.5880849927158679\n",
      "Epoch: 942 trainginLoss: 0.5872125453596947 ValidationLoss:0.5879340236469851\n",
      "Epoch: 943 trainginLoss: 0.5867772650398664 ValidationLoss:0.5877721841052427\n",
      "Epoch: 944 trainginLoss: 0.5869816761688899 ValidationLoss:0.5879995822906494\n",
      "Epoch: 945 trainginLoss: 0.5865085565003773 ValidationLoss:0.5877385707224829\n",
      "Epoch: 946 trainginLoss: 0.5863759137640063 ValidationLoss:0.5874893909793789\n",
      "Epoch: 947 trainginLoss: 0.5864996026026322 ValidationLoss:0.5871213648278835\n",
      "Epoch: 948 trainginLoss: 0.5863321987574532 ValidationLoss:0.5869905613236508\n",
      "Epoch: 949 trainginLoss: 0.5863648208995793 ValidationLoss:0.5866015707032155\n",
      "Epoch: 950 trainginLoss: 0.5862281138464909 ValidationLoss:0.5864601058475042\n",
      "Epoch: 951 trainginLoss: 0.5856958671704234 ValidationLoss:0.5864395400225106\n",
      "Epoch: 952 trainginLoss: 0.5854875329356866 ValidationLoss:0.5863505254357548\n",
      "Epoch: 953 trainginLoss: 0.5853236625658585 ValidationLoss:0.5861147872472213\n",
      "Epoch: 954 trainginLoss: 0.5853635344729328 ValidationLoss:0.5861204761569783\n",
      "Epoch: 955 trainginLoss: 0.585113927821985 ValidationLoss:0.5859186713978396\n",
      "Epoch: 956 trainginLoss: 0.5850155493557053 ValidationLoss:0.5856898271431357\n",
      "Epoch: 957 trainginLoss: 0.5849286897870518 ValidationLoss:0.5854818558288832\n",
      "Epoch: 958 trainginLoss: 0.5857178140806671 ValidationLoss:0.5854816313517296\n",
      "Epoch: 959 trainginLoss: 0.5846105633966074 ValidationLoss:0.5852980831922111\n",
      "Epoch: 960 trainginLoss: 0.5847858862588869 ValidationLoss:0.5849041704404152\n",
      "Epoch: 961 trainginLoss: 0.5841800690497327 ValidationLoss:0.5847515732555066\n",
      "Epoch: 962 trainginLoss: 0.5841360284178049 ValidationLoss:0.5846844079130787\n",
      "Epoch: 963 trainginLoss: 0.584450252904188 ValidationLoss:0.5844850879604534\n",
      "Epoch: 964 trainginLoss: 0.5838301777839661 ValidationLoss:0.5841767496981863\n",
      "Epoch: 965 trainginLoss: 0.583725094795227 ValidationLoss:0.584206401897689\n",
      "Epoch: 966 trainginLoss: 0.5836395465287586 ValidationLoss:0.5840169332795224\n",
      "Epoch: 967 trainginLoss: 0.5835348799724707 ValidationLoss:0.5839592159804651\n",
      "Epoch: 968 trainginLoss: 0.5833757467717933 ValidationLoss:0.5840665572780673\n",
      "Epoch: 969 trainginLoss: 0.5831688742509624 ValidationLoss:0.5838751324152542\n",
      "Epoch: 970 trainginLoss: 0.5829867788609242 ValidationLoss:0.583438396655907\n",
      "Epoch: 971 trainginLoss: 0.5829136919655256 ValidationLoss:0.5833177750393496\n",
      "Epoch: 972 trainginLoss: 0.5827610296691024 ValidationLoss:0.5829766574552504\n",
      "Epoch: 973 trainginLoss: 0.5825339379726641 ValidationLoss:0.5830048270144705\n",
      "Epoch: 974 trainginLoss: 0.5824970182956465 ValidationLoss:0.5826769196381003\n",
      "Epoch: 975 trainginLoss: 0.5822035890297602 ValidationLoss:0.5825575666912531\n",
      "Epoch: 976 trainginLoss: 0.5824756398296996 ValidationLoss:0.58283417245089\n",
      "Epoch: 977 trainginLoss: 0.5821033732203029 ValidationLoss:0.5826839234869359\n",
      "Epoch: 978 trainginLoss: 0.5820149847325062 ValidationLoss:0.5825237841929419\n",
      "Epoch: 979 trainginLoss: 0.5817092953112302 ValidationLoss:0.5819285817065482\n",
      "Epoch: 980 trainginLoss: 0.581623491024811 ValidationLoss:0.5814945418955916\n",
      "Epoch: 981 trainginLoss: 0.5818031622259409 ValidationLoss:0.5820064894223618\n",
      "Epoch: 982 trainginLoss: 0.5817025383846872 ValidationLoss:0.5812540302842351\n",
      "Epoch: 983 trainginLoss: 0.581108190869325 ValidationLoss:0.5810208447908951\n",
      "Epoch: 984 trainginLoss: 0.5813059978837135 ValidationLoss:0.5811927251896616\n",
      "Epoch: 985 trainginLoss: 0.5809935683372037 ValidationLoss:0.5811995213314638\n",
      "Epoch: 986 trainginLoss: 0.5808020938162836 ValidationLoss:0.5806220971931846\n",
      "Epoch: 987 trainginLoss: 0.5807725527142519 ValidationLoss:0.580363389395051\n",
      "Epoch: 988 trainginLoss: 0.5803303722567206 ValidationLoss:0.5803592681884766\n",
      "Epoch: 989 trainginLoss: 0.5804952983088141 ValidationLoss:0.5802641636234219\n",
      "Epoch: 990 trainginLoss: 0.580376025814338 ValidationLoss:0.5798655237181712\n",
      "Epoch: 991 trainginLoss: 0.5799542129439795 ValidationLoss:0.5796787629693242\n",
      "Epoch: 992 trainginLoss: 0.5802849735189605 ValidationLoss:0.5793047149302596\n",
      "Epoch: 993 trainginLoss: 0.5798202605055482 ValidationLoss:0.579276300890971\n",
      "Epoch: 994 trainginLoss: 0.5798329798967247 ValidationLoss:0.5792010673021866\n",
      "Epoch: 995 trainginLoss: 0.5792960628567126 ValidationLoss:0.5789261169352774\n",
      "Epoch: 996 trainginLoss: 0.579402437146078 ValidationLoss:0.5787903609922377\n",
      "Epoch: 997 trainginLoss: 0.5790955748334027 ValidationLoss:0.5788212287223945\n",
      "Epoch: 998 trainginLoss: 0.5788723630393111 ValidationLoss:0.5787809177980585\n",
      "Epoch: 999 trainginLoss: 0.5792885834738712 ValidationLoss:0.5781115406650608\n",
      "Epoch: 1000 trainginLoss: 0.5790730694796415 ValidationLoss:0.5779139276278221\n",
      "Epoch: 1001 trainginLoss: 0.578648828420063 ValidationLoss:0.5778835547172417\n",
      "Epoch: 1002 trainginLoss: 0.5784281544237329 ValidationLoss:0.5778572240118253\n",
      "Epoch: 1003 trainginLoss: 0.5786544216559237 ValidationLoss:0.5775885280916246\n",
      "Epoch: 1004 trainginLoss: 0.5782445025124006 ValidationLoss:0.5772069136975175\n",
      "Epoch: 1005 trainginLoss: 0.5780989635710748 ValidationLoss:0.5772172994532827\n",
      "Epoch: 1006 trainginLoss: 0.5776412615040004 ValidationLoss:0.5770815994779942\n",
      "Epoch: 1007 trainginLoss: 0.5783576577301793 ValidationLoss:0.5771031444355593\n",
      "Epoch: 1008 trainginLoss: 0.5778155662869447 ValidationLoss:0.5767947609141721\n",
      "Epoch: 1009 trainginLoss: 0.5782782535424968 ValidationLoss:0.5764473201864857\n",
      "Epoch: 1010 trainginLoss: 0.5770674223867839 ValidationLoss:0.5762744521690627\n",
      "Epoch: 1011 trainginLoss: 0.5772339653648786 ValidationLoss:0.5763833692518331\n",
      "Epoch: 1012 trainginLoss: 0.5770901521580332 ValidationLoss:0.5759350350347616\n",
      "Epoch: 1013 trainginLoss: 0.577698123935085 ValidationLoss:0.5760801701222436\n",
      "Epoch: 1014 trainginLoss: 0.5768736136839694 ValidationLoss:0.575576189412909\n",
      "Epoch: 1015 trainginLoss: 0.5763237296334849 ValidationLoss:0.5754752448049643\n",
      "Epoch: 1016 trainginLoss: 0.576532757922307 ValidationLoss:0.5753803439059499\n",
      "Epoch: 1017 trainginLoss: 0.5766530300946844 ValidationLoss:0.5750409336413367\n",
      "Epoch: 1018 trainginLoss: 0.5759651204883652 ValidationLoss:0.5749390159623098\n",
      "Epoch: 1019 trainginLoss: 0.5761642368047829 ValidationLoss:0.5749016650652481\n",
      "Epoch: 1020 trainginLoss: 0.5757688983974841 ValidationLoss:0.5744633472571938\n",
      "Epoch: 1021 trainginLoss: 0.5757708573501382 ValidationLoss:0.5742193506935895\n",
      "Epoch: 1022 trainginLoss: 0.5754964103634725 ValidationLoss:0.5740764696719283\n",
      "Epoch: 1023 trainginLoss: 0.5753425511737797 ValidationLoss:0.5739115088672961\n",
      "Epoch: 1024 trainginLoss: 0.5751274904148691 ValidationLoss:0.5737295639716973\n",
      "Epoch: 1025 trainginLoss: 0.575125954695196 ValidationLoss:0.5737855032338934\n",
      "Epoch: 1026 trainginLoss: 0.5748353856521965 ValidationLoss:0.5733818874520771\n",
      "Epoch: 1027 trainginLoss: 0.574659401538388 ValidationLoss:0.5732275273840306\n",
      "Epoch: 1028 trainginLoss: 0.5746878709569073 ValidationLoss:0.5730525374412536\n",
      "Epoch: 1029 trainginLoss: 0.5743295966378794 ValidationLoss:0.5729779104054984\n",
      "Epoch: 1030 trainginLoss: 0.5743025109271875 ValidationLoss:0.5726757318286573\n",
      "Epoch: 1031 trainginLoss: 0.5740345308444644 ValidationLoss:0.5724779009819031\n",
      "Epoch: 1032 trainginLoss: 0.5741002147629757 ValidationLoss:0.572329598362163\n",
      "Epoch: 1033 trainginLoss: 0.5737718575752822 ValidationLoss:0.5722469123743348\n",
      "Epoch: 1034 trainginLoss: 0.5735706066925254 ValidationLoss:0.5720232957500523\n",
      "Epoch: 1035 trainginLoss: 0.5736167422877062 ValidationLoss:0.5717617891602597\n",
      "Epoch: 1036 trainginLoss: 0.5733622900591601 ValidationLoss:0.5716977165917219\n",
      "Epoch: 1037 trainginLoss: 0.5736442968349329 ValidationLoss:0.5714116716788987\n",
      "Epoch: 1038 trainginLoss: 0.573224957357317 ValidationLoss:0.5714288410493883\n",
      "Epoch: 1039 trainginLoss: 0.5728004494769461 ValidationLoss:0.5712013396166139\n",
      "Epoch: 1040 trainginLoss: 0.5733985656859891 ValidationLoss:0.5714517306473296\n",
      "Epoch: 1041 trainginLoss: 0.5729320489320179 ValidationLoss:0.5712912024077722\n",
      "Epoch: 1042 trainginLoss: 0.5724641496703129 ValidationLoss:0.5707496144003787\n",
      "Epoch: 1043 trainginLoss: 0.5721796258983997 ValidationLoss:0.5707859144372455\n",
      "Epoch: 1044 trainginLoss: 0.572242458794741 ValidationLoss:0.5703172443276745\n",
      "Epoch: 1045 trainginLoss: 0.5722199614416033 ValidationLoss:0.5700640045990378\n",
      "Epoch: 1046 trainginLoss: 0.5726776623085841 ValidationLoss:0.5698965816174524\n",
      "Epoch: 1047 trainginLoss: 0.5716045850875394 ValidationLoss:0.5697495280686071\n",
      "Epoch: 1048 trainginLoss: 0.5714311967760124 ValidationLoss:0.5696354876130314\n",
      "Epoch: 1049 trainginLoss: 0.5712690521406647 ValidationLoss:0.5696668249065593\n",
      "Epoch: 1050 trainginLoss: 0.5711408949538366 ValidationLoss:0.5695977714102147\n",
      "Epoch: 1051 trainginLoss: 0.5713640147407583 ValidationLoss:0.5691405601420645\n",
      "Epoch: 1052 trainginLoss: 0.5707215086725734 ValidationLoss:0.5689555376262988\n",
      "Epoch: 1053 trainginLoss: 0.5709409241708333 ValidationLoss:0.5686966314154156\n",
      "Epoch: 1054 trainginLoss: 0.5710551870749301 ValidationLoss:0.5685316409094859\n",
      "Epoch: 1055 trainginLoss: 0.5702651235081205 ValidationLoss:0.5683523901438309\n",
      "Epoch: 1056 trainginLoss: 0.5706789317547075 ValidationLoss:0.56860416137566\n",
      "Epoch: 1057 trainginLoss: 0.57011668074051 ValidationLoss:0.5683045520620831\n",
      "Epoch: 1058 trainginLoss: 0.569896062348513 ValidationLoss:0.5678965974662263\n",
      "Epoch: 1059 trainginLoss: 0.57006139883259 ValidationLoss:0.5679892972364264\n",
      "Epoch: 1060 trainginLoss: 0.570120487037121 ValidationLoss:0.5678514969550957\n",
      "Epoch: 1061 trainginLoss: 0.5694427346223153 ValidationLoss:0.56729702909114\n",
      "Epoch: 1062 trainginLoss: 0.5692876557375761 ValidationLoss:0.5673485838760763\n",
      "Epoch: 1063 trainginLoss: 0.5697548093411746 ValidationLoss:0.5667520967580504\n",
      "Epoch: 1064 trainginLoss: 0.5691859218098173 ValidationLoss:0.5664730019488577\n",
      "Epoch: 1065 trainginLoss: 0.5691168960308869 ValidationLoss:0.5663609561273607\n",
      "Epoch: 1066 trainginLoss: 0.5687906578083166 ValidationLoss:0.5663129232697568\n",
      "Epoch: 1067 trainginLoss: 0.5687092902676371 ValidationLoss:0.5662158042697584\n",
      "Epoch: 1068 trainginLoss: 0.5686312297846647 ValidationLoss:0.566034032934803\n",
      "Epoch: 1069 trainginLoss: 0.568358694947006 ValidationLoss:0.5655860906940395\n",
      "Epoch: 1070 trainginLoss: 0.5680924926028156 ValidationLoss:0.5655414797491947\n",
      "Epoch: 1071 trainginLoss: 0.5682348908994022 ValidationLoss:0.5652196510363433\n",
      "Epoch: 1072 trainginLoss: 0.5678765813776311 ValidationLoss:0.5651547082399918\n",
      "Epoch: 1073 trainginLoss: 0.5677661307706129 ValidationLoss:0.5649566599878214\n",
      "Epoch: 1074 trainginLoss: 0.5674169251582767 ValidationLoss:0.5650180267075361\n",
      "Epoch: 1075 trainginLoss: 0.5675470420978214 ValidationLoss:0.5645913631229077\n",
      "Epoch: 1076 trainginLoss: 0.5671122678174269 ValidationLoss:0.5643150491229558\n",
      "Epoch: 1077 trainginLoss: 0.5671965247832689 ValidationLoss:0.5643739247726182\n",
      "Epoch: 1078 trainginLoss: 0.5668629543893289 ValidationLoss:0.5640210679021932\n",
      "Epoch: 1079 trainginLoss: 0.566901163366817 ValidationLoss:0.563801589052556\n",
      "Epoch: 1080 trainginLoss: 0.5666331396806961 ValidationLoss:0.5638786736181227\n",
      "Epoch: 1081 trainginLoss: 0.5667209365223879 ValidationLoss:0.5634620638216956\n",
      "Epoch: 1082 trainginLoss: 0.5661754624155544 ValidationLoss:0.5632997256214336\n",
      "Epoch: 1083 trainginLoss: 0.5661273102632305 ValidationLoss:0.5633338142249544\n",
      "Epoch: 1084 trainginLoss: 0.566029655853374 ValidationLoss:0.5631732645681349\n",
      "Epoch: 1085 trainginLoss: 0.5663771825348771 ValidationLoss:0.5629749863834704\n",
      "Epoch: 1086 trainginLoss: 0.5658078717705387 ValidationLoss:0.5630456613282026\n",
      "Epoch: 1087 trainginLoss: 0.5654061956693662 ValidationLoss:0.562596273018142\n",
      "Epoch: 1088 trainginLoss: 0.5652113288840992 ValidationLoss:0.562358122558917\n",
      "Epoch: 1089 trainginLoss: 0.5656226241348574 ValidationLoss:0.5620568895744065\n",
      "Epoch: 1090 trainginLoss: 0.56501920671271 ValidationLoss:0.5619501578605781\n",
      "Epoch: 1091 trainginLoss: 0.5650894933898977 ValidationLoss:0.5618605375289917\n",
      "Epoch: 1092 trainginLoss: 0.5647468810913546 ValidationLoss:0.5614358851465128\n",
      "Epoch: 1093 trainginLoss: 0.5645426807787595 ValidationLoss:0.5612151511644913\n",
      "Epoch: 1094 trainginLoss: 0.5644025186564299 ValidationLoss:0.560949119066788\n",
      "Epoch: 1095 trainginLoss: 0.5645061251301093 ValidationLoss:0.5607652563159748\n",
      "Epoch: 1096 trainginLoss: 0.5640754923724488 ValidationLoss:0.5607344578888457\n",
      "Epoch: 1097 trainginLoss: 0.5637811258335241 ValidationLoss:0.5606893414157932\n",
      "Epoch: 1098 trainginLoss: 0.5637214115802074 ValidationLoss:0.5604824039895656\n",
      "Epoch: 1099 trainginLoss: 0.5638925761184437 ValidationLoss:0.5605949349322561\n",
      "Epoch: 1100 trainginLoss: 0.5636666473126252 ValidationLoss:0.5599950812630734\n",
      "Epoch: 1101 trainginLoss: 0.5633624324062526 ValidationLoss:0.560120984457307\n",
      "Epoch: 1102 trainginLoss: 0.5631526436581707 ValidationLoss:0.5599281743421393\n",
      "Epoch: 1103 trainginLoss: 0.5630499212533836 ValidationLoss:0.5599197848368499\n",
      "Epoch: 1104 trainginLoss: 0.5626982414482424 ValidationLoss:0.5593133708177986\n",
      "Epoch: 1105 trainginLoss: 0.5632266726269818 ValidationLoss:0.5591773069511026\n",
      "Epoch: 1106 trainginLoss: 0.5623705311109556 ValidationLoss:0.5588831586352849\n",
      "Epoch: 1107 trainginLoss: 0.5625879576542233 ValidationLoss:0.55855162305347\n",
      "Epoch: 1108 trainginLoss: 0.5622750656716775 ValidationLoss:0.5584452035063404\n",
      "Epoch: 1109 trainginLoss: 0.5627143787057608 ValidationLoss:0.5584801871897811\n",
      "Epoch: 1110 trainginLoss: 0.5620417602910291 ValidationLoss:0.5580675919177168\n",
      "Epoch: 1111 trainginLoss: 0.5617315217152538 ValidationLoss:0.557852597964012\n",
      "Epoch: 1112 trainginLoss: 0.5618575223340284 ValidationLoss:0.5581674745527364\n",
      "Epoch: 1113 trainginLoss: 0.5615560276396323 ValidationLoss:0.5576669604091321\n",
      "Epoch: 1114 trainginLoss: 0.5614195266025979 ValidationLoss:0.5576826651217573\n",
      "Epoch: 1115 trainginLoss: 0.5610825311417548 ValidationLoss:0.5572395969245394\n",
      "Epoch: 1116 trainginLoss: 0.5607272710576153 ValidationLoss:0.5571761032282296\n",
      "Epoch: 1117 trainginLoss: 0.5609646891587533 ValidationLoss:0.5568807282690275\n",
      "Epoch: 1118 trainginLoss: 0.5616095874133527 ValidationLoss:0.5566961955215971\n",
      "Epoch: 1119 trainginLoss: 0.5603767105396962 ValidationLoss:0.5566907494755114\n",
      "Epoch: 1120 trainginLoss: 0.5601827714267194 ValidationLoss:0.5562865451230841\n",
      "Epoch: 1121 trainginLoss: 0.5604784288662392 ValidationLoss:0.5558838304826769\n",
      "Epoch: 1122 trainginLoss: 0.5603939250811635 ValidationLoss:0.5556872879044484\n",
      "Epoch: 1123 trainginLoss: 0.5600973911733436 ValidationLoss:0.5554993756746842\n",
      "Epoch: 1124 trainginLoss: 0.5598300911436145 ValidationLoss:0.5553583918991736\n",
      "Epoch: 1125 trainginLoss: 0.559672874892318 ValidationLoss:0.5552654260295933\n",
      "Epoch: 1126 trainginLoss: 0.5597990831272714 ValidationLoss:0.5550893137010477\n",
      "Epoch: 1127 trainginLoss: 0.5591363322814838 ValidationLoss:0.5547387175640818\n",
      "Epoch: 1128 trainginLoss: 0.5597870693910842 ValidationLoss:0.5551603592048258\n",
      "Epoch: 1129 trainginLoss: 0.5587936391766439 ValidationLoss:0.5545541409718788\n",
      "Epoch: 1130 trainginLoss: 0.558807181031912 ValidationLoss:0.5542945429430169\n",
      "Epoch: 1131 trainginLoss: 0.5582915604514563 ValidationLoss:0.5542018427687176\n",
      "Epoch: 1132 trainginLoss: 0.5586842394515172 ValidationLoss:0.5537928173097514\n",
      "Epoch: 1133 trainginLoss: 0.5586185479324136 ValidationLoss:0.5536624571024361\n",
      "Epoch: 1134 trainginLoss: 0.558576806679668 ValidationLoss:0.5534687521093983\n",
      "Epoch: 1135 trainginLoss: 0.5583327784634277 ValidationLoss:0.5532464098122161\n",
      "Epoch: 1136 trainginLoss: 0.5577862834770407 ValidationLoss:0.5530309766025866\n",
      "Epoch: 1137 trainginLoss: 0.5580219722434179 ValidationLoss:0.5528396331657798\n",
      "Epoch: 1138 trainginLoss: 0.5575688149304998 ValidationLoss:0.5526520159284948\n",
      "Epoch: 1139 trainginLoss: 0.5574189276503236 ValidationLoss:0.5526167301808373\n",
      "Epoch: 1140 trainginLoss: 0.5574442720253195 ValidationLoss:0.5523903844720226\n",
      "Epoch: 1141 trainginLoss: 0.5567593790540759 ValidationLoss:0.5524816567614927\n",
      "Epoch: 1142 trainginLoss: 0.5566479447704034 ValidationLoss:0.5519275418782639\n",
      "Epoch: 1143 trainginLoss: 0.5567471473008995 ValidationLoss:0.5517602049698264\n",
      "Epoch: 1144 trainginLoss: 0.5566493900830314 ValidationLoss:0.5515104867644229\n",
      "Epoch: 1145 trainginLoss: 0.557126152035374 ValidationLoss:0.5513329156374527\n",
      "Epoch: 1146 trainginLoss: 0.5560037917738793 ValidationLoss:0.5511846033193297\n",
      "Epoch: 1147 trainginLoss: 0.5562961865591523 ValidationLoss:0.5509887452852928\n",
      "Epoch: 1148 trainginLoss: 0.5561233606914546 ValidationLoss:0.5509302153425701\n",
      "Epoch: 1149 trainginLoss: 0.555941930553257 ValidationLoss:0.550573908878585\n",
      "Epoch: 1150 trainginLoss: 0.556086135790652 ValidationLoss:0.5504922676894625\n",
      "Epoch: 1151 trainginLoss: 0.5555298772434261 ValidationLoss:0.5502058144343102\n",
      "Epoch: 1152 trainginLoss: 0.5553628398268015 ValidationLoss:0.549989044262191\n",
      "Epoch: 1153 trainginLoss: 0.5550564227488217 ValidationLoss:0.5498200436769906\n",
      "Epoch: 1154 trainginLoss: 0.555571189262723 ValidationLoss:0.5500236858755855\n",
      "Epoch: 1155 trainginLoss: 0.5550248554888988 ValidationLoss:0.5498673295570632\n",
      "Epoch: 1156 trainginLoss: 0.5548278669382902 ValidationLoss:0.5496007109092453\n",
      "Epoch: 1157 trainginLoss: 0.5541043045536783 ValidationLoss:0.5491503784212015\n",
      "Epoch: 1158 trainginLoss: 0.5548623388245602 ValidationLoss:0.5489942851713148\n",
      "Epoch: 1159 trainginLoss: 0.553929631742055 ValidationLoss:0.548819036807044\n",
      "Epoch: 1160 trainginLoss: 0.5548900549843807 ValidationLoss:0.5484783645403587\n",
      "Epoch: 1161 trainginLoss: 0.5538945802106153 ValidationLoss:0.5482861134965541\n",
      "Epoch: 1162 trainginLoss: 0.5535574695407944 ValidationLoss:0.5484219745054083\n",
      "Epoch: 1163 trainginLoss: 0.5550117092644609 ValidationLoss:0.5490186957989709\n",
      "Epoch: 1164 trainginLoss: 0.5536487778561228 ValidationLoss:0.5479583879648628\n",
      "Epoch: 1165 trainginLoss: 0.5530669453160074 ValidationLoss:0.5475169258602595\n",
      "Epoch: 1166 trainginLoss: 0.5527996756886476 ValidationLoss:0.5474713523509138\n",
      "Epoch: 1167 trainginLoss: 0.5526563425192097 ValidationLoss:0.5473245620727539\n",
      "Epoch: 1168 trainginLoss: 0.5534584980283007 ValidationLoss:0.5473783436468092\n",
      "Epoch: 1169 trainginLoss: 0.5535922460508027 ValidationLoss:0.54761112079782\n",
      "Epoch: 1170 trainginLoss: 0.5523037362418719 ValidationLoss:0.5471816826674898\n",
      "Epoch: 1171 trainginLoss: 0.5520011382615007 ValidationLoss:0.5467854087635622\n",
      "Epoch: 1172 trainginLoss: 0.5521368268352227 ValidationLoss:0.5462549846051103\n",
      "Epoch: 1173 trainginLoss: 0.5516659445410607 ValidationLoss:0.5463280192876266\n",
      "Epoch: 1174 trainginLoss: 0.5522575670440725 ValidationLoss:0.5466127470388251\n",
      "Epoch: 1175 trainginLoss: 0.5519720199924187 ValidationLoss:0.5457410038527796\n",
      "Epoch: 1176 trainginLoss: 0.5513059092848093 ValidationLoss:0.545938549607487\n",
      "Epoch: 1177 trainginLoss: 0.5512140341253089 ValidationLoss:0.54534257852425\n",
      "Epoch: 1178 trainginLoss: 0.5508164831455922 ValidationLoss:0.5453324204784329\n",
      "Epoch: 1179 trainginLoss: 0.5511625017095733 ValidationLoss:0.5451218792947672\n",
      "Epoch: 1180 trainginLoss: 0.5504265331581935 ValidationLoss:0.5448009816266722\n",
      "Epoch: 1181 trainginLoss: 0.5504575447748171 ValidationLoss:0.544847038842864\n",
      "Epoch: 1182 trainginLoss: 0.5503404848527589 ValidationLoss:0.5445503152022928\n",
      "Epoch: 1183 trainginLoss: 0.5498678580226514 ValidationLoss:0.5443279343136286\n",
      "Epoch: 1184 trainginLoss: 0.5498669307503924 ValidationLoss:0.54390306088884\n",
      "Epoch: 1185 trainginLoss: 0.5497057517902963 ValidationLoss:0.5438001202324689\n",
      "Epoch: 1186 trainginLoss: 0.5496324144753834 ValidationLoss:0.5435430508548931\n",
      "Epoch: 1187 trainginLoss: 0.5493028779957918 ValidationLoss:0.5434576905379861\n",
      "Epoch: 1188 trainginLoss: 0.5491381167565417 ValidationLoss:0.5431520365052304\n",
      "Epoch: 1189 trainginLoss: 0.5489308690064706 ValidationLoss:0.5430328500472893\n",
      "Epoch: 1190 trainginLoss: 0.5491200797509828 ValidationLoss:0.5427562943959641\n",
      "Epoch: 1191 trainginLoss: 0.5484954750777891 ValidationLoss:0.5425691396503125\n",
      "Epoch: 1192 trainginLoss: 0.5486694478348597 ValidationLoss:0.5424174027927852\n",
      "Epoch: 1193 trainginLoss: 0.5483416542910889 ValidationLoss:0.5422420408766149\n",
      "Epoch: 1194 trainginLoss: 0.5484418717006709 ValidationLoss:0.5420947674977578\n",
      "Epoch: 1195 trainginLoss: 0.5481402737982322 ValidationLoss:0.541918219146082\n",
      "Epoch: 1196 trainginLoss: 0.5481415426971128 ValidationLoss:0.5419023921934225\n",
      "Epoch: 1197 trainginLoss: 0.5482093651822749 ValidationLoss:0.5415984984171592\n",
      "Epoch: 1198 trainginLoss: 0.547577685957787 ValidationLoss:0.5412024635379598\n",
      "Epoch: 1199 trainginLoss: 0.5472313997729513 ValidationLoss:0.5410752340898676\n",
      "Epoch: 1200 trainginLoss: 0.5479470963446086 ValidationLoss:0.5407723657155441\n",
      "Epoch: 1201 trainginLoss: 0.5470754524205355 ValidationLoss:0.5409322843713276\n",
      "Epoch: 1202 trainginLoss: 0.5468629462606955 ValidationLoss:0.540452319484646\n",
      "Epoch: 1203 trainginLoss: 0.5465726276372103 ValidationLoss:0.5402557407395314\n",
      "Epoch: 1204 trainginLoss: 0.5476963500048491 ValidationLoss:0.5402037309387983\n",
      "Epoch: 1205 trainginLoss: 0.547391044613499 ValidationLoss:0.5399629796965647\n",
      "Epoch: 1206 trainginLoss: 0.5467020845253195 ValidationLoss:0.5398031760070283\n",
      "Epoch: 1207 trainginLoss: 0.5463949021877058 ValidationLoss:0.5394835769119909\n",
      "Epoch: 1208 trainginLoss: 0.5458461098222924 ValidationLoss:0.5393158979335073\n",
      "Epoch: 1209 trainginLoss: 0.545672465890846 ValidationLoss:0.5392355619850805\n",
      "Epoch: 1210 trainginLoss: 0.5456954824844463 ValidationLoss:0.5389280331336845\n",
      "Epoch: 1211 trainginLoss: 0.5452656185867002 ValidationLoss:0.5388948741605726\n",
      "Epoch: 1212 trainginLoss: 0.5457298267607721 ValidationLoss:0.5384740710258484\n",
      "Epoch: 1213 trainginLoss: 0.5451426958077706 ValidationLoss:0.5382829607543299\n",
      "Epoch: 1214 trainginLoss: 0.5459329133865817 ValidationLoss:0.5382023298134238\n",
      "Epoch: 1215 trainginLoss: 0.5456732547523191 ValidationLoss:0.5378858863297156\n",
      "Epoch: 1216 trainginLoss: 0.5444771827467336 ValidationLoss:0.5378173807920036\n",
      "Epoch: 1217 trainginLoss: 0.5450244665945936 ValidationLoss:0.5375146104117571\n",
      "Epoch: 1218 trainginLoss: 0.5448205991079343 ValidationLoss:0.537514952077704\n",
      "Epoch: 1219 trainginLoss: 0.5446148360735618 ValidationLoss:0.5375851354356539\n",
      "Epoch: 1220 trainginLoss: 0.5445454016627881 ValidationLoss:0.5376406348357766\n",
      "Epoch: 1221 trainginLoss: 0.5446283153239513 ValidationLoss:0.5376300308663966\n",
      "Epoch: 1222 trainginLoss: 0.5440226857294173 ValidationLoss:0.5369979139101707\n",
      "Epoch: 1223 trainginLoss: 0.5438870363587501 ValidationLoss:0.5363837139081147\n",
      "Epoch: 1224 trainginLoss: 0.5435660073421146 ValidationLoss:0.5362931940515162\n",
      "Epoch: 1225 trainginLoss: 0.543053966640626 ValidationLoss:0.5363658486786536\n",
      "Epoch: 1226 trainginLoss: 0.5429900120568756 ValidationLoss:0.5358919988244267\n",
      "Epoch: 1227 trainginLoss: 0.5437524246689457 ValidationLoss:0.5360994773395991\n",
      "Epoch: 1228 trainginLoss: 0.543093808145331 ValidationLoss:0.535719855154975\n",
      "Epoch: 1229 trainginLoss: 0.5435745908109934 ValidationLoss:0.5352562641693374\n",
      "Epoch: 1230 trainginLoss: 0.5422287811368904 ValidationLoss:0.5350296958018157\n",
      "Epoch: 1231 trainginLoss: 0.5421954661407726 ValidationLoss:0.5348896796420469\n",
      "Epoch: 1232 trainginLoss: 0.542234829608226 ValidationLoss:0.5351582391787384\n",
      "Epoch: 1233 trainginLoss: 0.5423394685623629 ValidationLoss:0.5349126437963065\n",
      "Epoch: 1234 trainginLoss: 0.5420580586331003 ValidationLoss:0.5346030859623925\n",
      "Epoch: 1235 trainginLoss: 0.541662717025552 ValidationLoss:0.5341482172578068\n",
      "Epoch: 1236 trainginLoss: 0.5412490539902809 ValidationLoss:0.5340266118615361\n",
      "Epoch: 1237 trainginLoss: 0.541124470281921 ValidationLoss:0.5338305818832527\n",
      "Epoch: 1238 trainginLoss: 0.5410935822749298 ValidationLoss:0.5337001578282502\n",
      "Epoch: 1239 trainginLoss: 0.5413546554194201 ValidationLoss:0.5334944276486413\n",
      "Epoch: 1240 trainginLoss: 0.5408026920068985 ValidationLoss:0.533104180077375\n",
      "Epoch: 1241 trainginLoss: 0.5409198579372175 ValidationLoss:0.5330100465629061\n",
      "Epoch: 1242 trainginLoss: 0.5410340336345186 ValidationLoss:0.5329619359161895\n",
      "Epoch: 1243 trainginLoss: 0.5405019793734455 ValidationLoss:0.532624639696994\n",
      "Epoch: 1244 trainginLoss: 0.5399908027392906 ValidationLoss:0.5324770369772184\n",
      "Epoch: 1245 trainginLoss: 0.5407844393845372 ValidationLoss:0.5321414892956362\n",
      "Epoch: 1246 trainginLoss: 0.5397401704084153 ValidationLoss:0.5319429215738329\n",
      "Epoch: 1247 trainginLoss: 0.540653777602535 ValidationLoss:0.5317699800103398\n",
      "Epoch: 1248 trainginLoss: 0.5401691738391082 ValidationLoss:0.5315881743269452\n",
      "Epoch: 1249 trainginLoss: 0.5392608826592464 ValidationLoss:0.5313926464420254\n",
      "Epoch: 1250 trainginLoss: 0.5389279283133129 ValidationLoss:0.5311921871314614\n",
      "Epoch: 1251 trainginLoss: 0.5391672993666373 ValidationLoss:0.5310582167011196\n",
      "Epoch: 1252 trainginLoss: 0.5392928779525245 ValidationLoss:0.5311877236527912\n",
      "Epoch: 1253 trainginLoss: 0.5392060167837462 ValidationLoss:0.5306311383085736\n",
      "Epoch: 1254 trainginLoss: 0.5383510769613638 ValidationLoss:0.5304259124448744\n",
      "Epoch: 1255 trainginLoss: 0.5386518732013318 ValidationLoss:0.5302795497037597\n",
      "Epoch: 1256 trainginLoss: 0.5383892515361709 ValidationLoss:0.5304174269660045\n",
      "Epoch: 1257 trainginLoss: 0.5379877642497121 ValidationLoss:0.5300951646546186\n",
      "Epoch: 1258 trainginLoss: 0.5383367440444511 ValidationLoss:0.5309048339471979\n",
      "Epoch: 1259 trainginLoss: 0.5382236474312392 ValidationLoss:0.5307096434851825\n",
      "Epoch: 1260 trainginLoss: 0.537813018232384 ValidationLoss:0.5294906125230304\n",
      "Epoch: 1261 trainginLoss: 0.5372216949526896 ValidationLoss:0.5291316268807751\n",
      "Epoch: 1262 trainginLoss: 0.5372245187727397 ValidationLoss:0.5290723123792874\n",
      "Epoch: 1263 trainginLoss: 0.5374106942807269 ValidationLoss:0.5296690993389841\n",
      "Epoch: 1264 trainginLoss: 0.537236616115442 ValidationLoss:0.5291282704321004\n",
      "Epoch: 1265 trainginLoss: 0.5376435358252302 ValidationLoss:0.5285985627416837\n",
      "Epoch: 1266 trainginLoss: 0.5367328776609177 ValidationLoss:0.5281612169944634\n",
      "Epoch: 1267 trainginLoss: 0.5362947967228473 ValidationLoss:0.5280487882888923\n",
      "Epoch: 1268 trainginLoss: 0.5366151052833403 ValidationLoss:0.5277386000600912\n",
      "Epoch: 1269 trainginLoss: 0.535964837410306 ValidationLoss:0.5275513762134617\n",
      "Epoch: 1270 trainginLoss: 0.5359822727689807 ValidationLoss:0.527390239804478\n",
      "Epoch: 1271 trainginLoss: 0.536266314303315 ValidationLoss:0.5275116380998643\n",
      "Epoch: 1272 trainginLoss: 0.5354920365666384 ValidationLoss:0.5269890635700549\n",
      "Epoch: 1273 trainginLoss: 0.5351792773944419 ValidationLoss:0.5268256670337612\n",
      "Epoch: 1274 trainginLoss: 0.5355090366913968 ValidationLoss:0.5267912232269675\n",
      "Epoch: 1275 trainginLoss: 0.5357717239616702 ValidationLoss:0.5267564009811918\n",
      "Epoch: 1276 trainginLoss: 0.5352622506602499 ValidationLoss:0.5262379660444745\n",
      "Epoch: 1277 trainginLoss: 0.5352789947650577 ValidationLoss:0.5264750201823347\n",
      "Epoch: 1278 trainginLoss: 0.5345709023859677 ValidationLoss:0.5258752245013997\n",
      "Epoch: 1279 trainginLoss: 0.5347851154788229 ValidationLoss:0.5262578802593684\n",
      "Epoch: 1280 trainginLoss: 0.5345062769499401 ValidationLoss:0.5258006352489277\n",
      "Epoch: 1281 trainginLoss: 0.5344167943768854 ValidationLoss:0.5252932716224153\n",
      "Epoch: 1282 trainginLoss: 0.5341432650617305 ValidationLoss:0.5251162791656235\n",
      "Epoch: 1283 trainginLoss: 0.5339159097447491 ValidationLoss:0.5253202038296199\n",
      "Epoch: 1284 trainginLoss: 0.5340331928842019 ValidationLoss:0.5247171361567611\n",
      "Epoch: 1285 trainginLoss: 0.5340971714698228 ValidationLoss:0.5248491830744986\n",
      "Epoch: 1286 trainginLoss: 0.5332592641747238 ValidationLoss:0.5244476330482354\n",
      "Epoch: 1287 trainginLoss: 0.5336243698260928 ValidationLoss:0.5245555340233495\n",
      "Epoch: 1288 trainginLoss: 0.5329270254845587 ValidationLoss:0.5239903435868732\n",
      "Epoch: 1289 trainginLoss: 0.5332092162746711 ValidationLoss:0.5240782515477326\n",
      "Epoch: 1290 trainginLoss: 0.5324957542771461 ValidationLoss:0.5235886230307111\n",
      "Epoch: 1291 trainginLoss: 0.5323773882533079 ValidationLoss:0.5233815956923922\n",
      "Epoch: 1292 trainginLoss: 0.5326526445030366 ValidationLoss:0.5233343298152342\n",
      "Epoch: 1293 trainginLoss: 0.5321611942060842 ValidationLoss:0.5231510002734298\n",
      "Epoch: 1294 trainginLoss: 0.5322054304532556 ValidationLoss:0.5228718006004721\n",
      "Epoch: 1295 trainginLoss: 0.5315516751084551 ValidationLoss:0.5226723885132094\n",
      "Epoch: 1296 trainginLoss: 0.531498171339099 ValidationLoss:0.5225629402419268\n",
      "Epoch: 1297 trainginLoss: 0.5319321103544044 ValidationLoss:0.5222554178561194\n",
      "Epoch: 1298 trainginLoss: 0.5316454108129411 ValidationLoss:0.5220758282532126\n",
      "Epoch: 1299 trainginLoss: 0.5311345228012776 ValidationLoss:0.5220452326839253\n",
      "Epoch: 1300 trainginLoss: 0.5312146584459599 ValidationLoss:0.5216918122970452\n",
      "Epoch: 1301 trainginLoss: 0.5307008754086975 ValidationLoss:0.521564291695417\n",
      "Epoch: 1302 trainginLoss: 0.5304646880034632 ValidationLoss:0.5214442709744986\n",
      "Epoch: 1303 trainginLoss: 0.5305329805252536 ValidationLoss:0.5211011741120937\n",
      "Epoch: 1304 trainginLoss: 0.5308020698944195 ValidationLoss:0.520930627847122\n",
      "Epoch: 1305 trainginLoss: 0.5302202985590736 ValidationLoss:0.520957161006281\n",
      "Epoch: 1306 trainginLoss: 0.5301686371332847 ValidationLoss:0.521113767866361\n",
      "Epoch: 1307 trainginLoss: 0.5299050761949295 ValidationLoss:0.5206068917856378\n",
      "Epoch: 1308 trainginLoss: 0.5300507217445629 ValidationLoss:0.5205200318562783\n",
      "Epoch: 1309 trainginLoss: 0.5302394522516519 ValidationLoss:0.5212240435309329\n",
      "Epoch: 1310 trainginLoss: 0.529566533613525 ValidationLoss:0.5201063032877647\n",
      "Epoch: 1311 trainginLoss: 0.5301137406553998 ValidationLoss:0.5199013958543034\n",
      "Epoch: 1312 trainginLoss: 0.529232394375257 ValidationLoss:0.5196193543531127\n",
      "Epoch: 1313 trainginLoss: 0.5295933975069315 ValidationLoss:0.5198139271493686\n",
      "Epoch: 1314 trainginLoss: 0.5298138838886415 ValidationLoss:0.5194077412960894\n",
      "Epoch: 1315 trainginLoss: 0.5286061477741139 ValidationLoss:0.5189194309509406\n",
      "Epoch: 1316 trainginLoss: 0.5282495253838149 ValidationLoss:0.5188095100855423\n",
      "Epoch: 1317 trainginLoss: 0.5284599849841739 ValidationLoss:0.5186618608943486\n",
      "Epoch: 1318 trainginLoss: 0.5286570483406118 ValidationLoss:0.5185111763113636\n",
      "Epoch: 1319 trainginLoss: 0.529010566689024 ValidationLoss:0.5180947546231545\n",
      "Epoch: 1320 trainginLoss: 0.5281115786341213 ValidationLoss:0.5179248878511332\n",
      "Epoch: 1321 trainginLoss: 0.5276882588463342 ValidationLoss:0.5185041143732556\n",
      "Epoch: 1322 trainginLoss: 0.527946035733959 ValidationLoss:0.5179136565176107\n",
      "Epoch: 1323 trainginLoss: 0.5277954794016461 ValidationLoss:0.5175002653720016\n",
      "Epoch: 1324 trainginLoss: 0.52740639488169 ValidationLoss:0.5172066872402773\n",
      "Epoch: 1325 trainginLoss: 0.5272678564858917 ValidationLoss:0.5169785495531761\n",
      "Epoch: 1326 trainginLoss: 0.5270298299373396 ValidationLoss:0.516827714847306\n",
      "Epoch: 1327 trainginLoss: 0.5269953388495733 ValidationLoss:0.5168703247935085\n",
      "Epoch: 1328 trainginLoss: 0.5268808475276768 ValidationLoss:0.5165340282149234\n",
      "Epoch: 1329 trainginLoss: 0.5270267612982116 ValidationLoss:0.516563963688026\n",
      "Epoch: 1330 trainginLoss: 0.5270542886433185 ValidationLoss:0.5160626463970895\n",
      "Epoch: 1331 trainginLoss: 0.5258386811154001 ValidationLoss:0.515877997673164\n",
      "Epoch: 1332 trainginLoss: 0.5258570841494823 ValidationLoss:0.5159934103488922\n",
      "Epoch: 1333 trainginLoss: 0.5256200328769299 ValidationLoss:0.5155058561745337\n",
      "Epoch: 1334 trainginLoss: 0.5258041996283819 ValidationLoss:0.51531994322599\n",
      "Epoch: 1335 trainginLoss: 0.5253632714684378 ValidationLoss:0.515120969687478\n",
      "Epoch: 1336 trainginLoss: 0.5255375112463164 ValidationLoss:0.5149383601495775\n",
      "Epoch: 1337 trainginLoss: 0.526171772672026 ValidationLoss:0.5149577680280653\n",
      "Epoch: 1338 trainginLoss: 0.5250095545045481 ValidationLoss:0.514577951673734\n",
      "Epoch: 1339 trainginLoss: 0.5246102301866417 ValidationLoss:0.5144453161853855\n",
      "Epoch: 1340 trainginLoss: 0.5249159280085723 ValidationLoss:0.5142789994255971\n",
      "Epoch: 1341 trainginLoss: 0.5248021271404804 ValidationLoss:0.5140006076481383\n",
      "Epoch: 1342 trainginLoss: 0.5246931614491763 ValidationLoss:0.5142626043093407\n",
      "Epoch: 1343 trainginLoss: 0.5240418330935024 ValidationLoss:0.5138290036532839\n",
      "Epoch: 1344 trainginLoss: 0.5241625292989232 ValidationLoss:0.5134381917573638\n",
      "Epoch: 1345 trainginLoss: 0.5243383301984543 ValidationLoss:0.5132582470522089\n",
      "Epoch: 1346 trainginLoss: 0.5235494699254132 ValidationLoss:0.5130816263667608\n",
      "Epoch: 1347 trainginLoss: 0.5246101777825579 ValidationLoss:0.5130256391177743\n",
      "Epoch: 1348 trainginLoss: 0.5235995570285208 ValidationLoss:0.5127856437432564\n",
      "Epoch: 1349 trainginLoss: 0.5244430311574232 ValidationLoss:0.5125786992452912\n",
      "Epoch: 1350 trainginLoss: 0.5239743238327487 ValidationLoss:0.5126414035336446\n",
      "Epoch: 1351 trainginLoss: 0.5232500173901552 ValidationLoss:0.5121529422574124\n",
      "Epoch: 1352 trainginLoss: 0.5225225491011702 ValidationLoss:0.5120264284691568\n",
      "Epoch: 1353 trainginLoss: 0.5225816021029581 ValidationLoss:0.5118055578005516\n",
      "Epoch: 1354 trainginLoss: 0.5225109447968886 ValidationLoss:0.5115902872408851\n",
      "Epoch: 1355 trainginLoss: 0.5223150061280936 ValidationLoss:0.5115598395719366\n",
      "Epoch: 1356 trainginLoss: 0.5224704154385816 ValidationLoss:0.511271692837699\n",
      "Epoch: 1357 trainginLoss: 0.5217723860436638 ValidationLoss:0.5113067373380823\n",
      "Epoch: 1358 trainginLoss: 0.5216778468765668 ValidationLoss:0.5109103572570671\n",
      "Epoch: 1359 trainginLoss: 0.521289304998897 ValidationLoss:0.5106944326627052\n",
      "Epoch: 1360 trainginLoss: 0.5211466170797412 ValidationLoss:0.5106180635549254\n",
      "Epoch: 1361 trainginLoss: 0.5226032327885596 ValidationLoss:0.5105134349758342\n",
      "Epoch: 1362 trainginLoss: 0.5213518002689285 ValidationLoss:0.5105097963648327\n",
      "Epoch: 1363 trainginLoss: 0.5210045058055212 ValidationLoss:0.5104856463812165\n",
      "Epoch: 1364 trainginLoss: 0.520881284803352 ValidationLoss:0.5098316784632408\n",
      "Epoch: 1365 trainginLoss: 0.5208133835120489 ValidationLoss:0.5095744907855988\n",
      "Epoch: 1366 trainginLoss: 0.5209626687453097 ValidationLoss:0.5094644211106382\n",
      "Epoch: 1367 trainginLoss: 0.5214228802079323 ValidationLoss:0.5092110708608466\n",
      "Epoch: 1368 trainginLoss: 0.5201234061445966 ValidationLoss:0.5095783775135622\n",
      "Epoch: 1369 trainginLoss: 0.521075751157415 ValidationLoss:0.5093747040982973\n",
      "Epoch: 1370 trainginLoss: 0.5198029239705745 ValidationLoss:0.5086749774924779\n",
      "Epoch: 1371 trainginLoss: 0.5206653611772012 ValidationLoss:0.5087601054522951\n",
      "Epoch: 1372 trainginLoss: 0.5197570183933181 ValidationLoss:0.5084174588575201\n",
      "Epoch: 1373 trainginLoss: 0.519912404101967 ValidationLoss:0.5081401307704085\n",
      "Epoch: 1374 trainginLoss: 0.5194568294006706 ValidationLoss:0.508825283131357\n",
      "Epoch: 1375 trainginLoss: 0.5193967483187681 ValidationLoss:0.5078115843110166\n",
      "Epoch: 1376 trainginLoss: 0.5195355555355149 ValidationLoss:0.5076106790768898\n",
      "Epoch: 1377 trainginLoss: 0.5190267268843298 ValidationLoss:0.5077949990660457\n",
      "Epoch: 1378 trainginLoss: 0.5189419208757029 ValidationLoss:0.5080495942447145\n",
      "Epoch: 1379 trainginLoss: 0.5187564816250897 ValidationLoss:0.507155013993635\n",
      "Epoch: 1380 trainginLoss: 0.5183739698173215 ValidationLoss:0.5069547126858921\n",
      "Epoch: 1381 trainginLoss: 0.518199333408535 ValidationLoss:0.506880847478317\n",
      "Epoch: 1382 trainginLoss: 0.5180604333845561 ValidationLoss:0.5066023707389832\n",
      "Epoch: 1383 trainginLoss: 0.5178366317445 ValidationLoss:0.5064944073305292\n",
      "Epoch: 1384 trainginLoss: 0.5183934585360073 ValidationLoss:0.5062031659029298\n",
      "Epoch: 1385 trainginLoss: 0.5177030671362909 ValidationLoss:0.5059964985160504\n",
      "Epoch: 1386 trainginLoss: 0.5175173754660075 ValidationLoss:0.505838504080045\n",
      "Epoch: 1387 trainginLoss: 0.5173986590148618 ValidationLoss:0.505700703394615\n",
      "Epoch: 1388 trainginLoss: 0.5172476062438632 ValidationLoss:0.5057442095320104\n",
      "Epoch: 1389 trainginLoss: 0.5167722762031043 ValidationLoss:0.5053152125770762\n",
      "Epoch: 1390 trainginLoss: 0.5166645290067532 ValidationLoss:0.5050921380519867\n",
      "Epoch: 1391 trainginLoss: 0.516480253046791 ValidationLoss:0.5049644204519563\n",
      "Epoch: 1392 trainginLoss: 0.5164385257951365 ValidationLoss:0.5048279479398565\n",
      "Epoch: 1393 trainginLoss: 0.5167406925419032 ValidationLoss:0.50459072458542\n",
      "Epoch: 1394 trainginLoss: 0.516568556727979 ValidationLoss:0.5044200533527439\n",
      "Epoch: 1395 trainginLoss: 0.5159933991080162 ValidationLoss:0.5044791983345808\n",
      "Epoch: 1396 trainginLoss: 0.5159030382665212 ValidationLoss:0.5040322762424663\n",
      "Epoch: 1397 trainginLoss: 0.5155784031688767 ValidationLoss:0.5039512432227701\n",
      "Epoch: 1398 trainginLoss: 0.5162419828792546 ValidationLoss:0.503690171948934\n",
      "Epoch: 1399 trainginLoss: 0.5153504886083154 ValidationLoss:0.5035941941253209\n",
      "Epoch: 1400 trainginLoss: 0.5157999080299531 ValidationLoss:0.5033313442084749\n",
      "Epoch: 1401 trainginLoss: 0.5153321515793768 ValidationLoss:0.503225549095768\n",
      "Epoch: 1402 trainginLoss: 0.5147137729913597 ValidationLoss:0.503020424559965\n",
      "Epoch: 1403 trainginLoss: 0.5149527976176883 ValidationLoss:0.502973298602185\n",
      "Epoch: 1404 trainginLoss: 0.5152997536547232 ValidationLoss:0.5029232489860664\n",
      "Epoch: 1405 trainginLoss: 0.5147501478259195 ValidationLoss:0.5026171932786198\n",
      "Epoch: 1406 trainginLoss: 0.5146403804721448 ValidationLoss:0.5027488055875746\n",
      "Epoch: 1407 trainginLoss: 0.514904561458818 ValidationLoss:0.5021976472967762\n",
      "Epoch: 1408 trainginLoss: 0.5145011584230718 ValidationLoss:0.5020904550107859\n",
      "Epoch: 1409 trainginLoss: 0.5150447295816153 ValidationLoss:0.501924627413184\n",
      "Epoch: 1410 trainginLoss: 0.5138971399540869 ValidationLoss:0.5020286496412956\n",
      "Epoch: 1411 trainginLoss: 0.5140314782225845 ValidationLoss:0.5013999688423286\n",
      "Epoch: 1412 trainginLoss: 0.5135492534445436 ValidationLoss:0.501249558137635\n",
      "Epoch: 1413 trainginLoss: 0.5132630261398802 ValidationLoss:0.5010788679122925\n",
      "Epoch: 1414 trainginLoss: 0.5135642754151517 ValidationLoss:0.5008764253834547\n",
      "Epoch: 1415 trainginLoss: 0.5127915632004706 ValidationLoss:0.5010331707485651\n",
      "Epoch: 1416 trainginLoss: 0.5127607776014597 ValidationLoss:0.500550964222116\n",
      "Epoch: 1417 trainginLoss: 0.5125776141281896 ValidationLoss:0.5003753411567817\n",
      "Epoch: 1418 trainginLoss: 0.5125314129278964 ValidationLoss:0.5002978106676522\n",
      "Epoch: 1419 trainginLoss: 0.5127855071285427 ValidationLoss:0.5001525056564202\n",
      "Epoch: 1420 trainginLoss: 0.5126850396994777 ValidationLoss:0.5000322196443202\n",
      "Epoch: 1421 trainginLoss: 0.5124055373188633 ValidationLoss:0.4997055691177562\n",
      "Epoch: 1422 trainginLoss: 0.5122094498384719 ValidationLoss:0.49956696275937357\n",
      "Epoch: 1423 trainginLoss: 0.5117349776645634 ValidationLoss:0.49936336602194836\n",
      "Epoch: 1424 trainginLoss: 0.5119264189828963 ValidationLoss:0.4991612835455749\n",
      "Epoch: 1425 trainginLoss: 0.5117854695992182 ValidationLoss:0.49906697000487377\n",
      "Epoch: 1426 trainginLoss: 0.511524209240139 ValidationLoss:0.498924294871799\n",
      "Epoch: 1427 trainginLoss: 0.5111212248370152 ValidationLoss:0.4987999626135422\n",
      "Epoch: 1428 trainginLoss: 0.5114668723720832 ValidationLoss:0.4988894715147503\n",
      "Epoch: 1429 trainginLoss: 0.5122615362973821 ValidationLoss:0.4983768416663348\n",
      "Epoch: 1430 trainginLoss: 0.510604706786623 ValidationLoss:0.49815249180389665\n",
      "Epoch: 1431 trainginLoss: 0.5104833689312007 ValidationLoss:0.4979681572671664\n",
      "Epoch: 1432 trainginLoss: 0.5110362084119912 ValidationLoss:0.49784084275617435\n",
      "Epoch: 1433 trainginLoss: 0.5101493200999778 ValidationLoss:0.4976149564072237\n",
      "Epoch: 1434 trainginLoss: 0.5102487462478996 ValidationLoss:0.4975682592998117\n",
      "Epoch: 1435 trainginLoss: 0.5103203142249344 ValidationLoss:0.49735887202165896\n",
      "Epoch: 1436 trainginLoss: 0.5105433348041253 ValidationLoss:0.49710899227756566\n",
      "Epoch: 1437 trainginLoss: 0.5099710158053661 ValidationLoss:0.49693756103515624\n",
      "Epoch: 1438 trainginLoss: 0.5109421666836579 ValidationLoss:0.4970415426512896\n",
      "Epoch: 1439 trainginLoss: 0.5095001017087258 ValidationLoss:0.4966155445171615\n",
      "Epoch: 1440 trainginLoss: 0.5090310983609834 ValidationLoss:0.4964349389076233\n",
      "Epoch: 1441 trainginLoss: 0.5093016364430422 ValidationLoss:0.49638121744333685\n",
      "Epoch: 1442 trainginLoss: 0.5089409075327368 ValidationLoss:0.4962371168500286\n",
      "Epoch: 1443 trainginLoss: 0.5094324534371395 ValidationLoss:0.4959634882918859\n",
      "Epoch: 1444 trainginLoss: 0.509510976356148 ValidationLoss:0.49580268273919315\n",
      "Epoch: 1445 trainginLoss: 0.5090230555342348 ValidationLoss:0.49595185215190307\n",
      "Epoch: 1446 trainginLoss: 0.508865615465497 ValidationLoss:0.4954595164727357\n",
      "Epoch: 1447 trainginLoss: 0.5084183356105881 ValidationLoss:0.49557348459453904\n",
      "Epoch: 1448 trainginLoss: 0.5084686879343634 ValidationLoss:0.4952154848535182\n",
      "Epoch: 1449 trainginLoss: 0.5084948335717988 ValidationLoss:0.4950158581895343\n",
      "Epoch: 1450 trainginLoss: 0.5081201215718416 ValidationLoss:0.49509433912018597\n",
      "Epoch: 1451 trainginLoss: 0.5088085862214133 ValidationLoss:0.4946188131631431\n",
      "Epoch: 1452 trainginLoss: 0.5076796452471074 ValidationLoss:0.49443768060813514\n",
      "Epoch: 1453 trainginLoss: 0.5076079084569176 ValidationLoss:0.4942840716596377\n",
      "Epoch: 1454 trainginLoss: 0.5092626886079775 ValidationLoss:0.4941078119358774\n",
      "Epoch: 1455 trainginLoss: 0.5088096159016526 ValidationLoss:0.494060427354554\n",
      "Epoch: 1456 trainginLoss: 0.5068308769056461 ValidationLoss:0.49376336022958917\n",
      "Epoch: 1457 trainginLoss: 0.5071138595574655 ValidationLoss:0.4939696771613622\n",
      "Epoch: 1458 trainginLoss: 0.5072396945633344 ValidationLoss:0.49343802140930954\n",
      "Epoch: 1459 trainginLoss: 0.5063474622348811 ValidationLoss:0.493330343800076\n",
      "Epoch: 1460 trainginLoss: 0.5064636860917878 ValidationLoss:0.4938890544034667\n",
      "Epoch: 1461 trainginLoss: 0.5068757097993121 ValidationLoss:0.49300224821446303\n",
      "Epoch: 1462 trainginLoss: 0.5077473097199562 ValidationLoss:0.49320324528015264\n",
      "Epoch: 1463 trainginLoss: 0.5060520950179772 ValidationLoss:0.49284455321602905\n",
      "Epoch: 1464 trainginLoss: 0.5062642139476418 ValidationLoss:0.49297586325871745\n",
      "Epoch: 1465 trainginLoss: 0.5057762881253389 ValidationLoss:0.49240056943085236\n",
      "Epoch: 1466 trainginLoss: 0.5056650764590142 ValidationLoss:0.4921692481485464\n",
      "Epoch: 1467 trainginLoss: 0.5058603378750334 ValidationLoss:0.49195168048648513\n",
      "Epoch: 1468 trainginLoss: 0.5062796113475058 ValidationLoss:0.4917875024221711\n",
      "Epoch: 1469 trainginLoss: 0.5048536782296712 ValidationLoss:0.49162432046259863\n",
      "Epoch: 1470 trainginLoss: 0.504851335247091 ValidationLoss:0.49148997927116134\n",
      "Epoch: 1471 trainginLoss: 0.504603249514663 ValidationLoss:0.4912908319699562\n",
      "Epoch: 1472 trainginLoss: 0.5046500067582866 ValidationLoss:0.49118278764062007\n",
      "Epoch: 1473 trainginLoss: 0.5047822606643574 ValidationLoss:0.49096069265220127\n",
      "Epoch: 1474 trainginLoss: 0.5049386412505336 ValidationLoss:0.49116241861197907\n",
      "Epoch: 1475 trainginLoss: 0.5045143405863103 ValidationLoss:0.4908999377388065\n",
      "Epoch: 1476 trainginLoss: 0.5043129092894945 ValidationLoss:0.4904878923448466\n",
      "Epoch: 1477 trainginLoss: 0.5049414114664065 ValidationLoss:0.4917033768306344\n",
      "Epoch: 1478 trainginLoss: 0.5052114397087353 ValidationLoss:0.4907274489685641\n",
      "Epoch: 1479 trainginLoss: 0.5042729125726944 ValidationLoss:0.4907933500863738\n",
      "Epoch: 1480 trainginLoss: 0.5042610608491321 ValidationLoss:0.4900838442778183\n",
      "Epoch: 1481 trainginLoss: 0.5037761492617179 ValidationLoss:0.48994475512181296\n",
      "Epoch: 1482 trainginLoss: 0.5038583480671748 ValidationLoss:0.48950741088996497\n",
      "Epoch: 1483 trainginLoss: 0.5033636777192955 ValidationLoss:0.4893592467752554\n",
      "Epoch: 1484 trainginLoss: 0.5033329399240097 ValidationLoss:0.48945980334686023\n",
      "Epoch: 1485 trainginLoss: 0.503928449729945 ValidationLoss:0.4891826821585833\n",
      "Epoch: 1486 trainginLoss: 0.5029075613757908 ValidationLoss:0.4889733401395507\n",
      "Epoch: 1487 trainginLoss: 0.5028819053765111 ValidationLoss:0.48872755664890094\n",
      "Epoch: 1488 trainginLoss: 0.5024671682575405 ValidationLoss:0.48855790849459374\n",
      "Epoch: 1489 trainginLoss: 0.5022949084339526 ValidationLoss:0.48841321296611073\n",
      "Epoch: 1490 trainginLoss: 0.5025014301274446 ValidationLoss:0.48836790414179787\n",
      "Epoch: 1491 trainginLoss: 0.5031717763651138 ValidationLoss:0.4880759563486455\n",
      "Epoch: 1492 trainginLoss: 0.501678703815345 ValidationLoss:0.48794929324570346\n",
      "Epoch: 1493 trainginLoss: 0.5020390832984207 ValidationLoss:0.48847401445194827\n",
      "Epoch: 1494 trainginLoss: 0.5026181898261076 ValidationLoss:0.487662440740456\n",
      "Epoch: 1495 trainginLoss: 0.5018923788662725 ValidationLoss:0.4877025929547973\n",
      "Epoch: 1496 trainginLoss: 0.5014536616786215 ValidationLoss:0.48741513460369434\n",
      "Epoch: 1497 trainginLoss: 0.5016170414102158 ValidationLoss:0.4871906038058006\n",
      "Epoch: 1498 trainginLoss: 0.5010574956068257 ValidationLoss:0.48704466072179503\n",
      "Epoch: 1499 trainginLoss: 0.5010738616821749 ValidationLoss:0.4870790411860256\n",
      "Epoch: 1500 trainginLoss: 0.5011571965761633 ValidationLoss:0.48672462125956\n",
      "Epoch: 1501 trainginLoss: 0.5006088578461001 ValidationLoss:0.486492210727627\n",
      "Epoch: 1502 trainginLoss: 0.500378903926619 ValidationLoss:0.48633536888381185\n",
      "Epoch: 1503 trainginLoss: 0.5010554994512725 ValidationLoss:0.48633694507307923\n",
      "Epoch: 1504 trainginLoss: 0.5013517953405444 ValidationLoss:0.48633568367715607\n",
      "Epoch: 1505 trainginLoss: 0.50033025213536 ValidationLoss:0.48597176711438067\n",
      "Epoch: 1506 trainginLoss: 0.5014376418302523 ValidationLoss:0.4857672508490288\n",
      "Epoch: 1507 trainginLoss: 0.5000004592357866 ValidationLoss:0.48586209143622444\n",
      "Epoch: 1508 trainginLoss: 0.49986188723736963 ValidationLoss:0.4854118374444671\n",
      "Epoch: 1509 trainginLoss: 0.5002858408745503 ValidationLoss:0.4862620024357812\n",
      "Epoch: 1510 trainginLoss: 0.500103794888362 ValidationLoss:0.4852407576674122\n",
      "Epoch: 1511 trainginLoss: 0.4993384720495083 ValidationLoss:0.4851326392868818\n",
      "Epoch: 1512 trainginLoss: 0.4993939859755087 ValidationLoss:0.4848001782166756\n",
      "Epoch: 1513 trainginLoss: 0.4991348575825659 ValidationLoss:0.4849157038381544\n",
      "Epoch: 1514 trainginLoss: 0.5000676616726306 ValidationLoss:0.48535579099493514\n",
      "Epoch: 1515 trainginLoss: 0.49960469399522617 ValidationLoss:0.48510825128878576\n",
      "Epoch: 1516 trainginLoss: 0.4995572903012269 ValidationLoss:0.4842054103390645\n",
      "Epoch: 1517 trainginLoss: 0.4997480203641341 ValidationLoss:0.48403052889694603\n",
      "Epoch: 1518 trainginLoss: 0.49858966969803675 ValidationLoss:0.48390443607912226\n",
      "Epoch: 1519 trainginLoss: 0.49830980108888356 ValidationLoss:0.4838157659870083\n",
      "Epoch: 1520 trainginLoss: 0.49904467515497397 ValidationLoss:0.48360423314369333\n",
      "Epoch: 1521 trainginLoss: 0.4979498632402228 ValidationLoss:0.4837201779171572\n",
      "Epoch: 1522 trainginLoss: 0.5000688405644974 ValidationLoss:0.48390225161940364\n",
      "Epoch: 1523 trainginLoss: 0.49785189640602007 ValidationLoss:0.4832369346739882\n",
      "Epoch: 1524 trainginLoss: 0.4996359458305692 ValidationLoss:0.4830126217866348\n",
      "Epoch: 1525 trainginLoss: 0.49752563198140803 ValidationLoss:0.48350307830309464\n",
      "Epoch: 1526 trainginLoss: 0.4976851504121051 ValidationLoss:0.4828356747910128\n",
      "Epoch: 1527 trainginLoss: 0.4982399284439599 ValidationLoss:0.4825449030278093\n",
      "Epoch: 1528 trainginLoss: 0.49711517639608194 ValidationLoss:0.48237756187632935\n",
      "Epoch: 1529 trainginLoss: 0.4975114764783206 ValidationLoss:0.4823253968004453\n",
      "Epoch: 1530 trainginLoss: 0.4971254191942663 ValidationLoss:0.482389560089273\n",
      "Epoch: 1531 trainginLoss: 0.4970823844407229 ValidationLoss:0.48226433527671686\n",
      "Epoch: 1532 trainginLoss: 0.49722615944459136 ValidationLoss:0.48180280752101184\n",
      "Epoch: 1533 trainginLoss: 0.496314464199463 ValidationLoss:0.48165951805599666\n",
      "Epoch: 1534 trainginLoss: 0.49734194406727017 ValidationLoss:0.4815574582350456\n",
      "Epoch: 1535 trainginLoss: 0.49667275071944167 ValidationLoss:0.4813379382683059\n",
      "Epoch: 1536 trainginLoss: 0.4965564170539779 ValidationLoss:0.48144972112219214\n",
      "Epoch: 1537 trainginLoss: 0.4959378894543488 ValidationLoss:0.48105077107073896\n",
      "Epoch: 1538 trainginLoss: 0.49566052504033853 ValidationLoss:0.48102960222858493\n",
      "Epoch: 1539 trainginLoss: 0.4960564102902508 ValidationLoss:0.48079200558743235\n",
      "Epoch: 1540 trainginLoss: 0.496105770136686 ValidationLoss:0.4811119080600092\n",
      "Epoch: 1541 trainginLoss: 0.49644194573364003 ValidationLoss:0.48076570064334545\n",
      "Epoch: 1542 trainginLoss: 0.49566966235237636 ValidationLoss:0.48086496217776153\n",
      "Epoch: 1543 trainginLoss: 0.4953331203268678 ValidationLoss:0.4802106326919491\n",
      "Epoch: 1544 trainginLoss: 0.4951739083200493 ValidationLoss:0.48040940498901624\n",
      "Epoch: 1545 trainginLoss: 0.49520483673018895 ValidationLoss:0.4799064899905253\n",
      "Epoch: 1546 trainginLoss: 0.4949364888188023 ValidationLoss:0.4801039349224608\n",
      "Epoch: 1547 trainginLoss: 0.4945720870222821 ValidationLoss:0.4797631599135318\n",
      "Epoch: 1548 trainginLoss: 0.4948895061576126 ValidationLoss:0.47944587054899185\n",
      "Epoch: 1549 trainginLoss: 0.4945550382937361 ValidationLoss:0.4794955843586033\n",
      "Epoch: 1550 trainginLoss: 0.49540829258477126 ValidationLoss:0.47942255523245214\n",
      "Epoch: 1551 trainginLoss: 0.4947285184124172 ValidationLoss:0.47916916578502977\n",
      "Epoch: 1552 trainginLoss: 0.49458824588148387 ValidationLoss:0.47921605574882636\n",
      "Epoch: 1553 trainginLoss: 0.49462981672094974 ValidationLoss:0.47891889679229865\n",
      "Epoch: 1554 trainginLoss: 0.4948801974322172 ValidationLoss:0.4786086852267637\n",
      "Epoch: 1555 trainginLoss: 0.4937438420801355 ValidationLoss:0.4784668505191803\n",
      "Epoch: 1556 trainginLoss: 0.4942649682896249 ValidationLoss:0.47876120209693906\n",
      "Epoch: 1557 trainginLoss: 0.4943598592841385 ValidationLoss:0.47818224824081035\n",
      "Epoch: 1558 trainginLoss: 0.4932551752000847 ValidationLoss:0.47804131588693394\n",
      "Epoch: 1559 trainginLoss: 0.4937052850755269 ValidationLoss:0.47850440215256257\n",
      "Epoch: 1560 trainginLoss: 0.49415850119302734 ValidationLoss:0.47864586175498314\n",
      "Epoch: 1561 trainginLoss: 0.4933255774862814 ValidationLoss:0.47790067195892333\n",
      "Epoch: 1562 trainginLoss: 0.4928999359175663 ValidationLoss:0.4774838746604273\n",
      "Epoch: 1563 trainginLoss: 0.4929548157941575 ValidationLoss:0.47734877598487724\n",
      "Epoch: 1564 trainginLoss: 0.49271995409223057 ValidationLoss:0.47723104105157366\n",
      "Epoch: 1565 trainginLoss: 0.4925818353291326 ValidationLoss:0.4771583037861323\n",
      "Epoch: 1566 trainginLoss: 0.49226166377931635 ValidationLoss:0.47691609091677906\n",
      "Epoch: 1567 trainginLoss: 0.49282296711966495 ValidationLoss:0.4768161448381715\n",
      "Epoch: 1568 trainginLoss: 0.4923852382090268 ValidationLoss:0.4767574251708338\n",
      "Epoch: 1569 trainginLoss: 0.4921333289786473 ValidationLoss:0.4766875694363804\n",
      "Epoch: 1570 trainginLoss: 0.49272223206974514 ValidationLoss:0.47644734897855984\n",
      "Epoch: 1571 trainginLoss: 0.49170939674313435 ValidationLoss:0.4763649122189667\n",
      "Epoch: 1572 trainginLoss: 0.4916433069929981 ValidationLoss:0.47612120994066787\n",
      "Epoch: 1573 trainginLoss: 0.49295471418623954 ValidationLoss:0.4759385204921334\n",
      "Epoch: 1574 trainginLoss: 0.4912877498857127 ValidationLoss:0.47582647062964356\n",
      "Epoch: 1575 trainginLoss: 0.4920433821694163 ValidationLoss:0.47699828885369383\n",
      "Epoch: 1576 trainginLoss: 0.49178757863556777 ValidationLoss:0.47553500694743656\n",
      "Epoch: 1577 trainginLoss: 0.4911635122043174 ValidationLoss:0.47539872739274625\n",
      "Epoch: 1578 trainginLoss: 0.49085749615758856 ValidationLoss:0.47528557353100537\n",
      "Epoch: 1579 trainginLoss: 0.4906976198990073 ValidationLoss:0.4751707103292821\n",
      "Epoch: 1580 trainginLoss: 0.4910478119882161 ValidationLoss:0.4749969883490417\n",
      "Epoch: 1581 trainginLoss: 0.4907885621858123 ValidationLoss:0.47485292301339616\n",
      "Epoch: 1582 trainginLoss: 0.49032193722340883 ValidationLoss:0.47494514200647\n",
      "Epoch: 1583 trainginLoss: 0.490957348138694 ValidationLoss:0.47566948856337593\n",
      "Epoch: 1584 trainginLoss: 0.49067121264118474 ValidationLoss:0.47481417241743057\n",
      "Epoch: 1585 trainginLoss: 0.49090022368719116 ValidationLoss:0.47444264211897125\n",
      "Epoch: 1586 trainginLoss: 0.4898746213656944 ValidationLoss:0.474251752586688\n",
      "Epoch: 1587 trainginLoss: 0.4897153419536232 ValidationLoss:0.474079948966786\n",
      "Epoch: 1588 trainginLoss: 0.4895700616324508 ValidationLoss:0.4739160185143099\n",
      "Epoch: 1589 trainginLoss: 0.48971361041869094 ValidationLoss:0.47376767346414467\n",
      "Epoch: 1590 trainginLoss: 0.49091134455380026 ValidationLoss:0.4737586596254575\n",
      "Epoch: 1591 trainginLoss: 0.49059491869587224 ValidationLoss:0.47362755042011456\n",
      "Epoch: 1592 trainginLoss: 0.4896142474756945 ValidationLoss:0.4742527867777873\n",
      "Epoch: 1593 trainginLoss: 0.4896182093844318 ValidationLoss:0.4739475309848785\n",
      "Epoch: 1594 trainginLoss: 0.48938453757522893 ValidationLoss:0.4732476332430112\n",
      "Epoch: 1595 trainginLoss: 0.4886912763518775 ValidationLoss:0.47297625895273887\n",
      "Epoch: 1596 trainginLoss: 0.48887856614669695 ValidationLoss:0.47341396061040586\n",
      "Epoch: 1597 trainginLoss: 0.4893884298785421 ValidationLoss:0.47275351740546145\n",
      "Epoch: 1598 trainginLoss: 0.48887780248718776 ValidationLoss:0.4728407922437636\n",
      "Epoch: 1599 trainginLoss: 0.49005979599568666 ValidationLoss:0.47245401861303943\n",
      "Epoch: 1600 trainginLoss: 0.48827950326388314 ValidationLoss:0.472551145492974\n",
      "Epoch: 1601 trainginLoss: 0.48956543247171697 ValidationLoss:0.47220633555266817\n",
      "Epoch: 1602 trainginLoss: 0.4881271337902786 ValidationLoss:0.4722288438829325\n",
      "Epoch: 1603 trainginLoss: 0.4885841852066501 ValidationLoss:0.47193154328960485\n",
      "Epoch: 1604 trainginLoss: 0.48778086620689237 ValidationLoss:0.4717743782673852\n",
      "Epoch: 1605 trainginLoss: 0.488536599098436 ValidationLoss:0.47168470641313975\n",
      "Epoch: 1606 trainginLoss: 0.48849218103709635 ValidationLoss:0.47155781054900864\n",
      "Epoch: 1607 trainginLoss: 0.48804169293218014 ValidationLoss:0.47278231604624604\n",
      "Epoch: 1608 trainginLoss: 0.48799093137651484 ValidationLoss:0.4713123656935611\n",
      "Epoch: 1609 trainginLoss: 0.4879691928825122 ValidationLoss:0.4711304765636638\n",
      "Epoch: 1610 trainginLoss: 0.48768986611558285 ValidationLoss:0.47101674372867003\n",
      "Epoch: 1611 trainginLoss: 0.48695249485489506 ValidationLoss:0.4708577632904053\n",
      "Epoch: 1612 trainginLoss: 0.4879086973683146 ValidationLoss:0.4707663025896428\n",
      "Epoch: 1613 trainginLoss: 0.48773074910144676 ValidationLoss:0.4707978097058959\n",
      "Epoch: 1614 trainginLoss: 0.4887645504618651 ValidationLoss:0.4713613763704138\n",
      "Epoch: 1615 trainginLoss: 0.4868330633480277 ValidationLoss:0.47036419874530727\n",
      "Epoch: 1616 trainginLoss: 0.4866984298565244 ValidationLoss:0.47105988284288824\n",
      "Epoch: 1617 trainginLoss: 0.486649506444099 ValidationLoss:0.4700883629968611\n",
      "Epoch: 1618 trainginLoss: 0.4862716976028161 ValidationLoss:0.4703374890957849\n",
      "Epoch: 1619 trainginLoss: 0.4863599284783306 ValidationLoss:0.4698595223790508\n",
      "Epoch: 1620 trainginLoss: 0.4861876598540569 ValidationLoss:0.4697421078964815\n",
      "Epoch: 1621 trainginLoss: 0.48594309619609144 ValidationLoss:0.46974805987487406\n",
      "Epoch: 1622 trainginLoss: 0.48676304669188175 ValidationLoss:0.4698061270228887\n",
      "Epoch: 1623 trainginLoss: 0.48666762385592366 ValidationLoss:0.47005380068795155\n",
      "Epoch: 1624 trainginLoss: 0.4866419022515316 ValidationLoss:0.4721770479517468\n",
      "Epoch: 1625 trainginLoss: 0.4874039116321794 ValidationLoss:0.4718053065114102\n",
      "Epoch: 1626 trainginLoss: 0.4882613318478501 ValidationLoss:0.46973271380036563\n",
      "Epoch: 1627 trainginLoss: 0.4854916822190253 ValidationLoss:0.4688430465884128\n",
      "Epoch: 1628 trainginLoss: 0.4857121894023562 ValidationLoss:0.46884770726753494\n",
      "Epoch: 1629 trainginLoss: 0.48671223213208603 ValidationLoss:0.4686748645063174\n",
      "Epoch: 1630 trainginLoss: 0.48570044348703934 ValidationLoss:0.46863538077322103\n",
      "Epoch: 1631 trainginLoss: 0.48559215404843326 ValidationLoss:0.4684028175927825\n",
      "Epoch: 1632 trainginLoss: 0.484906191393833 ValidationLoss:0.46825527922581817\n",
      "Epoch: 1633 trainginLoss: 0.48478725532557343 ValidationLoss:0.4682651279336315\n",
      "Epoch: 1634 trainginLoss: 0.4846704754253362 ValidationLoss:0.468016699007002\n",
      "Epoch: 1635 trainginLoss: 0.48446812165663544 ValidationLoss:0.467891002812628\n",
      "Epoch: 1636 trainginLoss: 0.48423240208785806 ValidationLoss:0.467871625645686\n",
      "Epoch: 1637 trainginLoss: 0.48439807359804243 ValidationLoss:0.46791295085923146\n",
      "Epoch: 1638 trainginLoss: 0.48420621804743 ValidationLoss:0.467535905110634\n",
      "Epoch: 1639 trainginLoss: 0.4841392864316902 ValidationLoss:0.46798301430071815\n",
      "Epoch: 1640 trainginLoss: 0.48389441514975273 ValidationLoss:0.467645420563423\n",
      "Epoch: 1641 trainginLoss: 0.48430130785743664 ValidationLoss:0.46715226335040594\n",
      "Epoch: 1642 trainginLoss: 0.48388214259339657 ValidationLoss:0.4672321009433876\n",
      "Epoch: 1643 trainginLoss: 0.4838276641480875 ValidationLoss:0.4669924357179868\n",
      "Epoch: 1644 trainginLoss: 0.4848575080001114 ValidationLoss:0.46680425799499126\n",
      "Epoch: 1645 trainginLoss: 0.48459425028538544 ValidationLoss:0.4670624674376795\n",
      "Epoch: 1646 trainginLoss: 0.4845808556415891 ValidationLoss:0.4665430248793909\n",
      "Epoch: 1647 trainginLoss: 0.48418356868244655 ValidationLoss:0.4666612462472107\n",
      "Epoch: 1648 trainginLoss: 0.48302683774256866 ValidationLoss:0.46643019874217145\n",
      "Epoch: 1649 trainginLoss: 0.4828985233434895 ValidationLoss:0.46619368710760345\n",
      "Epoch: 1650 trainginLoss: 0.4844489925659743 ValidationLoss:0.46625211289373497\n",
      "Epoch: 1651 trainginLoss: 0.48256892145880115 ValidationLoss:0.46602185792842155\n",
      "Epoch: 1652 trainginLoss: 0.4826425681978264 ValidationLoss:0.46645955180717724\n",
      "Epoch: 1653 trainginLoss: 0.4825740614593429 ValidationLoss:0.4661655032028586\n",
      "Epoch: 1654 trainginLoss: 0.48336103518537227 ValidationLoss:0.4656018154095795\n",
      "Epoch: 1655 trainginLoss: 0.48275346483960246 ValidationLoss:0.46559694672034957\n",
      "Epoch: 1656 trainginLoss: 0.4826036359639776 ValidationLoss:0.4676542445764703\n",
      "Epoch: 1657 trainginLoss: 0.48468690590570435 ValidationLoss:0.46524553511102323\n",
      "Epoch: 1658 trainginLoss: 0.48294596624054364 ValidationLoss:0.4651950781628237\n",
      "Epoch: 1659 trainginLoss: 0.48192043532461126 ValidationLoss:0.4650169139191256\n",
      "Epoch: 1660 trainginLoss: 0.4823514239100002 ValidationLoss:0.46518844608533183\n",
      "Epoch: 1661 trainginLoss: 0.4822851039419238 ValidationLoss:0.4650574915489908\n",
      "Epoch: 1662 trainginLoss: 0.48325113102093636 ValidationLoss:0.4649620083429046\n",
      "Epoch: 1663 trainginLoss: 0.48168232296937263 ValidationLoss:0.46465840905399647\n",
      "Epoch: 1664 trainginLoss: 0.48185005604020703 ValidationLoss:0.46473022679151116\n",
      "Epoch: 1665 trainginLoss: 0.48267326979029096 ValidationLoss:0.46460932200237853\n",
      "Epoch: 1666 trainginLoss: 0.48115020630343647 ValidationLoss:0.4642072386660818\n",
      "Epoch: 1667 trainginLoss: 0.4815889398923656 ValidationLoss:0.4640940199464054\n",
      "Epoch: 1668 trainginLoss: 0.4829106398876881 ValidationLoss:0.4643461655762236\n",
      "Epoch: 1669 trainginLoss: 0.48085850777242006 ValidationLoss:0.46386166073508184\n",
      "Epoch: 1670 trainginLoss: 0.4806294339215195 ValidationLoss:0.46381000825914287\n",
      "Epoch: 1671 trainginLoss: 0.48127415916263655 ValidationLoss:0.4636511537988307\n",
      "Epoch: 1672 trainginLoss: 0.4807203752082466 ValidationLoss:0.46367500464795\n",
      "Epoch: 1673 trainginLoss: 0.48224681515821677 ValidationLoss:0.4636545551025261\n",
      "Epoch: 1674 trainginLoss: 0.48108228361046557 ValidationLoss:0.4633021425392668\n",
      "Epoch: 1675 trainginLoss: 0.4804751556991731 ValidationLoss:0.4632114483138262\n",
      "Epoch: 1676 trainginLoss: 0.4804031578886429 ValidationLoss:0.4640007421121759\n",
      "Epoch: 1677 trainginLoss: 0.4805052896474032 ValidationLoss:0.4637872358499947\n",
      "Epoch: 1678 trainginLoss: 0.48054989792356556 ValidationLoss:0.4628698374255229\n",
      "Epoch: 1679 trainginLoss: 0.48033370047607676 ValidationLoss:0.462787469059734\n",
      "Epoch: 1680 trainginLoss: 0.48012457677981996 ValidationLoss:0.46298418368323374\n",
      "Epoch: 1681 trainginLoss: 0.48019719923902676 ValidationLoss:0.4634729044922328\n",
      "Epoch: 1682 trainginLoss: 0.47994830624369167 ValidationLoss:0.46278212019952675\n",
      "Epoch: 1683 trainginLoss: 0.4802657973846333 ValidationLoss:0.46299119611917916\n",
      "Epoch: 1684 trainginLoss: 0.4805945846058378 ValidationLoss:0.4622713845665172\n",
      "Epoch: 1685 trainginLoss: 0.47964980798279677 ValidationLoss:0.4625327560861232\n",
      "Epoch: 1686 trainginLoss: 0.4797033845578264 ValidationLoss:0.46197809324426165\n",
      "Epoch: 1687 trainginLoss: 0.48087336413012255 ValidationLoss:0.4646390962398658\n",
      "Epoch: 1688 trainginLoss: 0.48008457246242753 ValidationLoss:0.4618443559792082\n",
      "Epoch: 1689 trainginLoss: 0.47944453558665795 ValidationLoss:0.46190807031372844\n",
      "Epoch: 1690 trainginLoss: 0.47927143689770024 ValidationLoss:0.4615405344356925\n",
      "Epoch: 1691 trainginLoss: 0.47869251518441525 ValidationLoss:0.4614432723845466\n",
      "Epoch: 1692 trainginLoss: 0.4785744017802629 ValidationLoss:0.46133406738103444\n",
      "Epoch: 1693 trainginLoss: 0.4790277661093129 ValidationLoss:0.46148574392674335\n",
      "Epoch: 1694 trainginLoss: 0.4788467806057642 ValidationLoss:0.4611634485802408\n",
      "Epoch: 1695 trainginLoss: 0.4794692865154087 ValidationLoss:0.4609996610778873\n",
      "Epoch: 1696 trainginLoss: 0.47923069772304305 ValidationLoss:0.460987034490553\n",
      "Epoch: 1697 trainginLoss: 0.4803743938471647 ValidationLoss:0.46079088661630274\n",
      "Epoch: 1698 trainginLoss: 0.47786763430441787 ValidationLoss:0.46067256775953\n",
      "Epoch: 1699 trainginLoss: 0.477999420774063 ValidationLoss:0.4606647933943797\n",
      "Epoch: 1700 trainginLoss: 0.4784684211215717 ValidationLoss:0.460492634975304\n",
      "Epoch: 1701 trainginLoss: 0.4778438502510122 ValidationLoss:0.46036840040804977\n",
      "Epoch: 1702 trainginLoss: 0.47803318260500094 ValidationLoss:0.46026928091453295\n",
      "Epoch: 1703 trainginLoss: 0.47781690315112174 ValidationLoss:0.4601234203678066\n",
      "Epoch: 1704 trainginLoss: 0.4774740394329865 ValidationLoss:0.4600311867261337\n",
      "Epoch: 1705 trainginLoss: 0.4782143679241206 ValidationLoss:0.4604503075955278\n",
      "Epoch: 1706 trainginLoss: 0.47789283866850324 ValidationLoss:0.46011184989395787\n",
      "Epoch: 1707 trainginLoss: 0.4783370618852193 ValidationLoss:0.46288841548612564\n",
      "Epoch: 1708 trainginLoss: 0.47834440365733716 ValidationLoss:0.4608664506572788\n",
      "Epoch: 1709 trainginLoss: 0.47725251377028904 ValidationLoss:0.45980981800515774\n",
      "Epoch: 1710 trainginLoss: 0.4787211780180067 ValidationLoss:0.459703557774172\n",
      "Epoch: 1711 trainginLoss: 0.47697306039349346 ValidationLoss:0.4593915902962119\n",
      "Epoch: 1712 trainginLoss: 0.4772874484926262 ValidationLoss:0.45945529614464714\n",
      "Epoch: 1713 trainginLoss: 0.47738891459951466 ValidationLoss:0.4591607111995503\n",
      "Epoch: 1714 trainginLoss: 0.4764329181021492 ValidationLoss:0.45908211530265164\n",
      "Epoch: 1715 trainginLoss: 0.47749004768045156 ValidationLoss:0.4589823328842551\n",
      "Epoch: 1716 trainginLoss: 0.47646292404040397 ValidationLoss:0.4589834172846907\n",
      "Epoch: 1717 trainginLoss: 0.47695609967180547 ValidationLoss:0.45897023465673803\n",
      "Epoch: 1718 trainginLoss: 0.47628708533792685 ValidationLoss:0.4586596359640865\n",
      "Epoch: 1719 trainginLoss: 0.47631045075871 ValidationLoss:0.4586502985428956\n",
      "Epoch: 1720 trainginLoss: 0.4764336895622663 ValidationLoss:0.45848551665322257\n",
      "Epoch: 1721 trainginLoss: 0.4766996158849473 ValidationLoss:0.4582402402061527\n",
      "Epoch: 1722 trainginLoss: 0.47578855828950867 ValidationLoss:0.4582081741195614\n",
      "Epoch: 1723 trainginLoss: 0.47644883034213276 ValidationLoss:0.45863769812099003\n",
      "Epoch: 1724 trainginLoss: 0.47547501965657174 ValidationLoss:0.4579978155887733\n",
      "Epoch: 1725 trainginLoss: 0.47585299471080705 ValidationLoss:0.4582438846765938\n",
      "Epoch: 1726 trainginLoss: 0.4753349625424251 ValidationLoss:0.45781111020152854\n",
      "Epoch: 1727 trainginLoss: 0.47556112376635507 ValidationLoss:0.45771396584429985\n",
      "Epoch: 1728 trainginLoss: 0.4764619369634846 ValidationLoss:0.45761904928643826\n",
      "Epoch: 1729 trainginLoss: 0.4751050548265444 ValidationLoss:0.4574605251772929\n",
      "Epoch: 1730 trainginLoss: 0.4754026200147283 ValidationLoss:0.4574526005882328\n",
      "Epoch: 1731 trainginLoss: 0.4754334740190698 ValidationLoss:0.4572825933917094\n",
      "Epoch: 1732 trainginLoss: 0.47520546825140114 ValidationLoss:0.4574044691304029\n",
      "Epoch: 1733 trainginLoss: 0.4749126698346746 ValidationLoss:0.4573066603329222\n",
      "Epoch: 1734 trainginLoss: 0.47516112659601556 ValidationLoss:0.4573322304224564\n",
      "Epoch: 1735 trainginLoss: 0.4747545543133012 ValidationLoss:0.45721930831165636\n",
      "Epoch: 1736 trainginLoss: 0.47531954794122067 ValidationLoss:0.456831620002197\n",
      "Epoch: 1737 trainginLoss: 0.4751210880759578 ValidationLoss:0.45670552536592646\n",
      "Epoch: 1738 trainginLoss: 0.47463702875495756 ValidationLoss:0.45662203774613846\n",
      "Epoch: 1739 trainginLoss: 0.4752069043633122 ValidationLoss:0.4580209439083681\n",
      "Epoch: 1740 trainginLoss: 0.47459614276885986 ValidationLoss:0.4564603505498272\n",
      "Epoch: 1741 trainginLoss: 0.4768779181794032 ValidationLoss:0.4562495613502244\n",
      "Epoch: 1742 trainginLoss: 0.4743166557894457 ValidationLoss:0.45648759724730154\n",
      "Epoch: 1743 trainginLoss: 0.47405787602367017 ValidationLoss:0.45623700669256306\n",
      "Epoch: 1744 trainginLoss: 0.4738021880988307 ValidationLoss:0.4559309695736837\n",
      "Epoch: 1745 trainginLoss: 0.47364423619020707 ValidationLoss:0.4567367062730304\n",
      "Epoch: 1746 trainginLoss: 0.4741427400368172 ValidationLoss:0.45573371689198383\n",
      "Epoch: 1747 trainginLoss: 0.4738296220366587 ValidationLoss:0.45579472739817734\n",
      "Epoch: 1748 trainginLoss: 0.4737160669877225 ValidationLoss:0.4559988833079904\n",
      "Epoch: 1749 trainginLoss: 0.4737145652706991 ValidationLoss:0.4560690016059552\n",
      "Epoch: 1750 trainginLoss: 0.4746762050078219 ValidationLoss:0.45535867345535147\n",
      "Epoch: 1751 trainginLoss: 0.4731523076559873 ValidationLoss:0.4553091100716995\n",
      "Epoch: 1752 trainginLoss: 0.4735791083150262 ValidationLoss:0.4552374057850595\n",
      "Epoch: 1753 trainginLoss: 0.47346468999881874 ValidationLoss:0.4550641595307043\n",
      "Epoch: 1754 trainginLoss: 0.47331100262251474 ValidationLoss:0.4551100185361959\n",
      "Epoch: 1755 trainginLoss: 0.4754375707383124 ValidationLoss:0.4574947677426419\n",
      "Epoch: 1756 trainginLoss: 0.4745004403111119 ValidationLoss:0.45480452048576486\n",
      "Epoch: 1757 trainginLoss: 0.47425934652354096 ValidationLoss:0.4547054243289818\n",
      "Epoch: 1758 trainginLoss: 0.47393180139912855 ValidationLoss:0.45489506418422115\n",
      "Epoch: 1759 trainginLoss: 0.473520477746157 ValidationLoss:0.4548712787991863\n",
      "Epoch: 1760 trainginLoss: 0.47252639668099833 ValidationLoss:0.45448564472845043\n",
      "Epoch: 1761 trainginLoss: 0.4724974618262093 ValidationLoss:0.4543814957141876\n",
      "Epoch: 1762 trainginLoss: 0.47249498423313935 ValidationLoss:0.45446107518874995\n",
      "Epoch: 1763 trainginLoss: 0.47369054419882345 ValidationLoss:0.45444781376143634\n",
      "Epoch: 1764 trainginLoss: 0.4722901910743457 ValidationLoss:0.4543278534533614\n",
      "Epoch: 1765 trainginLoss: 0.47259857170533814 ValidationLoss:0.45468320705122867\n",
      "Epoch: 1766 trainginLoss: 0.47520320067469707 ValidationLoss:0.45393427357835286\n",
      "Epoch: 1767 trainginLoss: 0.4729807820896174 ValidationLoss:0.45481152362742666\n",
      "Epoch: 1768 trainginLoss: 0.47430595475555265 ValidationLoss:0.4544296463667336\n",
      "Epoch: 1769 trainginLoss: 0.4734498436418956 ValidationLoss:0.4538601903592126\n",
      "Epoch: 1770 trainginLoss: 0.4720680115606961 ValidationLoss:0.45375846129352765\n",
      "Epoch: 1771 trainginLoss: 0.4725247411519889 ValidationLoss:0.45353225621126464\n",
      "Epoch: 1772 trainginLoss: 0.4722192695476865 ValidationLoss:0.4536157259496592\n",
      "Epoch: 1773 trainginLoss: 0.4728070957548666 ValidationLoss:0.453895942841546\n",
      "Epoch: 1774 trainginLoss: 0.47233066702849114 ValidationLoss:0.4547286977202205\n",
      "Epoch: 1775 trainginLoss: 0.4717541579032104 ValidationLoss:0.45305196056931707\n",
      "Epoch: 1776 trainginLoss: 0.47112156380742987 ValidationLoss:0.45334637478246526\n",
      "Epoch: 1777 trainginLoss: 0.47223615906382566 ValidationLoss:0.4534461290149365\n",
      "Epoch: 1778 trainginLoss: 0.4719701735365311 ValidationLoss:0.45340662689532263\n",
      "Epoch: 1779 trainginLoss: 0.47150817433459646 ValidationLoss:0.4527158852350914\n",
      "Epoch: 1780 trainginLoss: 0.47093692001880416 ValidationLoss:0.4532364140122624\n",
      "Epoch: 1781 trainginLoss: 0.47165662630292393 ValidationLoss:0.45302703229047486\n",
      "Epoch: 1782 trainginLoss: 0.4729496892666657 ValidationLoss:0.4528252957230907\n",
      "Epoch: 1783 trainginLoss: 0.47158246652391933 ValidationLoss:0.45246782494803606\n",
      "Epoch: 1784 trainginLoss: 0.4707767811397578 ValidationLoss:0.4523083979800596\n",
      "Epoch: 1785 trainginLoss: 0.47346888212549604 ValidationLoss:0.4538474388041739\n",
      "Epoch: 1786 trainginLoss: 0.4708078185583921 ValidationLoss:0.4522602290420209\n",
      "Epoch: 1787 trainginLoss: 0.470691475892227 ValidationLoss:0.4520315198575036\n",
      "Epoch: 1788 trainginLoss: 0.4710125151096574 ValidationLoss:0.45280493326106314\n",
      "Epoch: 1789 trainginLoss: 0.47088145629671596 ValidationLoss:0.4519401826090732\n",
      "Epoch: 1790 trainginLoss: 0.47051640544961765 ValidationLoss:0.4521295002961563\n",
      "Epoch: 1791 trainginLoss: 0.47107427012200326 ValidationLoss:0.4516807288436566\n",
      "Epoch: 1792 trainginLoss: 0.4708539627542432 ValidationLoss:0.4516609585891336\n",
      "Epoch: 1793 trainginLoss: 0.4704602460733196 ValidationLoss:0.4515234801728847\n",
      "Epoch: 1794 trainginLoss: 0.46976114959524784 ValidationLoss:0.45155278747364624\n",
      "Epoch: 1795 trainginLoss: 0.470100439234868 ValidationLoss:0.4516841240858628\n",
      "Epoch: 1796 trainginLoss: 0.46960933196464644 ValidationLoss:0.4514724300069324\n",
      "Epoch: 1797 trainginLoss: 0.46957376379294685 ValidationLoss:0.45182468042535295\n",
      "Epoch: 1798 trainginLoss: 0.46981476257311416 ValidationLoss:0.4513828727148347\n",
      "Epoch: 1799 trainginLoss: 0.4692995932278217 ValidationLoss:0.45101149839870003\n",
      "Epoch: 1800 trainginLoss: 0.4711382561082008 ValidationLoss:0.45158722643124855\n",
      "Epoch: 1801 trainginLoss: 0.46954903066558323 ValidationLoss:0.45129096043311945\n",
      "Epoch: 1802 trainginLoss: 0.469136456515165 ValidationLoss:0.4508020254514985\n",
      "Epoch: 1803 trainginLoss: 0.4692320331631091 ValidationLoss:0.4510541834063449\n",
      "Epoch: 1804 trainginLoss: 0.4696263322894205 ValidationLoss:0.4516939978478319\n",
      "Epoch: 1805 trainginLoss: 0.46937884860390783 ValidationLoss:0.45053379838749513\n",
      "Epoch: 1806 trainginLoss: 0.470553490139494 ValidationLoss:0.45062428470385274\n",
      "Epoch: 1807 trainginLoss: 0.46916213891650205 ValidationLoss:0.45311649269976856\n",
      "Epoch: 1808 trainginLoss: 0.4696039425446683 ValidationLoss:0.45136311377509164\n",
      "Epoch: 1809 trainginLoss: 0.4690531928267255 ValidationLoss:0.45054702172845096\n",
      "Epoch: 1810 trainginLoss: 0.46849177147718085 ValidationLoss:0.4501203200574649\n",
      "Epoch: 1811 trainginLoss: 0.4689436646915922 ValidationLoss:0.450067340317419\n",
      "Epoch: 1812 trainginLoss: 0.46932956756361377 ValidationLoss:0.44997706877983223\n",
      "Epoch: 1813 trainginLoss: 0.4710917018803974 ValidationLoss:0.4499308337599544\n",
      "Epoch: 1814 trainginLoss: 0.46912964118407074 ValidationLoss:0.45001747193983044\n",
      "Epoch: 1815 trainginLoss: 0.4702325339285319 ValidationLoss:0.4498819635076038\n",
      "Epoch: 1816 trainginLoss: 0.46851266970570454 ValidationLoss:0.45007169206263653\n",
      "Epoch: 1817 trainginLoss: 0.4682299748763142 ValidationLoss:0.44966823670823697\n",
      "Epoch: 1818 trainginLoss: 0.4680081145875406 ValidationLoss:0.4514912635593091\n",
      "Epoch: 1819 trainginLoss: 0.4690926130986054 ValidationLoss:0.44959456435704637\n",
      "Epoch: 1820 trainginLoss: 0.46789241777170426 ValidationLoss:0.44935737413875126\n",
      "Epoch: 1821 trainginLoss: 0.4684740770983216 ValidationLoss:0.451273358777418\n",
      "Epoch: 1822 trainginLoss: 0.46845665554072236 ValidationLoss:0.4493045103752007\n",
      "Epoch: 1823 trainginLoss: 0.4677939989022761 ValidationLoss:0.44909885172116554\n",
      "Epoch: 1824 trainginLoss: 0.46833322812246797 ValidationLoss:0.4493817620358225\n",
      "Epoch: 1825 trainginLoss: 0.4673371863045148 ValidationLoss:0.44914510916855377\n",
      "Epoch: 1826 trainginLoss: 0.4690857269619935 ValidationLoss:0.44883685960608016\n",
      "Epoch: 1827 trainginLoss: 0.46816877290706504 ValidationLoss:0.44957561149435527\n",
      "Epoch: 1828 trainginLoss: 0.46930397076894775 ValidationLoss:0.448845958608692\n",
      "Epoch: 1829 trainginLoss: 0.4682283921529783 ValidationLoss:0.44910706322071914\n",
      "Epoch: 1830 trainginLoss: 0.4672953142415757 ValidationLoss:0.4486370921134949\n",
      "Epoch: 1831 trainginLoss: 0.4674563841931772 ValidationLoss:0.4488927693690284\n",
      "Epoch: 1832 trainginLoss: 0.46799801780073436 ValidationLoss:0.4483734852176602\n",
      "Epoch: 1833 trainginLoss: 0.4671149085832122 ValidationLoss:0.44865168817972734\n",
      "Epoch: 1834 trainginLoss: 0.4682875515630581 ValidationLoss:0.4483761415643207\n",
      "Epoch: 1835 trainginLoss: 0.46802700125931096 ValidationLoss:0.44821571940082616\n",
      "Epoch: 1836 trainginLoss: 0.4675139672804199 ValidationLoss:0.44819517155825084\n",
      "Epoch: 1837 trainginLoss: 0.4687031513492533 ValidationLoss:0.4501692169803684\n",
      "Epoch: 1838 trainginLoss: 0.4690928215148465 ValidationLoss:0.44847732984413535\n",
      "Epoch: 1839 trainginLoss: 0.4681157057717342 ValidationLoss:0.44869641601029087\n",
      "Epoch: 1840 trainginLoss: 0.46647415025122213 ValidationLoss:0.44782310815180765\n",
      "Epoch: 1841 trainginLoss: 0.4662184117224393 ValidationLoss:0.44814859194270634\n",
      "Epoch: 1842 trainginLoss: 0.4676883698709859 ValidationLoss:0.4476982566259675\n",
      "Epoch: 1843 trainginLoss: 0.46779776079542684 ValidationLoss:0.4475552183086589\n",
      "Epoch: 1844 trainginLoss: 0.46660166858826707 ValidationLoss:0.4475706057023194\n",
      "Epoch: 1845 trainginLoss: 0.46921370633496534 ValidationLoss:0.4481618913553529\n",
      "Epoch: 1846 trainginLoss: 0.4679501614714629 ValidationLoss:0.44798110763905413\n",
      "Epoch: 1847 trainginLoss: 0.46691783262579234 ValidationLoss:0.44743199207014955\n",
      "Epoch: 1848 trainginLoss: 0.46640457082914827 ValidationLoss:0.4473228193945804\n",
      "Epoch: 1849 trainginLoss: 0.4663272772059345 ValidationLoss:0.4481853886175964\n",
      "Epoch: 1850 trainginLoss: 0.4661682224113669 ValidationLoss:0.4470358645511886\n",
      "Epoch: 1851 trainginLoss: 0.46621207382855 ValidationLoss:0.4470040005142406\n",
      "Epoch: 1852 trainginLoss: 0.4669678025197663 ValidationLoss:0.44690871915574804\n",
      "Epoch: 1853 trainginLoss: 0.46595437114670774 ValidationLoss:0.4474985152988111\n",
      "Epoch: 1854 trainginLoss: 0.46691473018403024 ValidationLoss:0.44788687774690533\n",
      "Epoch: 1855 trainginLoss: 0.46585119930689767 ValidationLoss:0.44800822320630995\n",
      "Epoch: 1856 trainginLoss: 0.46709851050536905 ValidationLoss:0.44660174533472224\n",
      "Epoch: 1857 trainginLoss: 0.46551316616519184 ValidationLoss:0.44658110717595634\n",
      "Epoch: 1858 trainginLoss: 0.4659411565169392 ValidationLoss:0.4493779476416313\n",
      "Epoch: 1859 trainginLoss: 0.466359696932287 ValidationLoss:0.4475757845377518\n",
      "Epoch: 1860 trainginLoss: 0.46490730375251516 ValidationLoss:0.4462929894358425\n",
      "Epoch: 1861 trainginLoss: 0.4651592841884434 ValidationLoss:0.44636992882874055\n",
      "Epoch: 1862 trainginLoss: 0.4649782784833204 ValidationLoss:0.44624839283652223\n",
      "Epoch: 1863 trainginLoss: 0.4652293784506369 ValidationLoss:0.44619778061317183\n",
      "Epoch: 1864 trainginLoss: 0.46839590060630903 ValidationLoss:0.44613416689937396\n",
      "Epoch: 1865 trainginLoss: 0.46519400389402504 ValidationLoss:0.44592873555118756\n",
      "Epoch: 1866 trainginLoss: 0.4650704788681645 ValidationLoss:0.44740068084102563\n",
      "Epoch: 1867 trainginLoss: 0.46605831864696223 ValidationLoss:0.4462601886967481\n",
      "Epoch: 1868 trainginLoss: 0.4651840643194698 ValidationLoss:0.4458843548419112\n",
      "Epoch: 1869 trainginLoss: 0.46467543288365304 ValidationLoss:0.44564855613950954\n",
      "Epoch: 1870 trainginLoss: 0.46479638090869724 ValidationLoss:0.4456427230673321\n",
      "Epoch: 1871 trainginLoss: 0.465243019913667 ValidationLoss:0.44552490660699745\n",
      "Epoch: 1872 trainginLoss: 0.4644182412416343 ValidationLoss:0.44566142336796905\n",
      "Epoch: 1873 trainginLoss: 0.46412417352599583 ValidationLoss:0.4454043261075424\n",
      "Epoch: 1874 trainginLoss: 0.4656185063739751 ValidationLoss:0.4454084766113152\n",
      "Epoch: 1875 trainginLoss: 0.4643882137016962 ValidationLoss:0.44585810505737694\n",
      "Epoch: 1876 trainginLoss: 0.4642484292087939 ValidationLoss:0.4455427077867217\n",
      "Epoch: 1877 trainginLoss: 0.4664195409957195 ValidationLoss:0.4450860303337291\n",
      "Epoch: 1878 trainginLoss: 0.4637732219776051 ValidationLoss:0.44545232694027787\n",
      "Epoch: 1879 trainginLoss: 0.46540717530570574 ValidationLoss:0.4449393578505112\n",
      "Epoch: 1880 trainginLoss: 0.46422682472523424 ValidationLoss:0.4457745835942737\n",
      "Epoch: 1881 trainginLoss: 0.46441035262690294 ValidationLoss:0.44482335448265076\n",
      "Epoch: 1882 trainginLoss: 0.4646666866020868 ValidationLoss:0.4458544778621803\n",
      "Epoch: 1883 trainginLoss: 0.46451679252138073 ValidationLoss:0.4457004359212972\n",
      "Epoch: 1884 trainginLoss: 0.4637587694513718 ValidationLoss:0.4445821361016419\n",
      "Epoch: 1885 trainginLoss: 0.463337820648347 ValidationLoss:0.4451214215513003\n",
      "Epoch: 1886 trainginLoss: 0.46387831556717024 ValidationLoss:0.44488750605259914\n",
      "Epoch: 1887 trainginLoss: 0.4635776111343563 ValidationLoss:0.44442780179492497\n",
      "Epoch: 1888 trainginLoss: 0.4637439042929835 ValidationLoss:0.44522941274158023\n",
      "Epoch: 1889 trainginLoss: 0.4641276579975282 ValidationLoss:0.44436362466569673\n",
      "Epoch: 1890 trainginLoss: 0.4636893704433569 ValidationLoss:0.4445110064441875\n",
      "Epoch: 1891 trainginLoss: 0.4634150194641728 ValidationLoss:0.44413727117797075\n",
      "Epoch: 1892 trainginLoss: 0.46385038739082796 ValidationLoss:0.44443172438670014\n",
      "Epoch: 1893 trainginLoss: 0.46404146668094914 ValidationLoss:0.44759402103343254\n",
      "Epoch: 1894 trainginLoss: 0.4653019637069446 ValidationLoss:0.4496412057997817\n",
      "Epoch: 1895 trainginLoss: 0.4652386795754401 ValidationLoss:0.44386067915770966\n",
      "Epoch: 1896 trainginLoss: 0.46262316795803554 ValidationLoss:0.44376582654856017\n",
      "Epoch: 1897 trainginLoss: 0.463880380128054 ValidationLoss:0.44371025127879643\n",
      "Epoch: 1898 trainginLoss: 0.4624272028871831 ValidationLoss:0.44577922649302726\n",
      "Epoch: 1899 trainginLoss: 0.4644649088782752 ValidationLoss:0.4452985591807608\n",
      "Epoch: 1900 trainginLoss: 0.46419256645561063 ValidationLoss:0.4453367475735939\n",
      "Epoch: 1901 trainginLoss: 0.46554322490756145 ValidationLoss:0.44359187099893216\n",
      "Epoch: 1902 trainginLoss: 0.4627673991974568 ValidationLoss:0.44338642762879193\n",
      "Epoch: 1903 trainginLoss: 0.4624391394173539 ValidationLoss:0.44342124977354275\n",
      "Epoch: 1904 trainginLoss: 0.46267471337478433 ValidationLoss:0.4445557029570563\n",
      "Epoch: 1905 trainginLoss: 0.4639683521833996 ValidationLoss:0.44336591888282256\n",
      "Epoch: 1906 trainginLoss: 0.4630977951440235 ValidationLoss:0.4431156343322689\n",
      "Epoch: 1907 trainginLoss: 0.46255221082860193 ValidationLoss:0.4430610271833711\n",
      "Epoch: 1908 trainginLoss: 0.46317665208906134 ValidationLoss:0.44325012211072246\n",
      "Epoch: 1909 trainginLoss: 0.46230687591053493 ValidationLoss:0.4435830603211613\n",
      "Epoch: 1910 trainginLoss: 0.46264351674374316 ValidationLoss:0.44297044650983003\n",
      "Epoch: 1911 trainginLoss: 0.4632204507021296 ValidationLoss:0.4428048168198537\n",
      "Epoch: 1912 trainginLoss: 0.4623466048064648 ValidationLoss:0.44312331636073227\n",
      "Epoch: 1913 trainginLoss: 0.4621197363274209 ValidationLoss:0.44318693714626767\n",
      "Epoch: 1914 trainginLoss: 0.4622432337511306 ValidationLoss:0.4428385797193495\n",
      "Epoch: 1915 trainginLoss: 0.4624426168883407 ValidationLoss:0.443363616022013\n",
      "Epoch: 1916 trainginLoss: 0.46144895765605387 ValidationLoss:0.4432193939968691\n",
      "Epoch: 1917 trainginLoss: 0.46293562130640015 ValidationLoss:0.4424447150553687\n",
      "Epoch: 1918 trainginLoss: 0.4615032740887379 ValidationLoss:0.44411208488173404\n",
      "Epoch: 1919 trainginLoss: 0.4640923268442986 ValidationLoss:0.44246665370666377\n",
      "Epoch: 1920 trainginLoss: 0.46225075073690225 ValidationLoss:0.44234766030715683\n",
      "Epoch: 1921 trainginLoss: 0.4623163944522806 ValidationLoss:0.44228017976728534\n",
      "Epoch: 1922 trainginLoss: 0.46158192261753467 ValidationLoss:0.44246828768212915\n",
      "Epoch: 1923 trainginLoss: 0.462394727756513 ValidationLoss:0.4423079042111413\n",
      "Epoch: 1924 trainginLoss: 0.4614136213024191 ValidationLoss:0.4426657864602946\n",
      "Epoch: 1925 trainginLoss: 0.46189481019973755 ValidationLoss:0.4419455205990096\n",
      "Epoch: 1926 trainginLoss: 0.46126633122463356 ValidationLoss:0.4421118003837133\n",
      "Epoch: 1927 trainginLoss: 0.46147923901577126 ValidationLoss:0.4417837511684935\n",
      "Epoch: 1928 trainginLoss: 0.46519811961474833 ValidationLoss:0.4417813111159761\n",
      "Epoch: 1929 trainginLoss: 0.4610962903739622 ValidationLoss:0.4418711917885279\n",
      "Epoch: 1930 trainginLoss: 0.46089446584650334 ValidationLoss:0.4421111696857517\n",
      "Epoch: 1931 trainginLoss: 0.4612435586500488 ValidationLoss:0.44155108504376167\n",
      "Epoch: 1932 trainginLoss: 0.46095844383207746 ValidationLoss:0.4414821884389651\n",
      "Epoch: 1933 trainginLoss: 0.460435460677883 ValidationLoss:0.44170624916836365\n",
      "Epoch: 1934 trainginLoss: 0.4607755433793036 ValidationLoss:0.4413693857395043\n",
      "Epoch: 1935 trainginLoss: 0.4606631326195378 ValidationLoss:0.4415441114013478\n",
      "Epoch: 1936 trainginLoss: 0.46053062649381243 ValidationLoss:0.4431999490422718\n",
      "Epoch: 1937 trainginLoss: 0.4621430963477832 ValidationLoss:0.4430266565185482\n",
      "Epoch: 1938 trainginLoss: 0.4627687307012161 ValidationLoss:0.4451631061101364\n",
      "Epoch: 1939 trainginLoss: 0.4609294117697133 ValidationLoss:0.44121724407551655\n",
      "Epoch: 1940 trainginLoss: 0.4622488881917608 ValidationLoss:0.44197110958018543\n",
      "Epoch: 1941 trainginLoss: 0.46140149615755016 ValidationLoss:0.441705757278507\n",
      "Epoch: 1942 trainginLoss: 0.46037651848473005 ValidationLoss:0.4414819498183364\n",
      "Epoch: 1943 trainginLoss: 0.4617098433859396 ValidationLoss:0.441932576389636\n",
      "Epoch: 1944 trainginLoss: 0.4616649388866937 ValidationLoss:0.4407755210237988\n",
      "Epoch: 1945 trainginLoss: 0.46154918266622813 ValidationLoss:0.44075054263664504\n",
      "Epoch: 1946 trainginLoss: 0.4605418619693526 ValidationLoss:0.44077312592732704\n",
      "Epoch: 1947 trainginLoss: 0.46034837189136735 ValidationLoss:0.44063964031510433\n",
      "Epoch: 1948 trainginLoss: 0.46095363885764307 ValidationLoss:0.4442818486084372\n",
      "Epoch: 1949 trainginLoss: 0.459673750320537 ValidationLoss:0.44068117424593134\n",
      "Epoch: 1950 trainginLoss: 0.461285272500659 ValidationLoss:0.4406314258858309\n",
      "Epoch: 1951 trainginLoss: 0.4611910769603396 ValidationLoss:0.44071382041704854\n",
      "Epoch: 1952 trainginLoss: 0.45945993265849633 ValidationLoss:0.4404786135180522\n",
      "Epoch: 1953 trainginLoss: 0.4594815193406688 ValidationLoss:0.44106822811951074\n",
      "Epoch: 1954 trainginLoss: 0.45986066868641234 ValidationLoss:0.44061772227287294\n",
      "Epoch: 1955 trainginLoss: 0.4598529944883897 ValidationLoss:0.4443149824263686\n",
      "Epoch: 1956 trainginLoss: 0.4610946726478986 ValidationLoss:0.4403085676290221\n",
      "Epoch: 1957 trainginLoss: 0.4593257806045097 ValidationLoss:0.440793272398286\n",
      "Epoch: 1958 trainginLoss: 0.4606112235744528 ValidationLoss:0.44013892907207297\n",
      "Epoch: 1959 trainginLoss: 0.45971489732697507 ValidationLoss:0.44096414658982874\n",
      "Epoch: 1960 trainginLoss: 0.4598827556075666 ValidationLoss:0.44006729287616275\n",
      "Epoch: 1961 trainginLoss: 0.4601790993005637 ValidationLoss:0.4408331401267294\n",
      "Epoch: 1962 trainginLoss: 0.4607325896320727 ValidationLoss:0.44229396844314317\n",
      "Epoch: 1963 trainginLoss: 0.4618873214161636 ValidationLoss:0.4399501948033349\n",
      "Epoch: 1964 trainginLoss: 0.4594774726253228 ValidationLoss:0.43981705237243135\n",
      "Epoch: 1965 trainginLoss: 0.458910342630924 ValidationLoss:0.4401243765475386\n",
      "Epoch: 1966 trainginLoss: 0.4592064933088802 ValidationLoss:0.44168102367449613\n",
      "Epoch: 1967 trainginLoss: 0.459697019533823 ValidationLoss:0.44232965909828575\n",
      "Epoch: 1968 trainginLoss: 0.4597945909372112 ValidationLoss:0.44036098597413403\n",
      "Epoch: 1969 trainginLoss: 0.4589330780026097 ValidationLoss:0.43941260545940725\n",
      "Epoch: 1970 trainginLoss: 0.4604849011305995 ValidationLoss:0.44279963869159505\n",
      "Epoch: 1971 trainginLoss: 0.4596253919521434 ValidationLoss:0.43971492193512995\n",
      "Epoch: 1972 trainginLoss: 0.4590771688310892 ValidationLoss:0.43923749176122373\n",
      "Epoch: 1973 trainginLoss: 0.4590405287758616 ValidationLoss:0.43918536123582874\n",
      "Epoch: 1974 trainginLoss: 0.4586509882203685 ValidationLoss:0.4393411999031649\n",
      "Epoch: 1975 trainginLoss: 0.45874269956710356 ValidationLoss:0.43995835447715503\n",
      "Epoch: 1976 trainginLoss: 0.45920642110325344 ValidationLoss:0.43931079042159904\n",
      "Epoch: 1977 trainginLoss: 0.45809600417245955 ValidationLoss:0.43909177547794276\n",
      "Epoch: 1978 trainginLoss: 0.4587919792072885 ValidationLoss:0.4395510733127594\n",
      "Epoch: 1979 trainginLoss: 0.4600694791582607 ValidationLoss:0.4400226129313647\n",
      "Epoch: 1980 trainginLoss: 0.4589395675083135 ValidationLoss:0.44050935668460395\n",
      "Epoch: 1981 trainginLoss: 0.45854664108897214 ValidationLoss:0.4388491493160442\n",
      "Epoch: 1982 trainginLoss: 0.4582433964582098 ValidationLoss:0.4388039688942796\n",
      "Epoch: 1983 trainginLoss: 0.4590664361947335 ValidationLoss:0.44017614813174233\n",
      "Epoch: 1984 trainginLoss: 0.4580833591870814 ValidationLoss:0.4390827632556527\n",
      "Epoch: 1985 trainginLoss: 0.4612910263490357 ValidationLoss:0.4387325536396544\n",
      "Epoch: 1986 trainginLoss: 0.4585629999637604 ValidationLoss:0.4391595317145525\n",
      "Epoch: 1987 trainginLoss: 0.4586106942804068 ValidationLoss:0.4399229682098001\n",
      "Epoch: 1988 trainginLoss: 0.45767926329734343 ValidationLoss:0.43840019864551094\n",
      "Epoch: 1989 trainginLoss: 0.45798636982904983 ValidationLoss:0.43865719973030737\n",
      "Epoch: 1990 trainginLoss: 0.45777648127319026 ValidationLoss:0.4390851361266637\n",
      "Epoch: 1991 trainginLoss: 0.45958514481582896 ValidationLoss:0.43899433380466396\n",
      "Epoch: 1992 trainginLoss: 0.4577254044929607 ValidationLoss:0.43876792889530375\n",
      "Epoch: 1993 trainginLoss: 0.4594125539664454 ValidationLoss:0.4410923596155846\n",
      "Epoch: 1994 trainginLoss: 0.45830723583298244 ValidationLoss:0.4391060976658837\n",
      "Epoch: 1995 trainginLoss: 0.4576945600893674 ValidationLoss:0.43813069937592847\n",
      "Epoch: 1996 trainginLoss: 0.46099939522327193 ValidationLoss:0.4385987258563607\n",
      "Epoch: 1997 trainginLoss: 0.45753256666580305 ValidationLoss:0.4380886265787028\n",
      "Epoch: 1998 trainginLoss: 0.4591185242537684 ValidationLoss:0.4387490674600763\n",
      "Epoch: 1999 trainginLoss: 0.4579116806087878 ValidationLoss:0.43930028444629604\n"
     ]
    }
   ],
   "source": [
    "bs = 128\n",
    "epochs = 2000\n",
    "lr = .001\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "\n",
    "model = TitanicNN_Model()\n",
    "\n",
    "\n",
    "\n",
    "model.to(dev)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.001)\n",
    " \n",
    "\n",
    "# loss_func = F.cross_entropy\n",
    "# loss_func = torch.nn.BCELoss()\n",
    "loss_func = F.binary_cross_entropy\n",
    "\n",
    "validate_loss_list, loss_list = fit(epochs, model, loss_func, optimizer, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "value=trainLoss<br>epoch=%{x}<br>Validation Loss=%{y}<extra></extra>",
         "legendgroup": "trainLoss",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "trainLoss",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999,
          1000,
          1001,
          1002,
          1003,
          1004,
          1005,
          1006,
          1007,
          1008,
          1009,
          1010,
          1011,
          1012,
          1013,
          1014,
          1015,
          1016,
          1017,
          1018,
          1019,
          1020,
          1021,
          1022,
          1023,
          1024,
          1025,
          1026,
          1027,
          1028,
          1029,
          1030,
          1031,
          1032,
          1033,
          1034,
          1035,
          1036,
          1037,
          1038,
          1039,
          1040,
          1041,
          1042,
          1043,
          1044,
          1045,
          1046,
          1047,
          1048,
          1049,
          1050,
          1051,
          1052,
          1053,
          1054,
          1055,
          1056,
          1057,
          1058,
          1059,
          1060,
          1061,
          1062,
          1063,
          1064,
          1065,
          1066,
          1067,
          1068,
          1069,
          1070,
          1071,
          1072,
          1073,
          1074,
          1075,
          1076,
          1077,
          1078,
          1079,
          1080,
          1081,
          1082,
          1083,
          1084,
          1085,
          1086,
          1087,
          1088,
          1089,
          1090,
          1091,
          1092,
          1093,
          1094,
          1095,
          1096,
          1097,
          1098,
          1099,
          1100,
          1101,
          1102,
          1103,
          1104,
          1105,
          1106,
          1107,
          1108,
          1109,
          1110,
          1111,
          1112,
          1113,
          1114,
          1115,
          1116,
          1117,
          1118,
          1119,
          1120,
          1121,
          1122,
          1123,
          1124,
          1125,
          1126,
          1127,
          1128,
          1129,
          1130,
          1131,
          1132,
          1133,
          1134,
          1135,
          1136,
          1137,
          1138,
          1139,
          1140,
          1141,
          1142,
          1143,
          1144,
          1145,
          1146,
          1147,
          1148,
          1149,
          1150,
          1151,
          1152,
          1153,
          1154,
          1155,
          1156,
          1157,
          1158,
          1159,
          1160,
          1161,
          1162,
          1163,
          1164,
          1165,
          1166,
          1167,
          1168,
          1169,
          1170,
          1171,
          1172,
          1173,
          1174,
          1175,
          1176,
          1177,
          1178,
          1179,
          1180,
          1181,
          1182,
          1183,
          1184,
          1185,
          1186,
          1187,
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194,
          1195,
          1196,
          1197,
          1198,
          1199,
          1200,
          1201,
          1202,
          1203,
          1204,
          1205,
          1206,
          1207,
          1208,
          1209,
          1210,
          1211,
          1212,
          1213,
          1214,
          1215,
          1216,
          1217,
          1218,
          1219,
          1220,
          1221,
          1222,
          1223,
          1224,
          1225,
          1226,
          1227,
          1228,
          1229,
          1230,
          1231,
          1232,
          1233,
          1234,
          1235,
          1236,
          1237,
          1238,
          1239,
          1240,
          1241,
          1242,
          1243,
          1244,
          1245,
          1246,
          1247,
          1248,
          1249,
          1250,
          1251,
          1252,
          1253,
          1254,
          1255,
          1256,
          1257,
          1258,
          1259,
          1260,
          1261,
          1262,
          1263,
          1264,
          1265,
          1266,
          1267,
          1268,
          1269,
          1270,
          1271,
          1272,
          1273,
          1274,
          1275,
          1276,
          1277,
          1278,
          1279,
          1280,
          1281,
          1282,
          1283,
          1284,
          1285,
          1286,
          1287,
          1288,
          1289,
          1290,
          1291,
          1292,
          1293,
          1294,
          1295,
          1296,
          1297,
          1298,
          1299,
          1300,
          1301,
          1302,
          1303,
          1304,
          1305,
          1306,
          1307,
          1308,
          1309,
          1310,
          1311,
          1312,
          1313,
          1314,
          1315,
          1316,
          1317,
          1318,
          1319,
          1320,
          1321,
          1322,
          1323,
          1324,
          1325,
          1326,
          1327,
          1328,
          1329,
          1330,
          1331,
          1332,
          1333,
          1334,
          1335,
          1336,
          1337,
          1338,
          1339,
          1340,
          1341,
          1342,
          1343,
          1344,
          1345,
          1346,
          1347,
          1348,
          1349,
          1350,
          1351,
          1352,
          1353,
          1354,
          1355,
          1356,
          1357,
          1358,
          1359,
          1360,
          1361,
          1362,
          1363,
          1364,
          1365,
          1366,
          1367,
          1368,
          1369,
          1370,
          1371,
          1372,
          1373,
          1374,
          1375,
          1376,
          1377,
          1378,
          1379,
          1380,
          1381,
          1382,
          1383,
          1384,
          1385,
          1386,
          1387,
          1388,
          1389,
          1390,
          1391,
          1392,
          1393,
          1394,
          1395,
          1396,
          1397,
          1398,
          1399,
          1400,
          1401,
          1402,
          1403,
          1404,
          1405,
          1406,
          1407,
          1408,
          1409,
          1410,
          1411,
          1412,
          1413,
          1414,
          1415,
          1416,
          1417,
          1418,
          1419,
          1420,
          1421,
          1422,
          1423,
          1424,
          1425,
          1426,
          1427,
          1428,
          1429,
          1430,
          1431,
          1432,
          1433,
          1434,
          1435,
          1436,
          1437,
          1438,
          1439,
          1440,
          1441,
          1442,
          1443,
          1444,
          1445,
          1446,
          1447,
          1448,
          1449,
          1450,
          1451,
          1452,
          1453,
          1454,
          1455,
          1456,
          1457,
          1458,
          1459,
          1460,
          1461,
          1462,
          1463,
          1464,
          1465,
          1466,
          1467,
          1468,
          1469,
          1470,
          1471,
          1472,
          1473,
          1474,
          1475,
          1476,
          1477,
          1478,
          1479,
          1480,
          1481,
          1482,
          1483,
          1484,
          1485,
          1486,
          1487,
          1488,
          1489,
          1490,
          1491,
          1492,
          1493,
          1494,
          1495,
          1496,
          1497,
          1498,
          1499,
          1500,
          1501,
          1502,
          1503,
          1504,
          1505,
          1506,
          1507,
          1508,
          1509,
          1510,
          1511,
          1512,
          1513,
          1514,
          1515,
          1516,
          1517,
          1518,
          1519,
          1520,
          1521,
          1522,
          1523,
          1524,
          1525,
          1526,
          1527,
          1528,
          1529,
          1530,
          1531,
          1532,
          1533,
          1534,
          1535,
          1536,
          1537,
          1538,
          1539,
          1540,
          1541,
          1542,
          1543,
          1544,
          1545,
          1546,
          1547,
          1548,
          1549,
          1550,
          1551,
          1552,
          1553,
          1554,
          1555,
          1556,
          1557,
          1558,
          1559,
          1560,
          1561,
          1562,
          1563,
          1564,
          1565,
          1566,
          1567,
          1568,
          1569,
          1570,
          1571,
          1572,
          1573,
          1574,
          1575,
          1576,
          1577,
          1578,
          1579,
          1580,
          1581,
          1582,
          1583,
          1584,
          1585,
          1586,
          1587,
          1588,
          1589,
          1590,
          1591,
          1592,
          1593,
          1594,
          1595,
          1596,
          1597,
          1598,
          1599,
          1600,
          1601,
          1602,
          1603,
          1604,
          1605,
          1606,
          1607,
          1608,
          1609,
          1610,
          1611,
          1612,
          1613,
          1614,
          1615,
          1616,
          1617,
          1618,
          1619,
          1620,
          1621,
          1622,
          1623,
          1624,
          1625,
          1626,
          1627,
          1628,
          1629,
          1630,
          1631,
          1632,
          1633,
          1634,
          1635,
          1636,
          1637,
          1638,
          1639,
          1640,
          1641,
          1642,
          1643,
          1644,
          1645,
          1646,
          1647,
          1648,
          1649,
          1650,
          1651,
          1652,
          1653,
          1654,
          1655,
          1656,
          1657,
          1658,
          1659,
          1660,
          1661,
          1662,
          1663,
          1664,
          1665,
          1666,
          1667,
          1668,
          1669,
          1670,
          1671,
          1672,
          1673,
          1674,
          1675,
          1676,
          1677,
          1678,
          1679,
          1680,
          1681,
          1682,
          1683,
          1684,
          1685,
          1686,
          1687,
          1688,
          1689,
          1690,
          1691,
          1692,
          1693,
          1694,
          1695,
          1696,
          1697,
          1698,
          1699,
          1700,
          1701,
          1702,
          1703,
          1704,
          1705,
          1706,
          1707,
          1708,
          1709,
          1710,
          1711,
          1712,
          1713,
          1714,
          1715,
          1716,
          1717,
          1718,
          1719,
          1720,
          1721,
          1722,
          1723,
          1724,
          1725,
          1726,
          1727,
          1728,
          1729,
          1730,
          1731,
          1732,
          1733,
          1734,
          1735,
          1736,
          1737,
          1738,
          1739,
          1740,
          1741,
          1742,
          1743,
          1744,
          1745,
          1746,
          1747,
          1748,
          1749,
          1750,
          1751,
          1752,
          1753,
          1754,
          1755,
          1756,
          1757,
          1758,
          1759,
          1760,
          1761,
          1762,
          1763,
          1764,
          1765,
          1766,
          1767,
          1768,
          1769,
          1770,
          1771,
          1772,
          1773,
          1774,
          1775,
          1776,
          1777,
          1778,
          1779,
          1780,
          1781,
          1782,
          1783,
          1784,
          1785,
          1786,
          1787,
          1788,
          1789,
          1790,
          1791,
          1792,
          1793,
          1794,
          1795,
          1796,
          1797,
          1798,
          1799,
          1800,
          1801,
          1802,
          1803,
          1804,
          1805,
          1806,
          1807,
          1808,
          1809,
          1810,
          1811,
          1812,
          1813,
          1814,
          1815,
          1816,
          1817,
          1818,
          1819,
          1820,
          1821,
          1822,
          1823,
          1824,
          1825,
          1826,
          1827,
          1828,
          1829,
          1830,
          1831,
          1832,
          1833,
          1834,
          1835,
          1836,
          1837,
          1838,
          1839,
          1840,
          1841,
          1842,
          1843,
          1844,
          1845,
          1846,
          1847,
          1848,
          1849,
          1850,
          1851,
          1852,
          1853,
          1854,
          1855,
          1856,
          1857,
          1858,
          1859,
          1860,
          1861,
          1862,
          1863,
          1864,
          1865,
          1866,
          1867,
          1868,
          1869,
          1870,
          1871,
          1872,
          1873,
          1874,
          1875,
          1876,
          1877,
          1878,
          1879,
          1880,
          1881,
          1882,
          1883,
          1884,
          1885,
          1886,
          1887,
          1888,
          1889,
          1890,
          1891,
          1892,
          1893,
          1894,
          1895,
          1896,
          1897,
          1898,
          1899,
          1900,
          1901,
          1902,
          1903,
          1904,
          1905,
          1906,
          1907,
          1908,
          1909,
          1910,
          1911,
          1912,
          1913,
          1914,
          1915,
          1916,
          1917,
          1918,
          1919,
          1920,
          1921,
          1922,
          1923,
          1924,
          1925,
          1926,
          1927,
          1928,
          1929,
          1930,
          1931,
          1932,
          1933,
          1934,
          1935,
          1936,
          1937,
          1938,
          1939,
          1940,
          1941,
          1942,
          1943,
          1944,
          1945,
          1946,
          1947,
          1948,
          1949,
          1950,
          1951,
          1952,
          1953,
          1954,
          1955,
          1956,
          1957,
          1958,
          1959,
          1960,
          1961,
          1962,
          1963,
          1964,
          1965,
          1966,
          1967,
          1968,
          1969,
          1970,
          1971,
          1972,
          1973,
          1974,
          1975,
          1976,
          1977,
          1978,
          1979,
          1980,
          1981,
          1982,
          1983,
          1984,
          1985,
          1986,
          1987,
          1988,
          1989,
          1990,
          1991,
          1992,
          1993,
          1994,
          1995,
          1996,
          1997,
          1998,
          1999
         ],
         "xaxis": "x",
         "y": [
          0.738819633154261,
          0.712376222114435,
          0.6946079066935802,
          0.6834909047856427,
          0.6761639862252562,
          0.670770641541321,
          0.6676551363612181,
          0.6651903890923365,
          0.6637296644633248,
          0.6627753965006579,
          0.6619730759787079,
          0.6615109431663616,
          0.6612976537454849,
          0.6607487425708131,
          0.6604554521157437,
          0.6602908312874353,
          0.6600218227245663,
          0.6598587448164921,
          0.6597270333526919,
          0.659735594419825,
          0.6595246751836482,
          0.6593722281039961,
          0.6592190797696977,
          0.6591617985059751,
          0.6590303798649935,
          0.6588976075185226,
          0.6587427262491827,
          0.6588556570494735,
          0.6586812978622897,
          0.6586156631476127,
          0.6586458074966534,
          0.6585156361528691,
          0.6584090562474808,
          0.6582938240678519,
          0.6582525176489913,
          0.6582426332787379,
          0.6582451226727274,
          0.6582080701853605,
          0.6581572214228995,
          0.6579823761978405,
          0.6578843733608323,
          0.6578531381267829,
          0.6580892133232731,
          0.6578444978534775,
          0.65774614058885,
          0.6576754690816738,
          0.6578247267127837,
          0.6575294961065254,
          0.6574652963036659,
          0.6573676790967083,
          0.6573833767199676,
          0.6572677181071083,
          0.6573254702075216,
          0.6571954420748973,
          0.6571632087630713,
          0.6572200263906646,
          0.6571334516442062,
          0.6570746334607169,
          0.656932958420491,
          0.6568546839208411,
          0.6568328230172996,
          0.6567720790837435,
          0.6567233284847849,
          0.6566423133715688,
          0.6565766090514676,
          0.6565400101994509,
          0.6565068572159581,
          0.6564727349569334,
          0.6563075776868219,
          0.6563907629691514,
          0.6562420745824007,
          0.6563612454689589,
          0.6560955511643582,
          0.656026597391039,
          0.6559667543276845,
          0.6560653764129485,
          0.6559191110150125,
          0.6559818615049324,
          0.6558144908623408,
          0.6557109615946776,
          0.6557485077205121,
          0.6558484311071818,
          0.6555940976878941,
          0.6554775998096338,
          0.6556776389979676,
          0.6554841031164131,
          0.6553951269828233,
          0.6553471368431245,
          0.6553172181116654,
          0.6552960224599647,
          0.6556531954931732,
          0.6550934550746176,
          0.655187483201891,
          0.6551589701799738,
          0.6550794111802274,
          0.654907719000874,
          0.6548674950663675,
          0.6550007866533011,
          0.6547597710718245,
          0.6546570718688453,
          0.6545886073336505,
          0.6545217789259533,
          0.6545550023149324,
          0.6545415500666472,
          0.6543711379870473,
          0.6543372069429231,
          0.6543951918614791,
          0.654399964633404,
          0.6542126820391456,
          0.6543141451458003,
          0.654033307261115,
          0.6540575599510398,
          0.6540595477059383,
          0.6541330742355961,
          0.6539187607349165,
          0.6539778765415986,
          0.653757354157083,
          0.6537825649216671,
          0.6537120446262744,
          0.6536939424156343,
          0.6536568207228743,
          0.6535806131842953,
          0.6537057105326812,
          0.6534458578832998,
          0.6533983777833465,
          0.653349151147292,
          0.653309929290874,
          0.6533236675614479,
          0.6532412251370066,
          0.6531213385946799,
          0.6530654426389093,
          0.6530760754674874,
          0.6530332809326632,
          0.653285854614821,
          0.6529412237589791,
          0.652886362683853,
          0.6528969819113712,
          0.6527520210150904,
          0.6527086224331952,
          0.6526577444684586,
          0.6526567911941733,
          0.6525861912925771,
          0.6525627862687079,
          0.6525030580143001,
          0.6524554363033116,
          0.6524823047970766,
          0.6524485113636759,
          0.6523230099838052,
          0.6523144745186671,
          0.6522781808904353,
          0.6522339470434508,
          0.6521680291067034,
          0.6522078410091016,
          0.65219295424903,
          0.6519719394261405,
          0.6520466652492549,
          0.6519054978485875,
          0.6518622744803461,
          0.6518235366616473,
          0.6519616794266156,
          0.6517038925382115,
          0.6516579409573702,
          0.6517061467138713,
          0.6517483500026217,
          0.6515723550879715,
          0.651740550994873,
          0.6515641092454028,
          0.6515037525420221,
          0.6514129642672186,
          0.6513504165931036,
          0.6513848448759757,
          0.6513498189465311,
          0.6511484548549524,
          0.6512322077815165,
          0.6512169293909265,
          0.6511945724487305,
          0.6509644161134758,
          0.6510551139812342,
          0.6508578174066224,
          0.6510387339047937,
          0.6509493237373812,
          0.6507639812943119,
          0.6508207093149223,
          0.6505974291155002,
          0.650706135186573,
          0.6506604320250902,
          0.6505166368196474,
          0.6504424414378684,
          0.6504108929794107,
          0.6504676330009563,
          0.6504725957876883,
          0.6503783240414306,
          0.6503244706448292,
          0.6502453364781885,
          0.6502187352052471,
          0.6500363593933566,
          0.6500120542993482,
          0.6499786684977128,
          0.6499642873770438,
          0.6499705394642465,
          0.6499425424825425,
          0.6498240064454559,
          0.6497286242126619,
          0.6497016585113218,
          0.649791664725182,
          0.6495835849102711,
          0.6498578766848417,
          0.6495625024673922,
          0.6495551014906608,
          0.6494102774050412,
          0.6494132256347861,
          0.6493675428748931,
          0.6493717476025523,
          0.6492817793916535,
          0.6492269443185538,
          0.6492707813346146,
          0.649164508253136,
          0.6491020669873129,
          0.6491342690967074,
          0.6491253372006769,
          0.6488880067063658,
          0.648906969384059,
          0.6489456507983624,
          0.6488015767711921,
          0.6487769288504683,
          0.6487575093371756,
          0.6487082142957905,
          0.6485546555295086,
          0.6487513480570493,
          0.648546858521916,
          0.6486121160071968,
          0.6483416753327287,
          0.6485553947871163,
          0.6484223100162992,
          0.6485052580801433,
          0.6481518333390255,
          0.648209271414968,
          0.6480552162100005,
          0.6480119200360855,
          0.6479798123340479,
          0.6481842242631336,
          0.6478348282359591,
          0.6477975353298572,
          0.647812454892485,
          0.6476842457016042,
          0.6479242476040885,
          0.6475890534836174,
          0.6475847899513757,
          0.6477493443745095,
          0.6475353492986435,
          0.6474978723782021,
          0.6473984582312156,
          0.6473325134923794,
          0.6473891451054772,
          0.6473304929349246,
          0.6472955094087844,
          0.6471320710726233,
          0.6471444972409498,
          0.647190624435476,
          0.6471470518400205,
          0.6469897571826141,
          0.6469268930838412,
          0.6469520582448716,
          0.6468108552414299,
          0.6466890685510316,
          0.6467516298262065,
          0.6465778902873097,
          0.6469014130982776,
          0.6465965285397216,
          0.6467621126430947,
          0.6464955374698511,
          0.6464668276325968,
          0.6464311340511245,
          0.6463800660715807,
          0.6461970446093771,
          0.6463409742252939,
          0.646199417194264,
          0.6461524111312508,
          0.6461087817313688,
          0.6460615604515844,
          0.6459220895831217,
          0.6459929659062584,
          0.645808441926969,
          0.6457638048485621,
          0.6457597373315952,
          0.6458242519589878,
          0.6457826287154383,
          0.6456351340217078,
          0.6457549405577999,
          0.6454230226926355,
          0.6453603574093556,
          0.6454008765668677,
          0.6453776267550936,
          0.6453789168556264,
          0.6452023971000774,
          0.6451959013938904,
          0.6452566469275711,
          0.6450974085186952,
          0.644997525935205,
          0.6450301664787651,
          0.644821013380217,
          0.644949921425557,
          0.6448077987504486,
          0.6447618847725375,
          0.6446630242686944,
          0.6446146016952975,
          0.6446261413945448,
          0.6444768301592577,
          0.6445175129295195,
          0.6443498142613661,
          0.6443065400891657,
          0.6444257841814285,
          0.6443100775648284,
          0.6442738515418648,
          0.6441740237626453,
          0.6441888985217817,
          0.6440293740906171,
          0.6441658375247213,
          0.6439066937305783,
          0.6439286894446251,
          0.6437881412922136,
          0.6438207162306613,
          0.6437641290210238,
          0.6437083914775976,
          0.6437934173033542,
          0.6435870972255733,
          0.643480080486144,
          0.6434430243984964,
          0.6433762931983743,
          0.6433397027470121,
          0.6433800306896236,
          0.6434689004149213,
          0.6430863914073713,
          0.643254976144573,
          0.6433093379807953,
          0.6429836906042675,
          0.6429923088758583,
          0.6428560906608632,
          0.642816549179538,
          0.6427170942293717,
          0.6426733508206054,
          0.6427069966425032,
          0.642590949999406,
          0.6426538108979296,
          0.6423910948254118,
          0.6423671037558741,
          0.6424998188978873,
          0.6425366293663947,
          0.6421768977338036,
          0.6422033045915949,
          0.6420620583847865,
          0.6420424400560008,
          0.6420436041467141,
          0.641953070691768,
          0.641927249879645,
          0.6419577126534993,
          0.641817488926369,
          0.6419580578804016,
          0.6416629604845239,
          0.6416281613727544,
          0.6415050061757133,
          0.6414699834465181,
          0.6413792567765153,
          0.641302035958975,
          0.6413241704838388,
          0.6412968519550042,
          0.6413263358525781,
          0.6410970763872134,
          0.6410919295861417,
          0.641113921699908,
          0.6410295171225631,
          0.6411191325059673,
          0.6408558783915219,
          0.6408839505790864,
          0.6406722400812495,
          0.6407537596337747,
          0.6406065565627693,
          0.640710933496488,
          0.6404079734879052,
          0.6406549403331424,
          0.6404327070153,
          0.6402324870128759,
          0.6402488402872277,
          0.6402156673021765,
          0.6402246067987992,
          0.6404852427092175,
          0.6399547837724622,
          0.6399537864947479,
          0.6400252508637089,
          0.6400251592565703,
          0.6398148036643163,
          0.639904251434659,
          0.6396638991048672,
          0.639553316487562,
          0.6396325518620894,
          0.6395083197811305,
          0.6395822767443304,
          0.6394516989688745,
          0.6393414595783157,
          0.6391578316688538,
          0.6391833508574722,
          0.63903342397421,
          0.6392495400153551,
          0.6389441314159624,
          0.6388502108970745,
          0.6388673830352374,
          0.6389183930102611,
          0.6389740365463615,
          0.6388749936283035,
          0.6385824352302807,
          0.6386542796288561,
          0.6384999360014129,
          0.6384875702377933,
          0.6382681579397829,
          0.6382496720992479,
          0.6382243829285539,
          0.6381065349450847,
          0.6383479701592618,
          0.6379819120336699,
          0.6380982907026406,
          0.6379688174132533,
          0.6378661402119886,
          0.6379884985469332,
          0.6378885279565849,
          0.6376668122790804,
          0.6375553479930699,
          0.6374842392518216,
          0.6375371181724856,
          0.6375950750888594,
          0.637393526982941,
          0.6373518917384564,
          0.6372887732198574,
          0.6372285309253923,
          0.6370094694547205,
          0.6371483294755821,
          0.6369657144450501,
          0.6371041296312473,
          0.6368085733996142,
          0.6368039974430263,
          0.6367065230471975,
          0.6366034494150405,
          0.6365747651797813,
          0.6364455451101265,
          0.6364008756291947,
          0.6363262822163985,
          0.6362950873854977,
          0.6364074917447647,
          0.636417540127799,
          0.6362186198266561,
          0.6361869305572254,
          0.6362701498422046,
          0.6358994081516394,
          0.6358427293348632,
          0.6359846011904262,
          0.6357331655969556,
          0.6356163464936634,
          0.6355809577359449,
          0.6354479549715183,
          0.6354299963720693,
          0.6354348399495119,
          0.6352740578203393,
          0.6353502329563935,
          0.6353080728710098,
          0.6350917144109739,
          0.6350826357035029,
          0.6350657807900602,
          0.6351127792524811,
          0.6349942568164544,
          0.6346876017199267,
          0.6346998462741007,
          0.634772439931063,
          0.634688351778376,
          0.6345088961940484,
          0.6344396599987209,
          0.6345391749535632,
          0.6343018732614966,
          0.6344261745478483,
          0.6342501248289275,
          0.6341261743699145,
          0.6339604918588728,
          0.6338977409689218,
          0.6340506892876338,
          0.6338896711400691,
          0.6337687969207764,
          0.633740544719184,
          0.6337030521975268,
          0.6334784382941739,
          0.6336423778693948,
          0.6333486161776037,
          0.6333062668774752,
          0.6331823076177764,
          0.6331940769349169,
          0.6331848810183122,
          0.6331784441167077,
          0.6329844706010499,
          0.6329127602129174,
          0.6329727296861226,
          0.6328386012339752,
          0.6329035966988378,
          0.6326084252971931,
          0.6325256788490603,
          0.6326177728256123,
          0.6324220711752873,
          0.6325609339963669,
          0.6322887739879173,
          0.6321165961707198,
          0.6321156116940031,
          0.6322916078087467,
          0.6320885799875195,
          0.6318620015310761,
          0.6318372824048036,
          0.6317824929352575,
          0.631843043653757,
          0.6316718431127152,
          0.6316689020835313,
          0.631420915158803,
          0.6314628520267922,
          0.6314934396103724,
          0.6313276974946861,
          0.631630569896442,
          0.6316653690082115,
          0.631093891275009,
          0.6310730840535772,
          0.6309315842270051,
          0.6307527134882523,
          0.6306556711260904,
          0.6309411469721954,
          0.6305545248441248,
          0.6305902868309277,
          0.6304370756917352,
          0.6303861553236942,
          0.6302926000332673,
          0.6301972306014707,
          0.6302281594116416,
          0.6300358048221409,
          0.6299979910754517,
          0.6299927242650282,
          0.6298995590049948,
          0.6298504551785105,
          0.6298237634985239,
          0.6298291663195463,
          0.6295726383292435,
          0.6294888662011832,
          0.629393120740084,
          0.6294136279381362,
          0.6293739392453392,
          0.6291942768448952,
          0.6290356608845243,
          0.629022971495686,
          0.6289218792179286,
          0.6290320190007255,
          0.6290066022200872,
          0.6287754682086458,
          0.6287294054191385,
          0.6285434233262235,
          0.6286555284621732,
          0.628375220618792,
          0.6283596170028584,
          0.6283505586969772,
          0.6282991906940537,
          0.6281566051828781,
          0.6280556733176212,
          0.6279070821384456,
          0.6280910712760567,
          0.627870278070437,
          0.6279952198066967,
          0.6277515752203513,
          0.6276473210962027,
          0.6274203482090227,
          0.627357690526335,
          0.6274469598827747,
          0.6272382396179558,
          0.6270959181273543,
          0.6270402989931555,
          0.6270102066481673,
          0.6268670170899205,
          0.6268380992364564,
          0.6267628429720066,
          0.6267134807253844,
          0.6270792128255703,
          0.6265435638843767,
          0.6268268919631139,
          0.6264908661778341,
          0.6262583636597499,
          0.6263206797158158,
          0.6261158721558999,
          0.6260437089324797,
          0.6259712800883607,
          0.6258100455239315,
          0.6258369908236817,
          0.6259737354797005,
          0.6256008332207699,
          0.6258274684816398,
          0.6255818405407387,
          0.6255212830217093,
          0.6252779648608009,
          0.6255419898353167,
          0.6251350225218191,
          0.6250293706887521,
          0.6249749268461394,
          0.6248795958173355,
          0.6249540084160414,
          0.6248098259804232,
          0.624773551953719,
          0.6247963661315458,
          0.6246222681647179,
          0.6243663302203953,
          0.62426015034618,
          0.6242093451871168,
          0.6245088929297941,
          0.6241173852210077,
          0.6238932645560911,
          0.6240185783213417,
          0.6238538682860816,
          0.6238022334623656,
          0.6237094586327572,
          0.6236576257936106,
          0.6237829131568038,
          0.6236107609416014,
          0.6232171326675671,
          0.6231423556404626,
          0.6232485255138986,
          0.623158210876004,
          0.6229455159014503,
          0.6228431263225991,
          0.6229723671938749,
          0.6225928212172233,
          0.6226997103467083,
          0.622540066306223,
          0.6227345186591948,
          0.6227579828876777,
          0.622438780413378,
          0.6223157548264369,
          0.6221485121938206,
          0.6219679413225827,
          0.6219770580330151,
          0.6218437908479831,
          0.6217940429713102,
          0.6220260442503347,
          0.6216106006763126,
          0.6216855645179749,
          0.6213276802293406,
          0.6212316023423368,
          0.6213215101485284,
          0.6210814490414306,
          0.6209668461108367,
          0.6211318761710353,
          0.6207710104500688,
          0.6208713890722134,
          0.620623068921518,
          0.6205338163663877,
          0.6204937432436335,
          0.6206028953494641,
          0.6203657996734516,
          0.6201614917524709,
          0.6202343350288851,
          0.620020831991362,
          0.619882824036899,
          0.6199295948815826,
          0.619713248822513,
          0.6196684857342867,
          0.6194672736545537,
          0.619466367983978,
          0.6194600630926606,
          0.6192337278551703,
          0.6191698636784649,
          0.6190027806582867,
          0.6189404638021584,
          0.6188494155070926,
          0.6188772312746752,
          0.6189265915211415,
          0.6186875732953117,
          0.618671978880095,
          0.618458036608344,
          0.6185841912391202,
          0.6185791924495825,
          0.6182719277055472,
          0.6180498608006727,
          0.6179113236049678,
          0.6178599340003609,
          0.6177385613422266,
          0.6181304286790374,
          0.6175455419009164,
          0.6175077017521698,
          0.6177223920822144,
          0.6172267330572909,
          0.6171610795411487,
          0.6173126725542465,
          0.6171527576926571,
          0.6170299609235469,
          0.6168569442409797,
          0.6173039474743325,
          0.6168299293358054,
          0.6166447169028673,
          0.6167184962522263,
          0.61650523083322,
          0.6162726939124549,
          0.6161487662552187,
          0.6161456280106666,
          0.6161087847396032,
          0.6157632718150248,
          0.615856577886031,
          0.61569421043332,
          0.6155764580573011,
          0.6155215129756287,
          0.6153986506014062,
          0.615445930685773,
          0.6151248992689504,
          0.6152461123146467,
          0.6149746007567284,
          0.6148076957504222,
          0.6146925379765914,
          0.6148074661325288,
          0.614756680174962,
          0.6146044171096494,
          0.6143630739026422,
          0.6142819443805105,
          0.6142723468326082,
          0.6141007986644771,
          0.6142854990575137,
          0.6140567232298371,
          0.6139408702018277,
          0.6137757365335554,
          0.6138612391964701,
          0.6135701485128211,
          0.6133833923595864,
          0.613549729321627,
          0.6133625535356918,
          0.613043332259927,
          0.6130475537888955,
          0.61286988594388,
          0.6126925721264526,
          0.6127919054671422,
          0.6125829411833078,
          0.612559624966359,
          0.6122790082989124,
          0.6127685332458291,
          0.6124159017665274,
          0.6120135400119244,
          0.6119478076896412,
          0.6118117250852136,
          0.6117172097199716,
          0.6118556969117799,
          0.6114739231615258,
          0.6115204899903112,
          0.611461977430638,
          0.6113414212361278,
          0.6114736511403283,
          0.6110734599548698,
          0.6115195271152778,
          0.6109934829225476,
          0.6107571049024595,
          0.6109112913176518,
          0.6106955145029414,
          0.6105184763069921,
          0.6101550783887005,
          0.6101198168408951,
          0.6099473762032169,
          0.6100138509833573,
          0.6099113829983961,
          0.6098495573805482,
          0.6097783358305092,
          0.6097571205772809,
          0.6096408975204365,
          0.6093265334231741,
          0.6092643885804503,
          0.6092518284016808,
          0.6089423030814869,
          0.6094745893606404,
          0.6088506063358896,
          0.6086346151044705,
          0.6086615508034725,
          0.6084439234445559,
          0.6083087405102365,
          0.6081727187105473,
          0.6081113755302942,
          0.6081855989142553,
          0.6079119915930217,
          0.607908033284565,
          0.6076013881888166,
          0.6075677603683216,
          0.6073786496316027,
          0.6073243870031113,
          0.6072634662557769,
          0.6074579394103696,
          0.6069925543445869,
          0.6070495647872054,
          0.6069781408213929,
          0.6065607814980833,
          0.6065366348164194,
          0.6063587373535105,
          0.6063540461879449,
          0.6066373566653105,
          0.6062104069946597,
          0.606252284258004,
          0.605943408588435,
          0.6057048527986412,
          0.6055667596375383,
          0.605512876238599,
          0.6057384206144601,
          0.605346411266583,
          0.6053676841243002,
          0.605204754627791,
          0.6050674063247322,
          0.6052233164742489,
          0.6045952063278864,
          0.6046857257817415,
          0.6046878439468025,
          0.6045285167310062,
          0.6044128257156218,
          0.6043571093738479,
          0.6043540787376813,
          0.6046099814792607,
          0.603986590500646,
          0.6039128699558693,
          0.6036704454646015,
          0.6036115796774025,
          0.6040931064010466,
          0.6033703388783755,
          0.6033715137699306,
          0.6032187338643427,
          0.6028871308237114,
          0.6029729303097565,
          0.6030801890680454,
          0.602656343639297,
          0.6024556055965039,
          0.6024371297567482,
          0.602329761389918,
          0.6021935403747046,
          0.6019948652926708,
          0.6016858292106014,
          0.6018268542001711,
          0.6015361075433309,
          0.6013343838237276,
          0.6011876375883217,
          0.6011192022554026,
          0.6009572568355791,
          0.6010573462351857,
          0.6006925886109371,
          0.6008724602276847,
          0.6004824134327421,
          0.6003716720030612,
          0.6002832843153268,
          0.6006640475868379,
          0.5999253532230454,
          0.599808626126923,
          0.5997112889417866,
          0.599714254372872,
          0.5996803457304936,
          0.5993421529763497,
          0.5992731223970451,
          0.5992343225735146,
          0.5988990132440657,
          0.5990603358153529,
          0.5990282985188017,
          0.5994131621098359,
          0.598597831774078,
          0.5984972355349752,
          0.5983547160289432,
          0.5981702396533634,
          0.5979754988779158,
          0.5979248949345326,
          0.5977071255645496,
          0.5975126208074941,
          0.5974382006081959,
          0.5972140475407542,
          0.5970970260216886,
          0.5973694964543285,
          0.5971832047372856,
          0.5969457818357736,
          0.5966268109795231,
          0.59646218175056,
          0.596307066462984,
          0.5963313419546857,
          0.5964231147062058,
          0.596275200779806,
          0.5957593197790568,
          0.5956412345771022,
          0.5959433441194112,
          0.5954417262301349,
          0.5952520330480281,
          0.5951404559532268,
          0.5951808375000154,
          0.595176394353777,
          0.5948165899155123,
          0.5945686057910023,
          0.5945037315355851,
          0.5945043991876129,
          0.594245239392223,
          0.594384900675524,
          0.5940657386043727,
          0.5942208738935074,
          0.5938698737413292,
          0.5936655266172934,
          0.5935588731061692,
          0.5935571409711902,
          0.5934333721263296,
          0.5930067540815213,
          0.5928402503064815,
          0.5928714431372265,
          0.5926547882540915,
          0.5926947517683042,
          0.5923917269546713,
          0.5923465386333081,
          0.5920402355642127,
          0.5919250745901325,
          0.5923808197047087,
          0.5917923354462489,
          0.5916105032767225,
          0.5914485874592058,
          0.5915940067912108,
          0.5912116749974705,
          0.5910918712615967,
          0.5909201882829602,
          0.5908002413359265,
          0.5906455336801157,
          0.5904031675933992,
          0.591024937645701,
          0.5903895713338916,
          0.5902081263945407,
          0.5901333329661581,
          0.5902635623144623,
          0.5900825822113345,
          0.5896240344783604,
          0.589737147692866,
          0.5898164498725994,
          0.589551542429316,
          0.5893848398387832,
          0.5888653209545468,
          0.5888725931212406,
          0.5887034127376224,
          0.5883552000026575,
          0.5883565025041567,
          0.5882960129904267,
          0.588096401835448,
          0.5879394972084353,
          0.5877544075850673,
          0.5879043232674567,
          0.5875883950483078,
          0.5874487077629806,
          0.5872188146482378,
          0.5872049435673145,
          0.5872125453596947,
          0.5867772650398664,
          0.5869816761688899,
          0.5865085565003773,
          0.5863759137640063,
          0.5864996026026322,
          0.5863321987574532,
          0.5863648208995793,
          0.5862281138464909,
          0.5856958671704234,
          0.5854875329356866,
          0.5853236625658585,
          0.5853635344729328,
          0.585113927821985,
          0.5850155493557053,
          0.5849286897870518,
          0.5857178140806671,
          0.5846105633966074,
          0.5847858862588869,
          0.5841800690497327,
          0.5841360284178049,
          0.584450252904188,
          0.5838301777839661,
          0.583725094795227,
          0.5836395465287586,
          0.5835348799724707,
          0.5833757467717933,
          0.5831688742509624,
          0.5829867788609242,
          0.5829136919655256,
          0.5827610296691024,
          0.5825339379726641,
          0.5824970182956465,
          0.5822035890297602,
          0.5824756398296996,
          0.5821033732203029,
          0.5820149847325062,
          0.5817092953112302,
          0.581623491024811,
          0.5818031622259409,
          0.5817025383846872,
          0.581108190869325,
          0.5813059978837135,
          0.5809935683372037,
          0.5808020938162836,
          0.5807725527142519,
          0.5803303722567206,
          0.5804952983088141,
          0.580376025814338,
          0.5799542129439795,
          0.5802849735189605,
          0.5798202605055482,
          0.5798329798967247,
          0.5792960628567126,
          0.579402437146078,
          0.5790955748334027,
          0.5788723630393111,
          0.5792885834738712,
          0.5790730694796415,
          0.578648828420063,
          0.5784281544237329,
          0.5786544216559237,
          0.5782445025124006,
          0.5780989635710748,
          0.5776412615040004,
          0.5783576577301793,
          0.5778155662869447,
          0.5782782535424968,
          0.5770674223867839,
          0.5772339653648786,
          0.5770901521580332,
          0.577698123935085,
          0.5768736136839694,
          0.5763237296334849,
          0.576532757922307,
          0.5766530300946844,
          0.5759651204883652,
          0.5761642368047829,
          0.5757688983974841,
          0.5757708573501382,
          0.5754964103634725,
          0.5753425511737797,
          0.5751274904148691,
          0.575125954695196,
          0.5748353856521965,
          0.574659401538388,
          0.5746878709569073,
          0.5743295966378794,
          0.5743025109271875,
          0.5740345308444644,
          0.5741002147629757,
          0.5737718575752822,
          0.5735706066925254,
          0.5736167422877062,
          0.5733622900591601,
          0.5736442968349329,
          0.573224957357317,
          0.5728004494769461,
          0.5733985656859891,
          0.5729320489320179,
          0.5724641496703129,
          0.5721796258983997,
          0.572242458794741,
          0.5722199614416033,
          0.5726776623085841,
          0.5716045850875394,
          0.5714311967760124,
          0.5712690521406647,
          0.5711408949538366,
          0.5713640147407583,
          0.5707215086725734,
          0.5709409241708333,
          0.5710551870749301,
          0.5702651235081205,
          0.5706789317547075,
          0.57011668074051,
          0.569896062348513,
          0.57006139883259,
          0.570120487037121,
          0.5694427346223153,
          0.5692876557375761,
          0.5697548093411746,
          0.5691859218098173,
          0.5691168960308869,
          0.5687906578083166,
          0.5687092902676371,
          0.5686312297846647,
          0.568358694947006,
          0.5680924926028156,
          0.5682348908994022,
          0.5678765813776311,
          0.5677661307706129,
          0.5674169251582767,
          0.5675470420978214,
          0.5671122678174269,
          0.5671965247832689,
          0.5668629543893289,
          0.566901163366817,
          0.5666331396806961,
          0.5667209365223879,
          0.5661754624155544,
          0.5661273102632305,
          0.566029655853374,
          0.5663771825348771,
          0.5658078717705387,
          0.5654061956693662,
          0.5652113288840992,
          0.5656226241348574,
          0.56501920671271,
          0.5650894933898977,
          0.5647468810913546,
          0.5645426807787595,
          0.5644025186564299,
          0.5645061251301093,
          0.5640754923724488,
          0.5637811258335241,
          0.5637214115802074,
          0.5638925761184437,
          0.5636666473126252,
          0.5633624324062526,
          0.5631526436581707,
          0.5630499212533836,
          0.5626982414482424,
          0.5632266726269818,
          0.5623705311109556,
          0.5625879576542233,
          0.5622750656716775,
          0.5627143787057608,
          0.5620417602910291,
          0.5617315217152538,
          0.5618575223340284,
          0.5615560276396323,
          0.5614195266025979,
          0.5610825311417548,
          0.5607272710576153,
          0.5609646891587533,
          0.5616095874133527,
          0.5603767105396962,
          0.5601827714267194,
          0.5604784288662392,
          0.5603939250811635,
          0.5600973911733436,
          0.5598300911436145,
          0.559672874892318,
          0.5597990831272714,
          0.5591363322814838,
          0.5597870693910842,
          0.5587936391766439,
          0.558807181031912,
          0.5582915604514563,
          0.5586842394515172,
          0.5586185479324136,
          0.558576806679668,
          0.5583327784634277,
          0.5577862834770407,
          0.5580219722434179,
          0.5575688149304998,
          0.5574189276503236,
          0.5574442720253195,
          0.5567593790540759,
          0.5566479447704034,
          0.5567471473008995,
          0.5566493900830314,
          0.557126152035374,
          0.5560037917738793,
          0.5562961865591523,
          0.5561233606914546,
          0.555941930553257,
          0.556086135790652,
          0.5555298772434261,
          0.5553628398268015,
          0.5550564227488217,
          0.555571189262723,
          0.5550248554888988,
          0.5548278669382902,
          0.5541043045536783,
          0.5548623388245602,
          0.553929631742055,
          0.5548900549843807,
          0.5538945802106153,
          0.5535574695407944,
          0.5550117092644609,
          0.5536487778561228,
          0.5530669453160074,
          0.5527996756886476,
          0.5526563425192097,
          0.5534584980283007,
          0.5535922460508027,
          0.5523037362418719,
          0.5520011382615007,
          0.5521368268352227,
          0.5516659445410607,
          0.5522575670440725,
          0.5519720199924187,
          0.5513059092848093,
          0.5512140341253089,
          0.5508164831455922,
          0.5511625017095733,
          0.5504265331581935,
          0.5504575447748171,
          0.5503404848527589,
          0.5498678580226514,
          0.5498669307503924,
          0.5497057517902963,
          0.5496324144753834,
          0.5493028779957918,
          0.5491381167565417,
          0.5489308690064706,
          0.5491200797509828,
          0.5484954750777891,
          0.5486694478348597,
          0.5483416542910889,
          0.5484418717006709,
          0.5481402737982322,
          0.5481415426971128,
          0.5482093651822749,
          0.547577685957787,
          0.5472313997729513,
          0.5479470963446086,
          0.5470754524205355,
          0.5468629462606955,
          0.5465726276372103,
          0.5476963500048491,
          0.547391044613499,
          0.5467020845253195,
          0.5463949021877058,
          0.5458461098222924,
          0.545672465890846,
          0.5456954824844463,
          0.5452656185867002,
          0.5457298267607721,
          0.5451426958077706,
          0.5459329133865817,
          0.5456732547523191,
          0.5444771827467336,
          0.5450244665945936,
          0.5448205991079343,
          0.5446148360735618,
          0.5445454016627881,
          0.5446283153239513,
          0.5440226857294173,
          0.5438870363587501,
          0.5435660073421146,
          0.543053966640626,
          0.5429900120568756,
          0.5437524246689457,
          0.543093808145331,
          0.5435745908109934,
          0.5422287811368904,
          0.5421954661407726,
          0.542234829608226,
          0.5423394685623629,
          0.5420580586331003,
          0.541662717025552,
          0.5412490539902809,
          0.541124470281921,
          0.5410935822749298,
          0.5413546554194201,
          0.5408026920068985,
          0.5409198579372175,
          0.5410340336345186,
          0.5405019793734455,
          0.5399908027392906,
          0.5407844393845372,
          0.5397401704084153,
          0.540653777602535,
          0.5401691738391082,
          0.5392608826592464,
          0.5389279283133129,
          0.5391672993666373,
          0.5392928779525245,
          0.5392060167837462,
          0.5383510769613638,
          0.5386518732013318,
          0.5383892515361709,
          0.5379877642497121,
          0.5383367440444511,
          0.5382236474312392,
          0.537813018232384,
          0.5372216949526896,
          0.5372245187727397,
          0.5374106942807269,
          0.537236616115442,
          0.5376435358252302,
          0.5367328776609177,
          0.5362947967228473,
          0.5366151052833403,
          0.535964837410306,
          0.5359822727689807,
          0.536266314303315,
          0.5354920365666384,
          0.5351792773944419,
          0.5355090366913968,
          0.5357717239616702,
          0.5352622506602499,
          0.5352789947650577,
          0.5345709023859677,
          0.5347851154788229,
          0.5345062769499401,
          0.5344167943768854,
          0.5341432650617305,
          0.5339159097447491,
          0.5340331928842019,
          0.5340971714698228,
          0.5332592641747238,
          0.5336243698260928,
          0.5329270254845587,
          0.5332092162746711,
          0.5324957542771461,
          0.5323773882533079,
          0.5326526445030366,
          0.5321611942060842,
          0.5322054304532556,
          0.5315516751084551,
          0.531498171339099,
          0.5319321103544044,
          0.5316454108129411,
          0.5311345228012776,
          0.5312146584459599,
          0.5307008754086975,
          0.5304646880034632,
          0.5305329805252536,
          0.5308020698944195,
          0.5302202985590736,
          0.5301686371332847,
          0.5299050761949295,
          0.5300507217445629,
          0.5302394522516519,
          0.529566533613525,
          0.5301137406553998,
          0.529232394375257,
          0.5295933975069315,
          0.5298138838886415,
          0.5286061477741139,
          0.5282495253838149,
          0.5284599849841739,
          0.5286570483406118,
          0.529010566689024,
          0.5281115786341213,
          0.5276882588463342,
          0.527946035733959,
          0.5277954794016461,
          0.52740639488169,
          0.5272678564858917,
          0.5270298299373396,
          0.5269953388495733,
          0.5268808475276768,
          0.5270267612982116,
          0.5270542886433185,
          0.5258386811154001,
          0.5258570841494823,
          0.5256200328769299,
          0.5258041996283819,
          0.5253632714684378,
          0.5255375112463164,
          0.526171772672026,
          0.5250095545045481,
          0.5246102301866417,
          0.5249159280085723,
          0.5248021271404804,
          0.5246931614491763,
          0.5240418330935024,
          0.5241625292989232,
          0.5243383301984543,
          0.5235494699254132,
          0.5246101777825579,
          0.5235995570285208,
          0.5244430311574232,
          0.5239743238327487,
          0.5232500173901552,
          0.5225225491011702,
          0.5225816021029581,
          0.5225109447968886,
          0.5223150061280936,
          0.5224704154385816,
          0.5217723860436638,
          0.5216778468765668,
          0.521289304998897,
          0.5211466170797412,
          0.5226032327885596,
          0.5213518002689285,
          0.5210045058055212,
          0.520881284803352,
          0.5208133835120489,
          0.5209626687453097,
          0.5214228802079323,
          0.5201234061445966,
          0.521075751157415,
          0.5198029239705745,
          0.5206653611772012,
          0.5197570183933181,
          0.519912404101967,
          0.5194568294006706,
          0.5193967483187681,
          0.5195355555355149,
          0.5190267268843298,
          0.5189419208757029,
          0.5187564816250897,
          0.5183739698173215,
          0.518199333408535,
          0.5180604333845561,
          0.5178366317445,
          0.5183934585360073,
          0.5177030671362909,
          0.5175173754660075,
          0.5173986590148618,
          0.5172476062438632,
          0.5167722762031043,
          0.5166645290067532,
          0.516480253046791,
          0.5164385257951365,
          0.5167406925419032,
          0.516568556727979,
          0.5159933991080162,
          0.5159030382665212,
          0.5155784031688767,
          0.5162419828792546,
          0.5153504886083154,
          0.5157999080299531,
          0.5153321515793768,
          0.5147137729913597,
          0.5149527976176883,
          0.5152997536547232,
          0.5147501478259195,
          0.5146403804721448,
          0.514904561458818,
          0.5145011584230718,
          0.5150447295816153,
          0.5138971399540869,
          0.5140314782225845,
          0.5135492534445436,
          0.5132630261398802,
          0.5135642754151517,
          0.5127915632004706,
          0.5127607776014597,
          0.5125776141281896,
          0.5125314129278964,
          0.5127855071285427,
          0.5126850396994777,
          0.5124055373188633,
          0.5122094498384719,
          0.5117349776645634,
          0.5119264189828963,
          0.5117854695992182,
          0.511524209240139,
          0.5111212248370152,
          0.5114668723720832,
          0.5122615362973821,
          0.510604706786623,
          0.5104833689312007,
          0.5110362084119912,
          0.5101493200999778,
          0.5102487462478996,
          0.5103203142249344,
          0.5105433348041253,
          0.5099710158053661,
          0.5109421666836579,
          0.5095001017087258,
          0.5090310983609834,
          0.5093016364430422,
          0.5089409075327368,
          0.5094324534371395,
          0.509510976356148,
          0.5090230555342348,
          0.508865615465497,
          0.5084183356105881,
          0.5084686879343634,
          0.5084948335717988,
          0.5081201215718416,
          0.5088085862214133,
          0.5076796452471074,
          0.5076079084569176,
          0.5092626886079775,
          0.5088096159016526,
          0.5068308769056461,
          0.5071138595574655,
          0.5072396945633344,
          0.5063474622348811,
          0.5064636860917878,
          0.5068757097993121,
          0.5077473097199562,
          0.5060520950179772,
          0.5062642139476418,
          0.5057762881253389,
          0.5056650764590142,
          0.5058603378750334,
          0.5062796113475058,
          0.5048536782296712,
          0.504851335247091,
          0.504603249514663,
          0.5046500067582866,
          0.5047822606643574,
          0.5049386412505336,
          0.5045143405863103,
          0.5043129092894945,
          0.5049414114664065,
          0.5052114397087353,
          0.5042729125726944,
          0.5042610608491321,
          0.5037761492617179,
          0.5038583480671748,
          0.5033636777192955,
          0.5033329399240097,
          0.503928449729945,
          0.5029075613757908,
          0.5028819053765111,
          0.5024671682575405,
          0.5022949084339526,
          0.5025014301274446,
          0.5031717763651138,
          0.501678703815345,
          0.5020390832984207,
          0.5026181898261076,
          0.5018923788662725,
          0.5014536616786215,
          0.5016170414102158,
          0.5010574956068257,
          0.5010738616821749,
          0.5011571965761633,
          0.5006088578461001,
          0.500378903926619,
          0.5010554994512725,
          0.5013517953405444,
          0.50033025213536,
          0.5014376418302523,
          0.5000004592357866,
          0.49986188723736963,
          0.5002858408745503,
          0.500103794888362,
          0.4993384720495083,
          0.4993939859755087,
          0.4991348575825659,
          0.5000676616726306,
          0.49960469399522617,
          0.4995572903012269,
          0.4997480203641341,
          0.49858966969803675,
          0.49830980108888356,
          0.49904467515497397,
          0.4979498632402228,
          0.5000688405644974,
          0.49785189640602007,
          0.4996359458305692,
          0.49752563198140803,
          0.4976851504121051,
          0.4982399284439599,
          0.49711517639608194,
          0.4975114764783206,
          0.4971254191942663,
          0.4970823844407229,
          0.49722615944459136,
          0.496314464199463,
          0.49734194406727017,
          0.49667275071944167,
          0.4965564170539779,
          0.4959378894543488,
          0.49566052504033853,
          0.4960564102902508,
          0.496105770136686,
          0.49644194573364003,
          0.49566966235237636,
          0.4953331203268678,
          0.4951739083200493,
          0.49520483673018895,
          0.4949364888188023,
          0.4945720870222821,
          0.4948895061576126,
          0.4945550382937361,
          0.49540829258477126,
          0.4947285184124172,
          0.49458824588148387,
          0.49462981672094974,
          0.4948801974322172,
          0.4937438420801355,
          0.4942649682896249,
          0.4943598592841385,
          0.4932551752000847,
          0.4937052850755269,
          0.49415850119302734,
          0.4933255774862814,
          0.4928999359175663,
          0.4929548157941575,
          0.49271995409223057,
          0.4925818353291326,
          0.49226166377931635,
          0.49282296711966495,
          0.4923852382090268,
          0.4921333289786473,
          0.49272223206974514,
          0.49170939674313435,
          0.4916433069929981,
          0.49295471418623954,
          0.4912877498857127,
          0.4920433821694163,
          0.49178757863556777,
          0.4911635122043174,
          0.49085749615758856,
          0.4906976198990073,
          0.4910478119882161,
          0.4907885621858123,
          0.49032193722340883,
          0.490957348138694,
          0.49067121264118474,
          0.49090022368719116,
          0.4898746213656944,
          0.4897153419536232,
          0.4895700616324508,
          0.48971361041869094,
          0.49091134455380026,
          0.49059491869587224,
          0.4896142474756945,
          0.4896182093844318,
          0.48938453757522893,
          0.4886912763518775,
          0.48887856614669695,
          0.4893884298785421,
          0.48887780248718776,
          0.49005979599568666,
          0.48827950326388314,
          0.48956543247171697,
          0.4881271337902786,
          0.4885841852066501,
          0.48778086620689237,
          0.488536599098436,
          0.48849218103709635,
          0.48804169293218014,
          0.48799093137651484,
          0.4879691928825122,
          0.48768986611558285,
          0.48695249485489506,
          0.4879086973683146,
          0.48773074910144676,
          0.4887645504618651,
          0.4868330633480277,
          0.4866984298565244,
          0.486649506444099,
          0.4862716976028161,
          0.4863599284783306,
          0.4861876598540569,
          0.48594309619609144,
          0.48676304669188175,
          0.48666762385592366,
          0.4866419022515316,
          0.4874039116321794,
          0.4882613318478501,
          0.4854916822190253,
          0.4857121894023562,
          0.48671223213208603,
          0.48570044348703934,
          0.48559215404843326,
          0.484906191393833,
          0.48478725532557343,
          0.4846704754253362,
          0.48446812165663544,
          0.48423240208785806,
          0.48439807359804243,
          0.48420621804743,
          0.4841392864316902,
          0.48389441514975273,
          0.48430130785743664,
          0.48388214259339657,
          0.4838276641480875,
          0.4848575080001114,
          0.48459425028538544,
          0.4845808556415891,
          0.48418356868244655,
          0.48302683774256866,
          0.4828985233434895,
          0.4844489925659743,
          0.48256892145880115,
          0.4826425681978264,
          0.4825740614593429,
          0.48336103518537227,
          0.48275346483960246,
          0.4826036359639776,
          0.48468690590570435,
          0.48294596624054364,
          0.48192043532461126,
          0.4823514239100002,
          0.4822851039419238,
          0.48325113102093636,
          0.48168232296937263,
          0.48185005604020703,
          0.48267326979029096,
          0.48115020630343647,
          0.4815889398923656,
          0.4829106398876881,
          0.48085850777242006,
          0.4806294339215195,
          0.48127415916263655,
          0.4807203752082466,
          0.48224681515821677,
          0.48108228361046557,
          0.4804751556991731,
          0.4804031578886429,
          0.4805052896474032,
          0.48054989792356556,
          0.48033370047607676,
          0.48012457677981996,
          0.48019719923902676,
          0.47994830624369167,
          0.4802657973846333,
          0.4805945846058378,
          0.47964980798279677,
          0.4797033845578264,
          0.48087336413012255,
          0.48008457246242753,
          0.47944453558665795,
          0.47927143689770024,
          0.47869251518441525,
          0.4785744017802629,
          0.4790277661093129,
          0.4788467806057642,
          0.4794692865154087,
          0.47923069772304305,
          0.4803743938471647,
          0.47786763430441787,
          0.477999420774063,
          0.4784684211215717,
          0.4778438502510122,
          0.47803318260500094,
          0.47781690315112174,
          0.4774740394329865,
          0.4782143679241206,
          0.47789283866850324,
          0.4783370618852193,
          0.47834440365733716,
          0.47725251377028904,
          0.4787211780180067,
          0.47697306039349346,
          0.4772874484926262,
          0.47738891459951466,
          0.4764329181021492,
          0.47749004768045156,
          0.47646292404040397,
          0.47695609967180547,
          0.47628708533792685,
          0.47631045075871,
          0.4764336895622663,
          0.4766996158849473,
          0.47578855828950867,
          0.47644883034213276,
          0.47547501965657174,
          0.47585299471080705,
          0.4753349625424251,
          0.47556112376635507,
          0.4764619369634846,
          0.4751050548265444,
          0.4754026200147283,
          0.4754334740190698,
          0.47520546825140114,
          0.4749126698346746,
          0.47516112659601556,
          0.4747545543133012,
          0.47531954794122067,
          0.4751210880759578,
          0.47463702875495756,
          0.4752069043633122,
          0.47459614276885986,
          0.4768779181794032,
          0.4743166557894457,
          0.47405787602367017,
          0.4738021880988307,
          0.47364423619020707,
          0.4741427400368172,
          0.4738296220366587,
          0.4737160669877225,
          0.4737145652706991,
          0.4746762050078219,
          0.4731523076559873,
          0.4735791083150262,
          0.47346468999881874,
          0.47331100262251474,
          0.4754375707383124,
          0.4745004403111119,
          0.47425934652354096,
          0.47393180139912855,
          0.473520477746157,
          0.47252639668099833,
          0.4724974618262093,
          0.47249498423313935,
          0.47369054419882345,
          0.4722901910743457,
          0.47259857170533814,
          0.47520320067469707,
          0.4729807820896174,
          0.47430595475555265,
          0.4734498436418956,
          0.4720680115606961,
          0.4725247411519889,
          0.4722192695476865,
          0.4728070957548666,
          0.47233066702849114,
          0.4717541579032104,
          0.47112156380742987,
          0.47223615906382566,
          0.4719701735365311,
          0.47150817433459646,
          0.47093692001880416,
          0.47165662630292393,
          0.4729496892666657,
          0.47158246652391933,
          0.4707767811397578,
          0.47346888212549604,
          0.4708078185583921,
          0.470691475892227,
          0.4710125151096574,
          0.47088145629671596,
          0.47051640544961765,
          0.47107427012200326,
          0.4708539627542432,
          0.4704602460733196,
          0.46976114959524784,
          0.470100439234868,
          0.46960933196464644,
          0.46957376379294685,
          0.46981476257311416,
          0.4692995932278217,
          0.4711382561082008,
          0.46954903066558323,
          0.469136456515165,
          0.4692320331631091,
          0.4696263322894205,
          0.46937884860390783,
          0.470553490139494,
          0.46916213891650205,
          0.4696039425446683,
          0.4690531928267255,
          0.46849177147718085,
          0.4689436646915922,
          0.46932956756361377,
          0.4710917018803974,
          0.46912964118407074,
          0.4702325339285319,
          0.46851266970570454,
          0.4682299748763142,
          0.4680081145875406,
          0.4690926130986054,
          0.46789241777170426,
          0.4684740770983216,
          0.46845665554072236,
          0.4677939989022761,
          0.46833322812246797,
          0.4673371863045148,
          0.4690857269619935,
          0.46816877290706504,
          0.46930397076894775,
          0.4682283921529783,
          0.4672953142415757,
          0.4674563841931772,
          0.46799801780073436,
          0.4671149085832122,
          0.4682875515630581,
          0.46802700125931096,
          0.4675139672804199,
          0.4687031513492533,
          0.4690928215148465,
          0.4681157057717342,
          0.46647415025122213,
          0.4662184117224393,
          0.4676883698709859,
          0.46779776079542684,
          0.46660166858826707,
          0.46921370633496534,
          0.4679501614714629,
          0.46691783262579234,
          0.46640457082914827,
          0.4663272772059345,
          0.4661682224113669,
          0.46621207382855,
          0.4669678025197663,
          0.46595437114670774,
          0.46691473018403024,
          0.46585119930689767,
          0.46709851050536905,
          0.46551316616519184,
          0.4659411565169392,
          0.466359696932287,
          0.46490730375251516,
          0.4651592841884434,
          0.4649782784833204,
          0.4652293784506369,
          0.46839590060630903,
          0.46519400389402504,
          0.4650704788681645,
          0.46605831864696223,
          0.4651840643194698,
          0.46467543288365304,
          0.46479638090869724,
          0.465243019913667,
          0.4644182412416343,
          0.46412417352599583,
          0.4656185063739751,
          0.4643882137016962,
          0.4642484292087939,
          0.4664195409957195,
          0.4637732219776051,
          0.46540717530570574,
          0.46422682472523424,
          0.46441035262690294,
          0.4646666866020868,
          0.46451679252138073,
          0.4637587694513718,
          0.463337820648347,
          0.46387831556717024,
          0.4635776111343563,
          0.4637439042929835,
          0.4641276579975282,
          0.4636893704433569,
          0.4634150194641728,
          0.46385038739082796,
          0.46404146668094914,
          0.4653019637069446,
          0.4652386795754401,
          0.46262316795803554,
          0.463880380128054,
          0.4624272028871831,
          0.4644649088782752,
          0.46419256645561063,
          0.46554322490756145,
          0.4627673991974568,
          0.4624391394173539,
          0.46267471337478433,
          0.4639683521833996,
          0.4630977951440235,
          0.46255221082860193,
          0.46317665208906134,
          0.46230687591053493,
          0.46264351674374316,
          0.4632204507021296,
          0.4623466048064648,
          0.4621197363274209,
          0.4622432337511306,
          0.4624426168883407,
          0.46144895765605387,
          0.46293562130640015,
          0.4615032740887379,
          0.4640923268442986,
          0.46225075073690225,
          0.4623163944522806,
          0.46158192261753467,
          0.462394727756513,
          0.4614136213024191,
          0.46189481019973755,
          0.46126633122463356,
          0.46147923901577126,
          0.46519811961474833,
          0.4610962903739622,
          0.46089446584650334,
          0.4612435586500488,
          0.46095844383207746,
          0.460435460677883,
          0.4607755433793036,
          0.4606631326195378,
          0.46053062649381243,
          0.4621430963477832,
          0.4627687307012161,
          0.4609294117697133,
          0.4622488881917608,
          0.46140149615755016,
          0.46037651848473005,
          0.4617098433859396,
          0.4616649388866937,
          0.46154918266622813,
          0.4605418619693526,
          0.46034837189136735,
          0.46095363885764307,
          0.459673750320537,
          0.461285272500659,
          0.4611910769603396,
          0.45945993265849633,
          0.4594815193406688,
          0.45986066868641234,
          0.4598529944883897,
          0.4610946726478986,
          0.4593257806045097,
          0.4606112235744528,
          0.45971489732697507,
          0.4598827556075666,
          0.4601790993005637,
          0.4607325896320727,
          0.4618873214161636,
          0.4594774726253228,
          0.458910342630924,
          0.4592064933088802,
          0.459697019533823,
          0.4597945909372112,
          0.4589330780026097,
          0.4604849011305995,
          0.4596253919521434,
          0.4590771688310892,
          0.4590405287758616,
          0.4586509882203685,
          0.45874269956710356,
          0.45920642110325344,
          0.45809600417245955,
          0.4587919792072885,
          0.4600694791582607,
          0.4589395675083135,
          0.45854664108897214,
          0.4582433964582098,
          0.4590664361947335,
          0.4580833591870814,
          0.4612910263490357,
          0.4585629999637604,
          0.4586106942804068,
          0.45767926329734343,
          0.45798636982904983,
          0.45777648127319026,
          0.45958514481582896,
          0.4577254044929607,
          0.4594125539664454,
          0.45830723583298244,
          0.4576945600893674,
          0.46099939522327193,
          0.45753256666580305,
          0.4591185242537684,
          0.4579116806087878
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "value=validationLoss<br>epoch=%{x}<br>Validation Loss=%{y}<extra></extra>",
         "legendgroup": "validationLoss",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "validationLoss",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999,
          1000,
          1001,
          1002,
          1003,
          1004,
          1005,
          1006,
          1007,
          1008,
          1009,
          1010,
          1011,
          1012,
          1013,
          1014,
          1015,
          1016,
          1017,
          1018,
          1019,
          1020,
          1021,
          1022,
          1023,
          1024,
          1025,
          1026,
          1027,
          1028,
          1029,
          1030,
          1031,
          1032,
          1033,
          1034,
          1035,
          1036,
          1037,
          1038,
          1039,
          1040,
          1041,
          1042,
          1043,
          1044,
          1045,
          1046,
          1047,
          1048,
          1049,
          1050,
          1051,
          1052,
          1053,
          1054,
          1055,
          1056,
          1057,
          1058,
          1059,
          1060,
          1061,
          1062,
          1063,
          1064,
          1065,
          1066,
          1067,
          1068,
          1069,
          1070,
          1071,
          1072,
          1073,
          1074,
          1075,
          1076,
          1077,
          1078,
          1079,
          1080,
          1081,
          1082,
          1083,
          1084,
          1085,
          1086,
          1087,
          1088,
          1089,
          1090,
          1091,
          1092,
          1093,
          1094,
          1095,
          1096,
          1097,
          1098,
          1099,
          1100,
          1101,
          1102,
          1103,
          1104,
          1105,
          1106,
          1107,
          1108,
          1109,
          1110,
          1111,
          1112,
          1113,
          1114,
          1115,
          1116,
          1117,
          1118,
          1119,
          1120,
          1121,
          1122,
          1123,
          1124,
          1125,
          1126,
          1127,
          1128,
          1129,
          1130,
          1131,
          1132,
          1133,
          1134,
          1135,
          1136,
          1137,
          1138,
          1139,
          1140,
          1141,
          1142,
          1143,
          1144,
          1145,
          1146,
          1147,
          1148,
          1149,
          1150,
          1151,
          1152,
          1153,
          1154,
          1155,
          1156,
          1157,
          1158,
          1159,
          1160,
          1161,
          1162,
          1163,
          1164,
          1165,
          1166,
          1167,
          1168,
          1169,
          1170,
          1171,
          1172,
          1173,
          1174,
          1175,
          1176,
          1177,
          1178,
          1179,
          1180,
          1181,
          1182,
          1183,
          1184,
          1185,
          1186,
          1187,
          1188,
          1189,
          1190,
          1191,
          1192,
          1193,
          1194,
          1195,
          1196,
          1197,
          1198,
          1199,
          1200,
          1201,
          1202,
          1203,
          1204,
          1205,
          1206,
          1207,
          1208,
          1209,
          1210,
          1211,
          1212,
          1213,
          1214,
          1215,
          1216,
          1217,
          1218,
          1219,
          1220,
          1221,
          1222,
          1223,
          1224,
          1225,
          1226,
          1227,
          1228,
          1229,
          1230,
          1231,
          1232,
          1233,
          1234,
          1235,
          1236,
          1237,
          1238,
          1239,
          1240,
          1241,
          1242,
          1243,
          1244,
          1245,
          1246,
          1247,
          1248,
          1249,
          1250,
          1251,
          1252,
          1253,
          1254,
          1255,
          1256,
          1257,
          1258,
          1259,
          1260,
          1261,
          1262,
          1263,
          1264,
          1265,
          1266,
          1267,
          1268,
          1269,
          1270,
          1271,
          1272,
          1273,
          1274,
          1275,
          1276,
          1277,
          1278,
          1279,
          1280,
          1281,
          1282,
          1283,
          1284,
          1285,
          1286,
          1287,
          1288,
          1289,
          1290,
          1291,
          1292,
          1293,
          1294,
          1295,
          1296,
          1297,
          1298,
          1299,
          1300,
          1301,
          1302,
          1303,
          1304,
          1305,
          1306,
          1307,
          1308,
          1309,
          1310,
          1311,
          1312,
          1313,
          1314,
          1315,
          1316,
          1317,
          1318,
          1319,
          1320,
          1321,
          1322,
          1323,
          1324,
          1325,
          1326,
          1327,
          1328,
          1329,
          1330,
          1331,
          1332,
          1333,
          1334,
          1335,
          1336,
          1337,
          1338,
          1339,
          1340,
          1341,
          1342,
          1343,
          1344,
          1345,
          1346,
          1347,
          1348,
          1349,
          1350,
          1351,
          1352,
          1353,
          1354,
          1355,
          1356,
          1357,
          1358,
          1359,
          1360,
          1361,
          1362,
          1363,
          1364,
          1365,
          1366,
          1367,
          1368,
          1369,
          1370,
          1371,
          1372,
          1373,
          1374,
          1375,
          1376,
          1377,
          1378,
          1379,
          1380,
          1381,
          1382,
          1383,
          1384,
          1385,
          1386,
          1387,
          1388,
          1389,
          1390,
          1391,
          1392,
          1393,
          1394,
          1395,
          1396,
          1397,
          1398,
          1399,
          1400,
          1401,
          1402,
          1403,
          1404,
          1405,
          1406,
          1407,
          1408,
          1409,
          1410,
          1411,
          1412,
          1413,
          1414,
          1415,
          1416,
          1417,
          1418,
          1419,
          1420,
          1421,
          1422,
          1423,
          1424,
          1425,
          1426,
          1427,
          1428,
          1429,
          1430,
          1431,
          1432,
          1433,
          1434,
          1435,
          1436,
          1437,
          1438,
          1439,
          1440,
          1441,
          1442,
          1443,
          1444,
          1445,
          1446,
          1447,
          1448,
          1449,
          1450,
          1451,
          1452,
          1453,
          1454,
          1455,
          1456,
          1457,
          1458,
          1459,
          1460,
          1461,
          1462,
          1463,
          1464,
          1465,
          1466,
          1467,
          1468,
          1469,
          1470,
          1471,
          1472,
          1473,
          1474,
          1475,
          1476,
          1477,
          1478,
          1479,
          1480,
          1481,
          1482,
          1483,
          1484,
          1485,
          1486,
          1487,
          1488,
          1489,
          1490,
          1491,
          1492,
          1493,
          1494,
          1495,
          1496,
          1497,
          1498,
          1499,
          1500,
          1501,
          1502,
          1503,
          1504,
          1505,
          1506,
          1507,
          1508,
          1509,
          1510,
          1511,
          1512,
          1513,
          1514,
          1515,
          1516,
          1517,
          1518,
          1519,
          1520,
          1521,
          1522,
          1523,
          1524,
          1525,
          1526,
          1527,
          1528,
          1529,
          1530,
          1531,
          1532,
          1533,
          1534,
          1535,
          1536,
          1537,
          1538,
          1539,
          1540,
          1541,
          1542,
          1543,
          1544,
          1545,
          1546,
          1547,
          1548,
          1549,
          1550,
          1551,
          1552,
          1553,
          1554,
          1555,
          1556,
          1557,
          1558,
          1559,
          1560,
          1561,
          1562,
          1563,
          1564,
          1565,
          1566,
          1567,
          1568,
          1569,
          1570,
          1571,
          1572,
          1573,
          1574,
          1575,
          1576,
          1577,
          1578,
          1579,
          1580,
          1581,
          1582,
          1583,
          1584,
          1585,
          1586,
          1587,
          1588,
          1589,
          1590,
          1591,
          1592,
          1593,
          1594,
          1595,
          1596,
          1597,
          1598,
          1599,
          1600,
          1601,
          1602,
          1603,
          1604,
          1605,
          1606,
          1607,
          1608,
          1609,
          1610,
          1611,
          1612,
          1613,
          1614,
          1615,
          1616,
          1617,
          1618,
          1619,
          1620,
          1621,
          1622,
          1623,
          1624,
          1625,
          1626,
          1627,
          1628,
          1629,
          1630,
          1631,
          1632,
          1633,
          1634,
          1635,
          1636,
          1637,
          1638,
          1639,
          1640,
          1641,
          1642,
          1643,
          1644,
          1645,
          1646,
          1647,
          1648,
          1649,
          1650,
          1651,
          1652,
          1653,
          1654,
          1655,
          1656,
          1657,
          1658,
          1659,
          1660,
          1661,
          1662,
          1663,
          1664,
          1665,
          1666,
          1667,
          1668,
          1669,
          1670,
          1671,
          1672,
          1673,
          1674,
          1675,
          1676,
          1677,
          1678,
          1679,
          1680,
          1681,
          1682,
          1683,
          1684,
          1685,
          1686,
          1687,
          1688,
          1689,
          1690,
          1691,
          1692,
          1693,
          1694,
          1695,
          1696,
          1697,
          1698,
          1699,
          1700,
          1701,
          1702,
          1703,
          1704,
          1705,
          1706,
          1707,
          1708,
          1709,
          1710,
          1711,
          1712,
          1713,
          1714,
          1715,
          1716,
          1717,
          1718,
          1719,
          1720,
          1721,
          1722,
          1723,
          1724,
          1725,
          1726,
          1727,
          1728,
          1729,
          1730,
          1731,
          1732,
          1733,
          1734,
          1735,
          1736,
          1737,
          1738,
          1739,
          1740,
          1741,
          1742,
          1743,
          1744,
          1745,
          1746,
          1747,
          1748,
          1749,
          1750,
          1751,
          1752,
          1753,
          1754,
          1755,
          1756,
          1757,
          1758,
          1759,
          1760,
          1761,
          1762,
          1763,
          1764,
          1765,
          1766,
          1767,
          1768,
          1769,
          1770,
          1771,
          1772,
          1773,
          1774,
          1775,
          1776,
          1777,
          1778,
          1779,
          1780,
          1781,
          1782,
          1783,
          1784,
          1785,
          1786,
          1787,
          1788,
          1789,
          1790,
          1791,
          1792,
          1793,
          1794,
          1795,
          1796,
          1797,
          1798,
          1799,
          1800,
          1801,
          1802,
          1803,
          1804,
          1805,
          1806,
          1807,
          1808,
          1809,
          1810,
          1811,
          1812,
          1813,
          1814,
          1815,
          1816,
          1817,
          1818,
          1819,
          1820,
          1821,
          1822,
          1823,
          1824,
          1825,
          1826,
          1827,
          1828,
          1829,
          1830,
          1831,
          1832,
          1833,
          1834,
          1835,
          1836,
          1837,
          1838,
          1839,
          1840,
          1841,
          1842,
          1843,
          1844,
          1845,
          1846,
          1847,
          1848,
          1849,
          1850,
          1851,
          1852,
          1853,
          1854,
          1855,
          1856,
          1857,
          1858,
          1859,
          1860,
          1861,
          1862,
          1863,
          1864,
          1865,
          1866,
          1867,
          1868,
          1869,
          1870,
          1871,
          1872,
          1873,
          1874,
          1875,
          1876,
          1877,
          1878,
          1879,
          1880,
          1881,
          1882,
          1883,
          1884,
          1885,
          1886,
          1887,
          1888,
          1889,
          1890,
          1891,
          1892,
          1893,
          1894,
          1895,
          1896,
          1897,
          1898,
          1899,
          1900,
          1901,
          1902,
          1903,
          1904,
          1905,
          1906,
          1907,
          1908,
          1909,
          1910,
          1911,
          1912,
          1913,
          1914,
          1915,
          1916,
          1917,
          1918,
          1919,
          1920,
          1921,
          1922,
          1923,
          1924,
          1925,
          1926,
          1927,
          1928,
          1929,
          1930,
          1931,
          1932,
          1933,
          1934,
          1935,
          1936,
          1937,
          1938,
          1939,
          1940,
          1941,
          1942,
          1943,
          1944,
          1945,
          1946,
          1947,
          1948,
          1949,
          1950,
          1951,
          1952,
          1953,
          1954,
          1955,
          1956,
          1957,
          1958,
          1959,
          1960,
          1961,
          1962,
          1963,
          1964,
          1965,
          1966,
          1967,
          1968,
          1969,
          1970,
          1971,
          1972,
          1973,
          1974,
          1975,
          1976,
          1977,
          1978,
          1979,
          1980,
          1981,
          1982,
          1983,
          1984,
          1985,
          1986,
          1987,
          1988,
          1989,
          1990,
          1991,
          1992,
          1993,
          1994,
          1995,
          1996,
          1997,
          1998,
          1999
         ],
         "xaxis": "x",
         "y": [
          0.721544521946018,
          0.7029225022105847,
          0.691653627864385,
          0.6841449814327693,
          0.6795274423340619,
          0.6768393362982799,
          0.6750326601125426,
          0.6739583872132382,
          0.6733471143043648,
          0.672955351764873,
          0.6727242665775751,
          0.6725636478197776,
          0.6724779405836332,
          0.672431002228947,
          0.6724019577947714,
          0.6724014712592302,
          0.6724064572382782,
          0.6724235312413361,
          0.6724752135195975,
          0.6723987793518325,
          0.6723446062055685,
          0.6723158951533043,
          0.6722304253254907,
          0.6722033801725356,
          0.6720461899951353,
          0.6719625923593165,
          0.6719214386859182,
          0.6719530269251032,
          0.6717834189786749,
          0.6716424564183768,
          0.6717630467172396,
          0.6716436153751308,
          0.6715081560409675,
          0.6714797189680196,
          0.6714561230045254,
          0.6714432283983393,
          0.6712979633929366,
          0.6713337950787301,
          0.6711139325368203,
          0.671053077608852,
          0.671023886082536,
          0.670911864709046,
          0.6707748651504517,
          0.6708753725229684,
          0.6708617786229667,
          0.670813775062561,
          0.6706580612619044,
          0.6706781288324777,
          0.6706016762781951,
          0.6705484493304107,
          0.6704310449503236,
          0.6704074392884465,
          0.6704338059586994,
          0.670269734576597,
          0.6702386862140591,
          0.6701351141525527,
          0.6701155834278818,
          0.669947283752894,
          0.6699317447209763,
          0.6699150869401834,
          0.6698035187640432,
          0.6696834879406428,
          0.6696341611571232,
          0.6695171010696281,
          0.6694452984858368,
          0.6693287901959177,
          0.6692046593811553,
          0.6692253539117716,
          0.6692061735411822,
          0.6692638591184454,
          0.669177012120263,
          0.669244667433076,
          0.6692349741014384,
          0.669224950418634,
          0.6691222546464306,
          0.6690400956040722,
          0.6690128247616655,
          0.669140060271247,
          0.6690696209163989,
          0.6689801735393072,
          0.668817386788837,
          0.6688114097562887,
          0.6686901836071985,
          0.6685856354438653,
          0.6685036760265545,
          0.6684376696408805,
          0.6683958863807937,
          0.6683252748796495,
          0.668255687164048,
          0.6682498347961296,
          0.6682087631548865,
          0.6680481003502668,
          0.668029361458148,
          0.667888611252025,
          0.6677888260049335,
          0.6676219380508035,
          0.6674884735527685,
          0.6674424118914847,
          0.6674228241888144,
          0.6673507563138412,
          0.6673432719909539,
          0.6673273092609341,
          0.6673351859642287,
          0.6673520300347926,
          0.6672736089108354,
          0.667177457324529,
          0.6671928912906323,
          0.6671859636145123,
          0.6672331167479693,
          0.6670451412766667,
          0.6669938905764434,
          0.6669082134456957,
          0.6668003902596943,
          0.6666207964137449,
          0.6666054881225199,
          0.6664858456385337,
          0.6664646445694616,
          0.6664459113347329,
          0.6663437196763895,
          0.6662832377320629,
          0.6662172028573893,
          0.6661489145230439,
          0.6659903073714951,
          0.6660244040570017,
          0.6660037095263853,
          0.6660334187038874,
          0.6660862409462363,
          0.6660415235212294,
          0.665940093185942,
          0.665909037549617,
          0.665892470489114,
          0.6658155839321978,
          0.665779197216034,
          0.6655897995172921,
          0.6656410453683239,
          0.6656628600621628,
          0.6656099198228221,
          0.6655302975137355,
          0.6654733084015927,
          0.6654199365842141,
          0.6654533511501247,
          0.6654763318724551,
          0.6653715226609828,
          0.6652267965219789,
          0.6650980838274552,
          0.6651353712809288,
          0.6650602061869735,
          0.6650163939443685,
          0.6649489018876674,
          0.6648553472454265,
          0.664733587079129,
          0.6646910188561779,
          0.6645716681318768,
          0.6645583896313684,
          0.6644980293209269,
          0.6643843424522271,
          0.6643994345503338,
          0.6643604145211689,
          0.6643887125839621,
          0.664348935878883,
          0.6643215401697967,
          0.6642889552197214,
          0.6643107521331917,
          0.6643508616140333,
          0.6641772518723698,
          0.6639524003206674,
          0.6639348567542384,
          0.6638320569264686,
          0.6638595686120502,
          0.6637700200080872,
          0.6637574428218906,
          0.663819100897191,
          0.6637192158375756,
          0.6635206388214887,
          0.6635541384502993,
          0.6635532702429819,
          0.6635260707240994,
          0.6635153728016352,
          0.6634602130469629,
          0.6634538711127588,
          0.6634236723689709,
          0.6632899379326125,
          0.6631315097970477,
          0.6630767895003497,
          0.6631620980925479,
          0.6631226476976427,
          0.6631259334289421,
          0.6630816049494985,
          0.6629723603442564,
          0.662909362073672,
          0.6627441650730068,
          0.6626683103836188,
          0.6626177252349207,
          0.6625719884694633,
          0.6624912015462325,
          0.6624490388369156,
          0.6623953128265122,
          0.6622967370485855,
          0.6622435907186088,
          0.6623079960629091,
          0.6622538946442685,
          0.6622647085432279,
          0.6621970994997832,
          0.662144280894328,
          0.6620469610569841,
          0.6620567756184077,
          0.6618345923342948,
          0.6616973590042632,
          0.6617021255573984,
          0.6616941579317642,
          0.6615341730036978,
          0.6616013326887357,
          0.6615466073407965,
          0.6615701632984614,
          0.6615184854652922,
          0.6615097443936235,
          0.6615148016961955,
          0.661444115638733,
          0.6613076280739347,
          0.6612444653349407,
          0.6611100364539583,
          0.6610054961705611,
          0.6609263506986327,
          0.6607682862524259,
          0.6607461595939378,
          0.6606830962633683,
          0.6605850654133295,
          0.6606459567102335,
          0.6605919140880391,
          0.6604517789210304,
          0.6604157193232391,
          0.6604372093233012,
          0.6602805913504908,
          0.660391097957805,
          0.6602208022343911,
          0.6601247233859563,
          0.6601333460565341,
          0.6601438386965606,
          0.6600858975264986,
          0.6600297293420565,
          0.6599618473295438,
          0.6598889391301042,
          0.6597869074950784,
          0.6596955899464882,
          0.6596333808818106,
          0.6595840607659291,
          0.659511163073071,
          0.6594218021732265,
          0.659477041737508,
          0.6595003835225509,
          0.6595410538932024,
          0.6595452951172651,
          0.6594401012032719,
          0.6594298312219523,
          0.6593761381456408,
          0.6593746094380395,
          0.6592411651449689,
          0.6591398469472336,
          0.6591177453429012,
          0.6590578560101784,
          0.6589377746743671,
          0.6588636232634723,
          0.6588373584262396,
          0.6588166653099706,
          0.6587962485976139,
          0.6586379608865511,
          0.6585150019597199,
          0.6584488480778063,
          0.6585519554251331,
          0.6585991014868526,
          0.6584718744633562,
          0.6584786920224206,
          0.6582881470858041,
          0.6580791148088746,
          0.658076802350707,
          0.6581334538378958,
          0.6581345105575303,
          0.658131241192252,
          0.6580389257204735,
          0.6581203282889673,
          0.6578955561427747,
          0.6579221159724866,
          0.6578147678051964,
          0.6576497583066003,
          0.657623410022865,
          0.6575301738108619,
          0.6573554333993944,
          0.6573010984113661,
          0.6572332265013355,
          0.6571346579971961,
          0.6570631554571249,
          0.6570919578358279,
          0.6570746167231414,
          0.6569662189079544,
          0.6569185434761694,
          0.6567308025845027,
          0.6567670907004405,
          0.6566819302106308,
          0.6566961617793067,
          0.6567390098410137,
          0.6566481487225678,
          0.656506985526974,
          0.6564844149654194,
          0.6564060661752346,
          0.6562721317097292,
          0.656132533186573,
          0.6560570710796421,
          0.6559093673350447,
          0.6559337078514745,
          0.6558665883743157,
          0.655770962117082,
          0.6556901188220008,
          0.6557445475610636,
          0.6556844749693144,
          0.6555484535330433,
          0.655433824304807,
          0.6554736854666371,
          0.6554024781210948,
          0.6553185283127477,
          0.6552736179303315,
          0.6552302322145236,
          0.655251993365207,
          0.6551446605536897,
          0.6551131333334971,
          0.6550312236203986,
          0.6549177248599165,
          0.6549566412376145,
          0.6549462831626505,
          0.6548693871094009,
          0.6546866057282787,
          0.654566236269676,
          0.6544851798122212,
          0.6543882723581993,
          0.6544651619458602,
          0.6543412988468752,
          0.6542122283224332,
          0.6542624522063691,
          0.6541133678565592,
          0.653966212878793,
          0.6538912833747217,
          0.6538081152964447,
          0.6538565593250727,
          0.6537102141622769,
          0.6538174101861857,
          0.6537315625255391,
          0.6535784115225581,
          0.6534128611370669,
          0.6535510422819751,
          0.653476893295676,
          0.653446874174021,
          0.6534073754892511,
          0.6532461166381835,
          0.653331684055975,
          0.6532703314797353,
          0.653162177336418,
          0.653006132982545,
          0.6529378371723628,
          0.6527763431355105,
          0.6527653829526093,
          0.6527329630770926,
          0.652727288108761,
          0.6525249848931522,
          0.6525144536616438,
          0.652458035541793,
          0.6523488978208122,
          0.6521778522911719,
          0.6522255440889779,
          0.6520826184143454,
          0.6519835393307573,
          0.6520461929046502,
          0.6520895268957494,
          0.6521691009149713,
          0.6519736298060013,
          0.6519466444597406,
          0.6518977456173655,
          0.6516958559973766,
          0.6515519495737755,
          0.6515023482047906,
          0.6513785927982654,
          0.6513776530653743,
          0.6514820294865107,
          0.6514024494057995,
          0.6512266730858107,
          0.6512683324894663,
          0.6512539113982249,
          0.6512641039945312,
          0.6511596590785657,
          0.6510471731929456,
          0.6508682901576414,
          0.6508315953157716,
          0.650596674620095,
          0.6504790340439748,
          0.6504603284900471,
          0.6503791419126219,
          0.6502250127873178,
          0.650073164600437,
          0.6500942599975457,
          0.6500417844723847,
          0.6500949934377509,
          0.6500903002286361,
          0.6499427706508313,
          0.6498416423797607,
          0.6497090937727589,
          0.649737474069757,
          0.6496077125355348,
          0.6494088106236215,
          0.6492213146161225,
          0.6493178917189776,
          0.649059992927616,
          0.6490074594142073,
          0.6489712975792966,
          0.6488494628566807,
          0.6488530084238214,
          0.6487937341302128,
          0.6486752049397614,
          0.6486783603490409,
          0.6486578337216782,
          0.648414092548823,
          0.6483559216483165,
          0.6484800403401003,
          0.6483802249876119,
          0.6484877160039999,
          0.6483229416911885,
          0.648218460002188,
          0.6480511198609562,
          0.6479596511792328,
          0.6478716569431757,
          0.6477855872299711,
          0.6476001264685292,
          0.6475683676994453,
          0.6476340562610303,
          0.647652314274998,
          0.6474947531344527,
          0.6473924390340255,
          0.6472829723762253,
          0.6473169112609605,
          0.6472636491565381,
          0.6470527828749963,
          0.6469648908760588,
          0.6468887605909573,
          0.6467512132757801,
          0.6467357679948968,
          0.6467241034669391,
          0.6466623011281935,
          0.6465066556203163,
          0.646452718872135,
          0.6462794023045039,
          0.6460662033598302,
          0.6460511969307722,
          0.6461590112265894,
          0.6460907350152226,
          0.6461322356078585,
          0.645974677902157,
          0.6458423226566637,
          0.6459079132241717,
          0.6457765338784557,
          0.6458005947581792,
          0.6456901439165665,
          0.6457197730824099,
          0.6457557278164362,
          0.64550060720767,
          0.6454741817409709,
          0.6453149342941026,
          0.6452439823393095,
          0.6452660025176356,
          0.6450953978603169,
          0.64474134121911,
          0.6447779675661507,
          0.6447271941071849,
          0.6446527101225772,
          0.6444295877117222,
          0.6444185709549208,
          0.6444155161663637,
          0.6444496579089407,
          0.6442922969995919,
          0.6442167393231796,
          0.6442488322823735,
          0.6442952794543767,
          0.6439826237953316,
          0.6439364504005949,
          0.6438397136785217,
          0.6437978857654636,
          0.6437766406495692,
          0.643637123754469,
          0.643554490905697,
          0.6434299206329605,
          0.6433668069920297,
          0.6432531981144921,
          0.6431677858708269,
          0.6430190177287085,
          0.6428746997299841,
          0.6426936048572346,
          0.6425358911692086,
          0.6424491765135426,
          0.6424245983867322,
          0.6424266079724845,
          0.642503599797265,
          0.6423073978747351,
          0.6421811406895266,
          0.6420861965518887,
          0.6420835634409371,
          0.6420651306540279,
          0.6418586205627959,
          0.6417944683866986,
          0.6416373784259214,
          0.6415574835518659,
          0.641465142823882,
          0.6412303762920832,
          0.6412250947144071,
          0.6411972884404458,
          0.6411606400699938,
          0.6411967350264727,
          0.6410962306847007,
          0.640977002604533,
          0.6410529215457076,
          0.6409519438016212,
          0.6408708715842942,
          0.6407066995814695,
          0.6408060647673526,
          0.6405676851838322,
          0.6404455756737014,
          0.6402715949688927,
          0.6401811229980598,
          0.6402313173827479,
          0.6400476148573019,
          0.6399037920822531,
          0.6399759044081478,
          0.6398882174895981,
          0.6396612254239745,
          0.639719355510453,
          0.6395610710321846,
          0.6394096479577533,
          0.6392615310216354,
          0.6393614439641016,
          0.6393312533022993,
          0.6391556081125291,
          0.6390112575838122,
          0.6389163924475848,
          0.6388151415323807,
          0.6385940679049088,
          0.6385704713352656,
          0.6386081366215722,
          0.6383586439035707,
          0.6383131946547557,
          0.6380451800459522,
          0.6381407759957395,
          0.63792354151354,
          0.6378516708390187,
          0.6376927191928282,
          0.6376184156385519,
          0.6374925431558641,
          0.6376105474213423,
          0.6374236577648228,
          0.63729620202113,
          0.6371839468762026,
          0.6369729537074849,
          0.6370382900965416,
          0.6369229987516242,
          0.6367605847827459,
          0.6368821598715702,
          0.6367254600686542,
          0.6367715613316681,
          0.6365831625663628,
          0.6365864097061804,
          0.6362747410596428,
          0.6362318602658934,
          0.6360317915172901,
          0.6360115687725908,
          0.6360723966259068,
          0.6358965255446353,
          0.6357296325392642,
          0.6355570522405334,
          0.6354660694882021,
          0.635441335985216,
          0.6355305835352105,
          0.6354825468386633,
          0.6353993937120599,
          0.6352069066742719,
          0.6350229540113675,
          0.6347314757815862,
          0.6348494368084406,
          0.6348172181743686,
          0.6348703020710056,
          0.6346354559316474,
          0.6343680195889231,
          0.6341961274712773,
          0.6342113393848225,
          0.6341493497460575,
          0.6341059834270154,
          0.6341151431455451,
          0.6341520163972499,
          0.6340336441993714,
          0.6338703961695655,
          0.6337637363854102,
          0.6334581558987246,
          0.6333637187036417,
          0.6333383576344636,
          0.6333779654260409,
          0.633333980430991,
          0.6330009601883969,
          0.6329829502913912,
          0.6328996549218388,
          0.6328259530714002,
          0.632716942439645,
          0.6326905672833071,
          0.6326013348870358,
          0.6324925739886397,
          0.6323344450885967,
          0.6322840935092862,
          0.6319470975358608,
          0.6318167577355595,
          0.6317183973425525,
          0.6316748051320092,
          0.6315259252564381,
          0.6315437858387575,
          0.6314981781830222,
          0.6316490553193174,
          0.6313676397679215,
          0.6313101946297338,
          0.6310919593956511,
          0.630929748688714,
          0.6307402578450866,
          0.630617163140895,
          0.6304407388477002,
          0.630402952129558,
          0.6303347684569278,
          0.6303200442912215,
          0.6301702461000216,
          0.6301098286095312,
          0.6302311012300394,
          0.6303175091743469,
          0.6302568692271993,
          0.6299424787699166,
          0.629539468126782,
          0.6295300800921553,
          0.6293704657231347,
          0.6294215438729626,
          0.6293086662130841,
          0.6290910765276116,
          0.6288038265907159,
          0.628679344209574,
          0.6286612825878596,
          0.6285097079761958,
          0.6283747018393824,
          0.6284564169786744,
          0.628258836067329,
          0.6282504592911672,
          0.6281064348705744,
          0.6281323596582574,
          0.6280246152716168,
          0.6279236809682038,
          0.62786542924784,
          0.6276956885547961,
          0.627398915614112,
          0.6272471197580887,
          0.6272811323909436,
          0.6272699398509527,
          0.6272723311084812,
          0.6272329968921209,
          0.6268738354666759,
          0.6265502834724168,
          0.6265385585316157,
          0.626492057210308,
          0.6265788045980163,
          0.6264537940591068,
          0.6264320480621467,
          0.6262282474566314,
          0.6259909128738662,
          0.6258225568270279,
          0.625956700413914,
          0.6256889793832423,
          0.6256881867424916,
          0.6256837083121477,
          0.6256005733700122,
          0.6255824891187377,
          0.6254540952585511,
          0.6251353241629519,
          0.6249285988888498,
          0.6249170050782672,
          0.6245516754813113,
          0.6244736527992507,
          0.6240895507699352,
          0.624087949526512,
          0.6239812267028679,
          0.6241918440592491,
          0.6239767563545098,
          0.6237074512546346,
          0.6235831834502139,
          0.6233342069690511,
          0.6232360300371202,
          0.6231643309027461,
          0.6229314228235665,
          0.6231096588959129,
          0.623087711051359,
          0.6231713792025033,
          0.6230327000052241,
          0.622703878354218,
          0.622711118197037,
          0.6225949085364908,
          0.6225133815054166,
          0.6223921983929004,
          0.6223499389018042,
          0.6219676365286617,
          0.6216912661568593,
          0.6215035727468587,
          0.6213986273539268,
          0.6212740516258498,
          0.6211405560121698,
          0.6211611978078293,
          0.6211215926428972,
          0.6209691694227316,
          0.6208051115779554,
          0.6205480613950956,
          0.6204041072877787,
          0.6204376372240358,
          0.6204599342103732,
          0.6202638411926011,
          0.61997374498238,
          0.6199436660540306,
          0.6196053201869383,
          0.6197239621210906,
          0.6198232780068608,
          0.6196111517437434,
          0.6194188895872084,
          0.6195374177674116,
          0.6193540952973446,
          0.6192718398773064,
          0.6191561913086197,
          0.6190803404581748,
          0.6188484163607582,
          0.6187285102019876,
          0.6185841968504049,
          0.6186451451253083,
          0.6183428742117801,
          0.6179441559112678,
          0.6178497870089644,
          0.6175580283342782,
          0.6175382874779782,
          0.6175151800705214,
          0.6174377980878798,
          0.6172529362015805,
          0.6171192732907957,
          0.6171773839805086,
          0.617092531010256,
          0.6171569870690168,
          0.6169548422603284,
          0.6170505117561857,
          0.6170568193419504,
          0.6166264689574807,
          0.6160182356834412,
          0.6162069181264457,
          0.6162509708081262,
          0.6160468309612598,
          0.6159055459297309,
          0.6156914531174352,
          0.6156261874457537,
          0.6154460361448385,
          0.615313743938834,
          0.615216534622645,
          0.6149545253333398,
          0.6146219429323229,
          0.6143987217191923,
          0.6143224742452977,
          0.6143032647795597,
          0.6141308481410398,
          0.6139773659786936,
          0.6138817082017155,
          0.6139217483795295,
          0.6136275364180742,
          0.6135711728516272,
          0.6134820861331487,
          0.6134390909793013,
          0.6134562506514081,
          0.6134265424841542,
          0.6133484646425409,
          0.613006439451444,
          0.6128668235520185,
          0.6124618299936844,
          0.6125926581479735,
          0.6123151763010833,
          0.6123522758483887,
          0.6121973734790996,
          0.6120485469446344,
          0.6116928979501886,
          0.6114230018551067,
          0.6113560581611375,
          0.6111269269959402,
          0.6110811524471994,
          0.6109996098583027,
          0.6109491891780142,
          0.610688772848097,
          0.6106080374475252,
          0.6102470230248015,
          0.6101113931607391,
          0.6101161221326408,
          0.6101081702668788,
          0.6099056274203931,
          0.6098894828456943,
          0.6096259509102773,
          0.6099484530545898,
          0.6095173084129721,
          0.6091181251962307,
          0.6090670557345375,
          0.6091653179314177,
          0.6088526699502589,
          0.6087537341198679,
          0.608493933839313,
          0.6083326309414233,
          0.608381522105912,
          0.6081777354418221,
          0.6079097026485508,
          0.6076875601784658,
          0.6074684383505482,
          0.6075734320333448,
          0.6073671126769761,
          0.6075161414631343,
          0.6072357527280258,
          0.6070387644282842,
          0.6068788174855507,
          0.6069691457990872,
          0.6065749770503933,
          0.6063714009220317,
          0.6061677253852457,
          0.605945769609031,
          0.6059099272146063,
          0.6057945291874772,
          0.6060203467385243,
          0.6059920112965471,
          0.6055657839370986,
          0.6055821948132273,
          0.6052571046150337,
          0.6051992933628922,
          0.6048522326905849,
          0.6047907558538146,
          0.6046848921452539,
          0.6045940195099783,
          0.604378832598864,
          0.6045052207122414,
          0.6043702570058531,
          0.6040244589417668,
          0.6038900629948761,
          0.603618456347514,
          0.6034743175668231,
          0.6032994872432644,
          0.6032641798762952,
          0.6030788536799156,
          0.6029314109834574,
          0.6028232548196437,
          0.6026182514126018,
          0.6024400727223542,
          0.6022938265638836,
          0.6019807922638069,
          0.601951698731568,
          0.6016042476993496,
          0.6013796929585732,
          0.601324785765955,
          0.6013713941735737,
          0.6011448051969884,
          0.6009290838645677,
          0.600828103493836,
          0.6008327150748948,
          0.6005856813010523,
          0.600390284546351,
          0.6004110178704989,
          0.6001793693687956,
          0.6000912082397332,
          0.6000221711094097,
          0.5995487906165042,
          0.599608521340257,
          0.5996802263340708,
          0.5992942016003495,
          0.5992582589893017,
          0.5992004131866714,
          0.5989217584416018,
          0.5985739119982315,
          0.5984496565188392,
          0.5983706417730299,
          0.5982704136331203,
          0.5980790158449594,
          0.5978862449274225,
          0.5978202704655922,
          0.5976474856926223,
          0.5974221853886621,
          0.597206408694639,
          0.5971682314145363,
          0.5969949609142239,
          0.5969387937400301,
          0.596946878756507,
          0.5967482700186261,
          0.596613451181832,
          0.5962457105264826,
          0.5962818117465003,
          0.5960879612777192,
          0.5960044230444956,
          0.5955453015990176,
          0.5955705576023813,
          0.5951853097495386,
          0.5952089760263087,
          0.5950110079878468,
          0.5950782303082741,
          0.5947637735787085,
          0.5949616533214763,
          0.594559386221029,
          0.5941607760170758,
          0.5940958615076743,
          0.5938451615430541,
          0.593628139819129,
          0.5934279215537895,
          0.5933510855092841,
          0.5932138669288765,
          0.593346597582607,
          0.5929902951596147,
          0.5931455618244107,
          0.5928805615942357,
          0.5925458999003395,
          0.5924534575413849,
          0.5921987093101113,
          0.5920459555367292,
          0.5919372281785739,
          0.5916015679553404,
          0.5914438817460658,
          0.5911900744599811,
          0.5911625082209959,
          0.5908951167332924,
          0.5909835457801819,
          0.5910138045327138,
          0.5908381949036808,
          0.5904947537486837,
          0.5903697143166752,
          0.5901653059458328,
          0.5899470064599636,
          0.5898473822464377,
          0.5896422598321559,
          0.5893057136212365,
          0.5892687306565754,
          0.5892778669373464,
          0.5891311623282351,
          0.5892368142887697,
          0.5885555820950007,
          0.5884577256138042,
          0.5882722882901208,
          0.5880849927158679,
          0.5879340236469851,
          0.5877721841052427,
          0.5879995822906494,
          0.5877385707224829,
          0.5874893909793789,
          0.5871213648278835,
          0.5869905613236508,
          0.5866015707032155,
          0.5864601058475042,
          0.5864395400225106,
          0.5863505254357548,
          0.5861147872472213,
          0.5861204761569783,
          0.5859186713978396,
          0.5856898271431357,
          0.5854818558288832,
          0.5854816313517296,
          0.5852980831922111,
          0.5849041704404152,
          0.5847515732555066,
          0.5846844079130787,
          0.5844850879604534,
          0.5841767496981863,
          0.584206401897689,
          0.5840169332795224,
          0.5839592159804651,
          0.5840665572780673,
          0.5838751324152542,
          0.583438396655907,
          0.5833177750393496,
          0.5829766574552504,
          0.5830048270144705,
          0.5826769196381003,
          0.5825575666912531,
          0.58283417245089,
          0.5826839234869359,
          0.5825237841929419,
          0.5819285817065482,
          0.5814945418955916,
          0.5820064894223618,
          0.5812540302842351,
          0.5810208447908951,
          0.5811927251896616,
          0.5811995213314638,
          0.5806220971931846,
          0.580363389395051,
          0.5803592681884766,
          0.5802641636234219,
          0.5798655237181712,
          0.5796787629693242,
          0.5793047149302596,
          0.579276300890971,
          0.5792010673021866,
          0.5789261169352774,
          0.5787903609922377,
          0.5788212287223945,
          0.5787809177980585,
          0.5781115406650608,
          0.5779139276278221,
          0.5778835547172417,
          0.5778572240118253,
          0.5775885280916246,
          0.5772069136975175,
          0.5772172994532827,
          0.5770815994779942,
          0.5771031444355593,
          0.5767947609141721,
          0.5764473201864857,
          0.5762744521690627,
          0.5763833692518331,
          0.5759350350347616,
          0.5760801701222436,
          0.575576189412909,
          0.5754752448049643,
          0.5753803439059499,
          0.5750409336413367,
          0.5749390159623098,
          0.5749016650652481,
          0.5744633472571938,
          0.5742193506935895,
          0.5740764696719283,
          0.5739115088672961,
          0.5737295639716973,
          0.5737855032338934,
          0.5733818874520771,
          0.5732275273840306,
          0.5730525374412536,
          0.5729779104054984,
          0.5726757318286573,
          0.5724779009819031,
          0.572329598362163,
          0.5722469123743348,
          0.5720232957500523,
          0.5717617891602597,
          0.5716977165917219,
          0.5714116716788987,
          0.5714288410493883,
          0.5712013396166139,
          0.5714517306473296,
          0.5712912024077722,
          0.5707496144003787,
          0.5707859144372455,
          0.5703172443276745,
          0.5700640045990378,
          0.5698965816174524,
          0.5697495280686071,
          0.5696354876130314,
          0.5696668249065593,
          0.5695977714102147,
          0.5691405601420645,
          0.5689555376262988,
          0.5686966314154156,
          0.5685316409094859,
          0.5683523901438309,
          0.56860416137566,
          0.5683045520620831,
          0.5678965974662263,
          0.5679892972364264,
          0.5678514969550957,
          0.56729702909114,
          0.5673485838760763,
          0.5667520967580504,
          0.5664730019488577,
          0.5663609561273607,
          0.5663129232697568,
          0.5662158042697584,
          0.566034032934803,
          0.5655860906940395,
          0.5655414797491947,
          0.5652196510363433,
          0.5651547082399918,
          0.5649566599878214,
          0.5650180267075361,
          0.5645913631229077,
          0.5643150491229558,
          0.5643739247726182,
          0.5640210679021932,
          0.563801589052556,
          0.5638786736181227,
          0.5634620638216956,
          0.5632997256214336,
          0.5633338142249544,
          0.5631732645681349,
          0.5629749863834704,
          0.5630456613282026,
          0.562596273018142,
          0.562358122558917,
          0.5620568895744065,
          0.5619501578605781,
          0.5618605375289917,
          0.5614358851465128,
          0.5612151511644913,
          0.560949119066788,
          0.5607652563159748,
          0.5607344578888457,
          0.5606893414157932,
          0.5604824039895656,
          0.5605949349322561,
          0.5599950812630734,
          0.560120984457307,
          0.5599281743421393,
          0.5599197848368499,
          0.5593133708177986,
          0.5591773069511026,
          0.5588831586352849,
          0.55855162305347,
          0.5584452035063404,
          0.5584801871897811,
          0.5580675919177168,
          0.557852597964012,
          0.5581674745527364,
          0.5576669604091321,
          0.5576826651217573,
          0.5572395969245394,
          0.5571761032282296,
          0.5568807282690275,
          0.5566961955215971,
          0.5566907494755114,
          0.5562865451230841,
          0.5558838304826769,
          0.5556872879044484,
          0.5554993756746842,
          0.5553583918991736,
          0.5552654260295933,
          0.5550893137010477,
          0.5547387175640818,
          0.5551603592048258,
          0.5545541409718788,
          0.5542945429430169,
          0.5542018427687176,
          0.5537928173097514,
          0.5536624571024361,
          0.5534687521093983,
          0.5532464098122161,
          0.5530309766025866,
          0.5528396331657798,
          0.5526520159284948,
          0.5526167301808373,
          0.5523903844720226,
          0.5524816567614927,
          0.5519275418782639,
          0.5517602049698264,
          0.5515104867644229,
          0.5513329156374527,
          0.5511846033193297,
          0.5509887452852928,
          0.5509302153425701,
          0.550573908878585,
          0.5504922676894625,
          0.5502058144343102,
          0.549989044262191,
          0.5498200436769906,
          0.5500236858755855,
          0.5498673295570632,
          0.5496007109092453,
          0.5491503784212015,
          0.5489942851713148,
          0.548819036807044,
          0.5484783645403587,
          0.5482861134965541,
          0.5484219745054083,
          0.5490186957989709,
          0.5479583879648628,
          0.5475169258602595,
          0.5474713523509138,
          0.5473245620727539,
          0.5473783436468092,
          0.54761112079782,
          0.5471816826674898,
          0.5467854087635622,
          0.5462549846051103,
          0.5463280192876266,
          0.5466127470388251,
          0.5457410038527796,
          0.545938549607487,
          0.54534257852425,
          0.5453324204784329,
          0.5451218792947672,
          0.5448009816266722,
          0.544847038842864,
          0.5445503152022928,
          0.5443279343136286,
          0.54390306088884,
          0.5438001202324689,
          0.5435430508548931,
          0.5434576905379861,
          0.5431520365052304,
          0.5430328500472893,
          0.5427562943959641,
          0.5425691396503125,
          0.5424174027927852,
          0.5422420408766149,
          0.5420947674977578,
          0.541918219146082,
          0.5419023921934225,
          0.5415984984171592,
          0.5412024635379598,
          0.5410752340898676,
          0.5407723657155441,
          0.5409322843713276,
          0.540452319484646,
          0.5402557407395314,
          0.5402037309387983,
          0.5399629796965647,
          0.5398031760070283,
          0.5394835769119909,
          0.5393158979335073,
          0.5392355619850805,
          0.5389280331336845,
          0.5388948741605726,
          0.5384740710258484,
          0.5382829607543299,
          0.5382023298134238,
          0.5378858863297156,
          0.5378173807920036,
          0.5375146104117571,
          0.537514952077704,
          0.5375851354356539,
          0.5376406348357766,
          0.5376300308663966,
          0.5369979139101707,
          0.5363837139081147,
          0.5362931940515162,
          0.5363658486786536,
          0.5358919988244267,
          0.5360994773395991,
          0.535719855154975,
          0.5352562641693374,
          0.5350296958018157,
          0.5348896796420469,
          0.5351582391787384,
          0.5349126437963065,
          0.5346030859623925,
          0.5341482172578068,
          0.5340266118615361,
          0.5338305818832527,
          0.5337001578282502,
          0.5334944276486413,
          0.533104180077375,
          0.5330100465629061,
          0.5329619359161895,
          0.532624639696994,
          0.5324770369772184,
          0.5321414892956362,
          0.5319429215738329,
          0.5317699800103398,
          0.5315881743269452,
          0.5313926464420254,
          0.5311921871314614,
          0.5310582167011196,
          0.5311877236527912,
          0.5306311383085736,
          0.5304259124448744,
          0.5302795497037597,
          0.5304174269660045,
          0.5300951646546186,
          0.5309048339471979,
          0.5307096434851825,
          0.5294906125230304,
          0.5291316268807751,
          0.5290723123792874,
          0.5296690993389841,
          0.5291282704321004,
          0.5285985627416837,
          0.5281612169944634,
          0.5280487882888923,
          0.5277386000600912,
          0.5275513762134617,
          0.527390239804478,
          0.5275116380998643,
          0.5269890635700549,
          0.5268256670337612,
          0.5267912232269675,
          0.5267564009811918,
          0.5262379660444745,
          0.5264750201823347,
          0.5258752245013997,
          0.5262578802593684,
          0.5258006352489277,
          0.5252932716224153,
          0.5251162791656235,
          0.5253202038296199,
          0.5247171361567611,
          0.5248491830744986,
          0.5244476330482354,
          0.5245555340233495,
          0.5239903435868732,
          0.5240782515477326,
          0.5235886230307111,
          0.5233815956923922,
          0.5233343298152342,
          0.5231510002734298,
          0.5228718006004721,
          0.5226723885132094,
          0.5225629402419268,
          0.5222554178561194,
          0.5220758282532126,
          0.5220452326839253,
          0.5216918122970452,
          0.521564291695417,
          0.5214442709744986,
          0.5211011741120937,
          0.520930627847122,
          0.520957161006281,
          0.521113767866361,
          0.5206068917856378,
          0.5205200318562783,
          0.5212240435309329,
          0.5201063032877647,
          0.5199013958543034,
          0.5196193543531127,
          0.5198139271493686,
          0.5194077412960894,
          0.5189194309509406,
          0.5188095100855423,
          0.5186618608943486,
          0.5185111763113636,
          0.5180947546231545,
          0.5179248878511332,
          0.5185041143732556,
          0.5179136565176107,
          0.5175002653720016,
          0.5172066872402773,
          0.5169785495531761,
          0.516827714847306,
          0.5168703247935085,
          0.5165340282149234,
          0.516563963688026,
          0.5160626463970895,
          0.515877997673164,
          0.5159934103488922,
          0.5155058561745337,
          0.51531994322599,
          0.515120969687478,
          0.5149383601495775,
          0.5149577680280653,
          0.514577951673734,
          0.5144453161853855,
          0.5142789994255971,
          0.5140006076481383,
          0.5142626043093407,
          0.5138290036532839,
          0.5134381917573638,
          0.5132582470522089,
          0.5130816263667608,
          0.5130256391177743,
          0.5127856437432564,
          0.5125786992452912,
          0.5126414035336446,
          0.5121529422574124,
          0.5120264284691568,
          0.5118055578005516,
          0.5115902872408851,
          0.5115598395719366,
          0.511271692837699,
          0.5113067373380823,
          0.5109103572570671,
          0.5106944326627052,
          0.5106180635549254,
          0.5105134349758342,
          0.5105097963648327,
          0.5104856463812165,
          0.5098316784632408,
          0.5095744907855988,
          0.5094644211106382,
          0.5092110708608466,
          0.5095783775135622,
          0.5093747040982973,
          0.5086749774924779,
          0.5087601054522951,
          0.5084174588575201,
          0.5081401307704085,
          0.508825283131357,
          0.5078115843110166,
          0.5076106790768898,
          0.5077949990660457,
          0.5080495942447145,
          0.507155013993635,
          0.5069547126858921,
          0.506880847478317,
          0.5066023707389832,
          0.5064944073305292,
          0.5062031659029298,
          0.5059964985160504,
          0.505838504080045,
          0.505700703394615,
          0.5057442095320104,
          0.5053152125770762,
          0.5050921380519867,
          0.5049644204519563,
          0.5048279479398565,
          0.50459072458542,
          0.5044200533527439,
          0.5044791983345808,
          0.5040322762424663,
          0.5039512432227701,
          0.503690171948934,
          0.5035941941253209,
          0.5033313442084749,
          0.503225549095768,
          0.503020424559965,
          0.502973298602185,
          0.5029232489860664,
          0.5026171932786198,
          0.5027488055875746,
          0.5021976472967762,
          0.5020904550107859,
          0.501924627413184,
          0.5020286496412956,
          0.5013999688423286,
          0.501249558137635,
          0.5010788679122925,
          0.5008764253834547,
          0.5010331707485651,
          0.500550964222116,
          0.5003753411567817,
          0.5002978106676522,
          0.5001525056564202,
          0.5000322196443202,
          0.4997055691177562,
          0.49956696275937357,
          0.49936336602194836,
          0.4991612835455749,
          0.49906697000487377,
          0.498924294871799,
          0.4987999626135422,
          0.4988894715147503,
          0.4983768416663348,
          0.49815249180389665,
          0.4979681572671664,
          0.49784084275617435,
          0.4976149564072237,
          0.4975682592998117,
          0.49735887202165896,
          0.49710899227756566,
          0.49693756103515624,
          0.4970415426512896,
          0.4966155445171615,
          0.4964349389076233,
          0.49638121744333685,
          0.4962371168500286,
          0.4959634882918859,
          0.49580268273919315,
          0.49595185215190307,
          0.4954595164727357,
          0.49557348459453904,
          0.4952154848535182,
          0.4950158581895343,
          0.49509433912018597,
          0.4946188131631431,
          0.49443768060813514,
          0.4942840716596377,
          0.4941078119358774,
          0.494060427354554,
          0.49376336022958917,
          0.4939696771613622,
          0.49343802140930954,
          0.493330343800076,
          0.4938890544034667,
          0.49300224821446303,
          0.49320324528015264,
          0.49284455321602905,
          0.49297586325871745,
          0.49240056943085236,
          0.4921692481485464,
          0.49195168048648513,
          0.4917875024221711,
          0.49162432046259863,
          0.49148997927116134,
          0.4912908319699562,
          0.49118278764062007,
          0.49096069265220127,
          0.49116241861197907,
          0.4908999377388065,
          0.4904878923448466,
          0.4917033768306344,
          0.4907274489685641,
          0.4907933500863738,
          0.4900838442778183,
          0.48994475512181296,
          0.48950741088996497,
          0.4893592467752554,
          0.48945980334686023,
          0.4891826821585833,
          0.4889733401395507,
          0.48872755664890094,
          0.48855790849459374,
          0.48841321296611073,
          0.48836790414179787,
          0.4880759563486455,
          0.48794929324570346,
          0.48847401445194827,
          0.487662440740456,
          0.4877025929547973,
          0.48741513460369434,
          0.4871906038058006,
          0.48704466072179503,
          0.4870790411860256,
          0.48672462125956,
          0.486492210727627,
          0.48633536888381185,
          0.48633694507307923,
          0.48633568367715607,
          0.48597176711438067,
          0.4857672508490288,
          0.48586209143622444,
          0.4854118374444671,
          0.4862620024357812,
          0.4852407576674122,
          0.4851326392868818,
          0.4848001782166756,
          0.4849157038381544,
          0.48535579099493514,
          0.48510825128878576,
          0.4842054103390645,
          0.48403052889694603,
          0.48390443607912226,
          0.4838157659870083,
          0.48360423314369333,
          0.4837201779171572,
          0.48390225161940364,
          0.4832369346739882,
          0.4830126217866348,
          0.48350307830309464,
          0.4828356747910128,
          0.4825449030278093,
          0.48237756187632935,
          0.4823253968004453,
          0.482389560089273,
          0.48226433527671686,
          0.48180280752101184,
          0.48165951805599666,
          0.4815574582350456,
          0.4813379382683059,
          0.48144972112219214,
          0.48105077107073896,
          0.48102960222858493,
          0.48079200558743235,
          0.4811119080600092,
          0.48076570064334545,
          0.48086496217776153,
          0.4802106326919491,
          0.48040940498901624,
          0.4799064899905253,
          0.4801039349224608,
          0.4797631599135318,
          0.47944587054899185,
          0.4794955843586033,
          0.47942255523245214,
          0.47916916578502977,
          0.47921605574882636,
          0.47891889679229865,
          0.4786086852267637,
          0.4784668505191803,
          0.47876120209693906,
          0.47818224824081035,
          0.47804131588693394,
          0.47850440215256257,
          0.47864586175498314,
          0.47790067195892333,
          0.4774838746604273,
          0.47734877598487724,
          0.47723104105157366,
          0.4771583037861323,
          0.47691609091677906,
          0.4768161448381715,
          0.4767574251708338,
          0.4766875694363804,
          0.47644734897855984,
          0.4763649122189667,
          0.47612120994066787,
          0.4759385204921334,
          0.47582647062964356,
          0.47699828885369383,
          0.47553500694743656,
          0.47539872739274625,
          0.47528557353100537,
          0.4751707103292821,
          0.4749969883490417,
          0.47485292301339616,
          0.47494514200647,
          0.47566948856337593,
          0.47481417241743057,
          0.47444264211897125,
          0.474251752586688,
          0.474079948966786,
          0.4739160185143099,
          0.47376767346414467,
          0.4737586596254575,
          0.47362755042011456,
          0.4742527867777873,
          0.4739475309848785,
          0.4732476332430112,
          0.47297625895273887,
          0.47341396061040586,
          0.47275351740546145,
          0.4728407922437636,
          0.47245401861303943,
          0.472551145492974,
          0.47220633555266817,
          0.4722288438829325,
          0.47193154328960485,
          0.4717743782673852,
          0.47168470641313975,
          0.47155781054900864,
          0.47278231604624604,
          0.4713123656935611,
          0.4711304765636638,
          0.47101674372867003,
          0.4708577632904053,
          0.4707663025896428,
          0.4707978097058959,
          0.4713613763704138,
          0.47036419874530727,
          0.47105988284288824,
          0.4700883629968611,
          0.4703374890957849,
          0.4698595223790508,
          0.4697421078964815,
          0.46974805987487406,
          0.4698061270228887,
          0.47005380068795155,
          0.4721770479517468,
          0.4718053065114102,
          0.46973271380036563,
          0.4688430465884128,
          0.46884770726753494,
          0.4686748645063174,
          0.46863538077322103,
          0.4684028175927825,
          0.46825527922581817,
          0.4682651279336315,
          0.468016699007002,
          0.467891002812628,
          0.467871625645686,
          0.46791295085923146,
          0.467535905110634,
          0.46798301430071815,
          0.467645420563423,
          0.46715226335040594,
          0.4672321009433876,
          0.4669924357179868,
          0.46680425799499126,
          0.4670624674376795,
          0.4665430248793909,
          0.4666612462472107,
          0.46643019874217145,
          0.46619368710760345,
          0.46625211289373497,
          0.46602185792842155,
          0.46645955180717724,
          0.4661655032028586,
          0.4656018154095795,
          0.46559694672034957,
          0.4676542445764703,
          0.46524553511102323,
          0.4651950781628237,
          0.4650169139191256,
          0.46518844608533183,
          0.4650574915489908,
          0.4649620083429046,
          0.46465840905399647,
          0.46473022679151116,
          0.46460932200237853,
          0.4642072386660818,
          0.4640940199464054,
          0.4643461655762236,
          0.46386166073508184,
          0.46381000825914287,
          0.4636511537988307,
          0.46367500464795,
          0.4636545551025261,
          0.4633021425392668,
          0.4632114483138262,
          0.4640007421121759,
          0.4637872358499947,
          0.4628698374255229,
          0.462787469059734,
          0.46298418368323374,
          0.4634729044922328,
          0.46278212019952675,
          0.46299119611917916,
          0.4622713845665172,
          0.4625327560861232,
          0.46197809324426165,
          0.4646390962398658,
          0.4618443559792082,
          0.46190807031372844,
          0.4615405344356925,
          0.4614432723845466,
          0.46133406738103444,
          0.46148574392674335,
          0.4611634485802408,
          0.4609996610778873,
          0.460987034490553,
          0.46079088661630274,
          0.46067256775953,
          0.4606647933943797,
          0.460492634975304,
          0.46036840040804977,
          0.46026928091453295,
          0.4601234203678066,
          0.4600311867261337,
          0.4604503075955278,
          0.46011184989395787,
          0.46288841548612564,
          0.4608664506572788,
          0.45980981800515774,
          0.459703557774172,
          0.4593915902962119,
          0.45945529614464714,
          0.4591607111995503,
          0.45908211530265164,
          0.4589823328842551,
          0.4589834172846907,
          0.45897023465673803,
          0.4586596359640865,
          0.4586502985428956,
          0.45848551665322257,
          0.4582402402061527,
          0.4582081741195614,
          0.45863769812099003,
          0.4579978155887733,
          0.4582438846765938,
          0.45781111020152854,
          0.45771396584429985,
          0.45761904928643826,
          0.4574605251772929,
          0.4574526005882328,
          0.4572825933917094,
          0.4574044691304029,
          0.4573066603329222,
          0.4573322304224564,
          0.45721930831165636,
          0.456831620002197,
          0.45670552536592646,
          0.45662203774613846,
          0.4580209439083681,
          0.4564603505498272,
          0.4562495613502244,
          0.45648759724730154,
          0.45623700669256306,
          0.4559309695736837,
          0.4567367062730304,
          0.45573371689198383,
          0.45579472739817734,
          0.4559988833079904,
          0.4560690016059552,
          0.45535867345535147,
          0.4553091100716995,
          0.4552374057850595,
          0.4550641595307043,
          0.4551100185361959,
          0.4574947677426419,
          0.45480452048576486,
          0.4547054243289818,
          0.45489506418422115,
          0.4548712787991863,
          0.45448564472845043,
          0.4543814957141876,
          0.45446107518874995,
          0.45444781376143634,
          0.4543278534533614,
          0.45468320705122867,
          0.45393427357835286,
          0.45481152362742666,
          0.4544296463667336,
          0.4538601903592126,
          0.45375846129352765,
          0.45353225621126464,
          0.4536157259496592,
          0.453895942841546,
          0.4547286977202205,
          0.45305196056931707,
          0.45334637478246526,
          0.4534461290149365,
          0.45340662689532263,
          0.4527158852350914,
          0.4532364140122624,
          0.45302703229047486,
          0.4528252957230907,
          0.45246782494803606,
          0.4523083979800596,
          0.4538474388041739,
          0.4522602290420209,
          0.4520315198575036,
          0.45280493326106314,
          0.4519401826090732,
          0.4521295002961563,
          0.4516807288436566,
          0.4516609585891336,
          0.4515234801728847,
          0.45155278747364624,
          0.4516841240858628,
          0.4514724300069324,
          0.45182468042535295,
          0.4513828727148347,
          0.45101149839870003,
          0.45158722643124855,
          0.45129096043311945,
          0.4508020254514985,
          0.4510541834063449,
          0.4516939978478319,
          0.45053379838749513,
          0.45062428470385274,
          0.45311649269976856,
          0.45136311377509164,
          0.45054702172845096,
          0.4501203200574649,
          0.450067340317419,
          0.44997706877983223,
          0.4499308337599544,
          0.45001747193983044,
          0.4498819635076038,
          0.45007169206263653,
          0.44966823670823697,
          0.4514912635593091,
          0.44959456435704637,
          0.44935737413875126,
          0.451273358777418,
          0.4493045103752007,
          0.44909885172116554,
          0.4493817620358225,
          0.44914510916855377,
          0.44883685960608016,
          0.44957561149435527,
          0.448845958608692,
          0.44910706322071914,
          0.4486370921134949,
          0.4488927693690284,
          0.4483734852176602,
          0.44865168817972734,
          0.4483761415643207,
          0.44821571940082616,
          0.44819517155825084,
          0.4501692169803684,
          0.44847732984413535,
          0.44869641601029087,
          0.44782310815180765,
          0.44814859194270634,
          0.4476982566259675,
          0.4475552183086589,
          0.4475706057023194,
          0.4481618913553529,
          0.44798110763905413,
          0.44743199207014955,
          0.4473228193945804,
          0.4481853886175964,
          0.4470358645511886,
          0.4470040005142406,
          0.44690871915574804,
          0.4474985152988111,
          0.44788687774690533,
          0.44800822320630995,
          0.44660174533472224,
          0.44658110717595634,
          0.4493779476416313,
          0.4475757845377518,
          0.4462929894358425,
          0.44636992882874055,
          0.44624839283652223,
          0.44619778061317183,
          0.44613416689937396,
          0.44592873555118756,
          0.44740068084102563,
          0.4462601886967481,
          0.4458843548419112,
          0.44564855613950954,
          0.4456427230673321,
          0.44552490660699745,
          0.44566142336796905,
          0.4454043261075424,
          0.4454084766113152,
          0.44585810505737694,
          0.4455427077867217,
          0.4450860303337291,
          0.44545232694027787,
          0.4449393578505112,
          0.4457745835942737,
          0.44482335448265076,
          0.4458544778621803,
          0.4457004359212972,
          0.4445821361016419,
          0.4451214215513003,
          0.44488750605259914,
          0.44442780179492497,
          0.44522941274158023,
          0.44436362466569673,
          0.4445110064441875,
          0.44413727117797075,
          0.44443172438670014,
          0.44759402103343254,
          0.4496412057997817,
          0.44386067915770966,
          0.44376582654856017,
          0.44371025127879643,
          0.44577922649302726,
          0.4452985591807608,
          0.4453367475735939,
          0.44359187099893216,
          0.44338642762879193,
          0.44342124977354275,
          0.4445557029570563,
          0.44336591888282256,
          0.4431156343322689,
          0.4430610271833711,
          0.44325012211072246,
          0.4435830603211613,
          0.44297044650983003,
          0.4428048168198537,
          0.44312331636073227,
          0.44318693714626767,
          0.4428385797193495,
          0.443363616022013,
          0.4432193939968691,
          0.4424447150553687,
          0.44411208488173404,
          0.44246665370666377,
          0.44234766030715683,
          0.44228017976728534,
          0.44246828768212915,
          0.4423079042111413,
          0.4426657864602946,
          0.4419455205990096,
          0.4421118003837133,
          0.4417837511684935,
          0.4417813111159761,
          0.4418711917885279,
          0.4421111696857517,
          0.44155108504376167,
          0.4414821884389651,
          0.44170624916836365,
          0.4413693857395043,
          0.4415441114013478,
          0.4431999490422718,
          0.4430266565185482,
          0.4451631061101364,
          0.44121724407551655,
          0.44197110958018543,
          0.441705757278507,
          0.4414819498183364,
          0.441932576389636,
          0.4407755210237988,
          0.44075054263664504,
          0.44077312592732704,
          0.44063964031510433,
          0.4442818486084372,
          0.44068117424593134,
          0.4406314258858309,
          0.44071382041704854,
          0.4404786135180522,
          0.44106822811951074,
          0.44061772227287294,
          0.4443149824263686,
          0.4403085676290221,
          0.440793272398286,
          0.44013892907207297,
          0.44096414658982874,
          0.44006729287616275,
          0.4408331401267294,
          0.44229396844314317,
          0.4399501948033349,
          0.43981705237243135,
          0.4401243765475386,
          0.44168102367449613,
          0.44232965909828575,
          0.44036098597413403,
          0.43941260545940725,
          0.44279963869159505,
          0.43971492193512995,
          0.43923749176122373,
          0.43918536123582874,
          0.4393411999031649,
          0.43995835447715503,
          0.43931079042159904,
          0.43909177547794276,
          0.4395510733127594,
          0.4400226129313647,
          0.44050935668460395,
          0.4388491493160442,
          0.4388039688942796,
          0.44017614813174233,
          0.4390827632556527,
          0.4387325536396544,
          0.4391595317145525,
          0.4399229682098001,
          0.43840019864551094,
          0.43865719973030737,
          0.4390851361266637,
          0.43899433380466396,
          0.43876792889530375,
          0.4410923596155846,
          0.4391060976658837,
          0.43813069937592847,
          0.4385987258563607,
          0.4380886265787028,
          0.4387490674600763,
          0.43930028444629604
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "orientation": "h",
         "title": {
          "text": ""
         },
         "tracegroupgap": 0,
         "y": 1.02
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "font": {
           "size": 18
          },
          "xaxis": {
           "title": {
            "font": {
             "size": 24
            }
           }
          },
          "yaxis": {
           "title": {
            "font": {
             "size": 24
            }
           }
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "epoch"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Validation Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validationDF = pd.DataFrame({'trainLoss': loss_list,'validationLoss': validate_loss_list})\n",
    "validationDF.index.name = 'epoch'\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "plot_template = dict(\n",
    "    layout=go.Layout({\n",
    "        \"font_size\": 18,\n",
    "        \"xaxis_title_font_size\": 24,\n",
    "        \"yaxis_title_font_size\": 24})\n",
    ")\n",
    "\n",
    "fig = px.line(validationDF, labels=dict(\n",
    "    created_at=\"Date\", value=\"Validation Loss\", variable=\"value\"\n",
    "))\n",
    "fig.update_layout(\n",
    "  template=plot_template, legend=dict(orientation='h', y=1.02, title_text=\"\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(xbTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([596, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbTemp,ybtemp = train_ds[0:]\n",
    "xbTemp.shape\n",
    "pred = model(xbTemp)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features = torch.tensor(test_scaled.to_numpy())\n",
    " \n",
    "# test_prediction = model(test_features.float()).detach().numpy().flatten()\n",
    " \n",
    "# test_prediction_binary = (test_prediction > 0.5).astype(np.int)\n",
    " \n",
    "# test_prediction_df = pd.DataFrame(test_prediction_binary,\n",
    "#                                   index=test.index,\n",
    "#                                   columns=[\"Survived\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(loss_list, linewidth=3)\n",
    "# plt.plot(validate_loss_list, linewidth=3)\n",
    "# plt.legend((\"Training Loss\", \"Validation Loss\"))\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"BCE Loss\")\n",
    "\n",
    "# print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.Sequential(torch.nn.Linear(12, 50),\n",
    "#                             torch.nn.ReLU(),\n",
    "#                             torch.nn.Linear(50, 1),\n",
    "#                             torch.nn.Sigmoid())\n",
    "\n",
    "\n",
    "# model[0].weight\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d018c0188c58e58e60ac3cb19763249ff342a612f7de0403bc8a5c09a7c73107"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
